"""
This file is auto-generated by _generate_layers.py.
"""

from __future__ import annotations
from typing import Union, Optional, Tuple, List, Dict, Any
from returnn.util.basic import NotSpecified
from .base import ILayerMaker, LayerRef, LayerDictRaw


class _Base(ILayerMaker):
  """
  This is the base class for all layers.
  Every layer by default has a list of source layers `sources` and defines `self.output` which is of type :class:`Data`.
  It shares some common functionality across all layers, such as explicitly defining the output format,
  some parameter regularization, and more.

  If you want to implement your own layer::

      class YourOwnLayer(_ConcatInputLayer):  # e.g. either _ConcatInputLayer or LayerBase as a base
          " some docstring "
          layer_class = "your_layer_name"

          def __init__(self, your_kwarg1, your_opt_kwarg2=None, **kwargs):
              " docstring, document the args! "
              super(YourOwnLayer, self).__init__(**kwargs)
              # Now we need to set self.output, which must be of type :class:`Data`.
              # It is set at this point to whatever we got from `self.get_out_data_from_opts()`,
              # so it is enough if we set self.output.placeholder and self.output.size_placeholder,
              # but we could also reset self.output.
              self.output.placeholder = self.input_data.placeholder + 42  # whatever you want to do
              # If you don't modify the sizes (e.g. sequence-length), just copy the input sizes.
              self.output.size_placeholder = self.input_data.size_placeholder.copy()

          @classmethod
          def get_out_data_from_opts(cls, **kwargs):
              " This is supposed to return a :class:`Data` instance as a template, given the arguments. "
              # example, just the same as the input:
              return get_concat_sources_data_template(kwargs["sources"], name="%s_output" % kwargs["name"])

  """

  def __init__(self,
               *,
               param_device: Optional[str] = NotSpecified,
               is_output_layer: Optional[bool] = NotSpecified,
               only_on_eval: bool = NotSpecified,
               only_on_search: bool = NotSpecified,
               copy_output_loss_from_source_idx: Optional[int] = NotSpecified,
               l2: Optional[float] = NotSpecified,
               darc1: Optional[float] = NotSpecified,
               spatial_smoothing: Optional[float] = NotSpecified,
               param_variational_noise: Optional[float] = NotSpecified,
               updater_opts: Optional[Dict[str]] = NotSpecified,
               initial_output: Union[str, float] = NotSpecified,
               collocate_with: Optional[List[str]] = NotSpecified,
               trainable: Optional[bool] = NotSpecified,
               custom_param_importer: Optional[Union[str, callable]] = NotSpecified,
               register_as_extern_data: Optional[str] = NotSpecified,
               ):
    """
    Usually the arguments, when specified in the network dict,
    are going through :func:`transform_config_dict`, before they are passed to here.
    See :func:`TFNetwork.construct_from_dict`.

    :param str|None param_device: e.g. "CPU", etc. any valid name for tf.device.
      see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/device_name_utils.h
    :param bool|None is_output_layer:
    :param bool only_on_eval: if True, this layer will only be calculated in eval
    :param bool only_on_search: if True, this layer will only be calculated when search is done
    :param int|None copy_output_loss_from_source_idx: if set, will copy output_loss from this source
    :param float|None l2: for constraints
    :param float|None darc1: for constraints. see Generalization in Deep Learning, https://arxiv.org/abs/1710.05468
    :param float|None spatial_smoothing: see :func:`TFUtil.spatial_smoothing_energy`
    :param float|None param_variational_noise: adds variational noise to the params during training
    :param dict[str]|None updater_opts: accepts similar opts as TFUpdater, e.g. "optimizer", "learning_rate", ...
    :param str|float initial_output: used for recurrent layer, see self.get_rec_initial_output()
    :param list[str]|None collocate_with: in the rec layer, collocate with the specified other layers
    :param bool|None trainable: whether the parameters of this layer will be trained.
      default (None) inherits from the parent layer if there is one, or otherwise True.
    :param str|callable|None custom_param_importer: used by :func:`set_param_values_by_dict`
    :param str|None register_as_extern_data: registers output in network.extern_data
    """
    super().__init__()
    self.param_device = param_device
    self.is_output_layer = is_output_layer
    self.only_on_eval = only_on_eval
    self.only_on_search = only_on_search
    self.copy_output_loss_from_source_idx = copy_output_loss_from_source_idx
    self.l2 = l2
    self.darc1 = darc1
    self.spatial_smoothing = spatial_smoothing
    self.param_variational_noise = param_variational_noise
    self.updater_opts = updater_opts
    self.initial_output = initial_output
    self.collocate_with = collocate_with
    self.trainable = trainable
    self.custom_param_importer = custom_param_importer
    self.register_as_extern_data = register_as_extern_data

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'param_device': self.param_device,
      'is_output_layer': self.is_output_layer,
      'only_on_eval': self.only_on_eval,
      'only_on_search': self.only_on_search,
      'copy_output_loss_from_source_idx': self.copy_output_loss_from_source_idx,
      'L2': self.l2,
      'darc1': self.darc1,
      'spatial_smoothing': self.spatial_smoothing,
      'param_variational_noise': self.param_variational_noise,
      'updater_opts': self.updater_opts,
      'initial_output': self.initial_output,
      'collocate_with': self.collocate_with,
      'trainable': self.trainable,
      'custom_param_importer': self.custom_param_importer,
      'register_as_extern_data': self.register_as_extern_data,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return opts

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Source(_Base):
  """
  This gives access to some entry from network.extern_data (:class:`ExternData`).
  """

  def __init__(self,
               *,
               data_key: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param str|None data_key:
    """
    super().__init__(**kwargs)
    self.data_key = data_key

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'data_key': self.data_key,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'source',
      **self.get_opts()}


class _ConcatInput(_Base):
  """
  Base layer which concatenates all incoming source layers in the feature dimension,
  and provides that as `self.input_data`, which is of type :class:`Data`.
  This is the most common thing what many layers do with the input sources.
  If there is only a single source, will not do anything.
  This layer also optionally can do dropout on the input.
  """

  def __init__(self,
               *,
               dropout: float = NotSpecified,
               dropout_noise_shape: Any = NotSpecified,
               dropout_on_forward: bool = NotSpecified,
               mask: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param float dropout: 0.0 means to apply no dropout. dropout will only be applied during training
    :param dict[str|tuple,int|None] dropout_noise_shape: see :func:`TFUtil.get_bc_shape`
    :param bool dropout_on_forward: apply dropout during inference
    :param str|None mask: "dropout" or "unity" or None. this is obsolete and only here for historical reasons
    """
    super().__init__(**kwargs)
    self.dropout = dropout
    self.dropout_noise_shape = dropout_noise_shape
    self.dropout_on_forward = dropout_on_forward
    self.mask = mask

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dropout': self.dropout,
      'dropout_noise_shape': self.dropout_noise_shape,
      'dropout_on_forward': self.dropout_on_forward,
      'mask': self.mask,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Copy(_ConcatInput):
  """
  This layer does nothing, it copies its input.
  If multiple sources are provided, they are concatenated in the feature-dim.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'copy',
      'from': source,
      **self.get_opts()}


class Dropout(Copy):
  """
  Just the same as :class:`CopyLayer`, because that one already supports dropout.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dropout',
      'from': source,
      **self.get_opts()}


class ScaledGradient(Copy):
  """
  Just tf.identity in the forward pass.
  Scales the gradient by some factor in backprop.
  Can be used as gradient reversal layer (with negative factor).
  Uses :func:`TFUtil.scaled_gradient`, or :func:`tf.stop_gradient`
  """

  def __init__(self,
               *,
               scale: float,
               **kwargs):
    """
    :param float scale: if 0., will use tf.stop_gradient
    """
    super().__init__(**kwargs)
    self.scale = scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'scale': self.scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'scaled_grad',
      'from': source,
      **self.get_opts()}


class Activation(_ConcatInput):
  """
  This layer just applies an activation function.
  See :func:`TFUtil.get_activation_function` about supported functions.
  Also see :class:`EvalLayer` and :class:`CombineLayer` for similar layers.
  """

  def __init__(self,
               *,
               activation: str,
               **kwargs):
    """
    :param str activation: e.g. "relu", "tanh", etc
    """
    super().__init__(**kwargs)
    self.activation = activation

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'activation': self.activation,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'activation',
      'from': source,
      **self.get_opts()}


class BatchNorm(Copy):
  """
  Implements batch-normalization (http://arxiv.org/abs/1502.03167) as a separate layer.

  Also see :class:`NormLayer`.
  """

  def __init__(self,
               *,
               use_shift: bool = NotSpecified,
               use_std: bool = NotSpecified,
               use_sample: float = NotSpecified,
               force_sample: bool = NotSpecified,
               momentum: float = NotSpecified,
               epsilon: float = NotSpecified,
               update_sample_only_in_training: bool = NotSpecified,
               delay_sample_update: bool = NotSpecified,
               param_version: int = NotSpecified,
               gamma_init: Union[str, float] = NotSpecified,
               beta_init: Union[str, float] = NotSpecified,
               masked_time: bool = NotSpecified,
               **kwargs):
    """
    The default settings for these variables are set in the function "batch_norm" of the LayerBase. If you do not want
    to change them you can leave them undefined here.
    With our default settings:

    - In training: use_sample=0, i.e. not using running average, using current batch mean/var.
    - Not in training (e.g. eval): use_sample=1, i.e. using running average, not using current batch mean/var.
    - The running average includes the statistics of the current batch.
    - The running average is also updated when not training.

    :param bool use_shift:
    :param bool use_std:
    :param float use_sample: defaults to 0.0 which is used in training
    :param bool force_sample: even in eval, use the use_sample factor
    :param float momentum: for the running average of sample_mean and sample_std
    :param float epsilon:
    :param bool update_sample_only_in_training:
    :param bool delay_sample_update:
    :param int param_version: 0 or 1
    :param str|float gamma_init: see :func:`TFUtil.get_initializer`, for the scale
    :param str|float beta_init: see :func:`TFUtil.get_initializer`, for the mean
    :param bool masked_time: flatten and mask input tensor
    """
    super().__init__(**kwargs)
    self.use_shift = use_shift
    self.use_std = use_std
    self.use_sample = use_sample
    self.force_sample = force_sample
    self.momentum = momentum
    self.epsilon = epsilon
    self.update_sample_only_in_training = update_sample_only_in_training
    self.delay_sample_update = delay_sample_update
    self.param_version = param_version
    self.gamma_init = gamma_init
    self.beta_init = beta_init
    self.masked_time = masked_time

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'use_shift': self.use_shift,
      'use_std': self.use_std,
      'use_sample': self.use_sample,
      'force_sample': self.force_sample,
      'momentum': self.momentum,
      'epsilon': self.epsilon,
      'update_sample_only_in_training': self.update_sample_only_in_training,
      'delay_sample_update': self.delay_sample_update,
      'param_version': self.param_version,
      'gamma_init': self.gamma_init,
      'beta_init': self.beta_init,
      'masked_time': self.masked_time,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'batch_norm',
      'from': source,
      **self.get_opts()}


class LayerNorm(_ConcatInput):
  """
  Applies `layer-normalization <https://arxiv.org/abs/1607.06450>`__.

  Note that we *just* normalize over the feature-dim axis here.
  This is consistent to the default behavior of :class:`tf.keras.layers.LayerNormalization`
  and also how it is commonly used in many models, including Transformer.

  However, there are cases where it would be common to normalize over all axes except batch-dim,
  or all axes except batch and time.
  For a more generic variant, see :class:`NormLayer`.
  """

  def __init__(self,
               *,
               epsilon: float = NotSpecified,
               **kwargs):
    """
    :param float epsilon:
    """
    super().__init__(**kwargs)
    self.epsilon = epsilon

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'epsilon': self.epsilon,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'layer_norm',
      'from': source,
      **self.get_opts()}


class Norm(_ConcatInput):
  """
  Normalize over specified axes, e.g. time and/or feature axis.

  Note: For calculating a norm, see :class:`MathNormLayer` instead.

  In case of just feature (``axes="F"``),
  this corresponds to `layer normalization <https://arxiv.org/abs/1607.06450>`__ (see :class:`LayerNormLayer`).
  In case of time and feature (``axes="TF"``) for a 3D input,
  or more general all except batch (``axes="except_batch"``),
  this corresponds to `group normalization <https://arxiv.org/abs/1803.08494>`__ with G=1,
  or non-standard layer normalization.
  (The definition of layer-normalization is not clear on what axes should be normalized over.
  In many other frameworks, the default axis is just the last axis,
  which is usually the feature axis.
  However, in certain implementations and models,
  it is also common to normalize over all axes except batch.)

  The statistics are calculated just on the input.
  There are no running statistics (in contrast to batch normalization, see :class:`BatchNormLayer`).

  For some discussion on the definition of layer-norm vs group-norm,
  also see
  `here <https://stats.stackexchange.com/questions/485550/is-group-norm-with-g-1-equiv-to-layer-norm>`__
  and `here <https://github.com/tensorflow/addons/issues/2143>`__.
  """

  def __init__(self,
               *,
               axes: Any,
               param_shape: Any = NotSpecified,
               scale: bool = NotSpecified,
               bias: bool = NotSpecified,
               epsilon: float = NotSpecified,
               **kwargs):
    """
    :param str|list[str] axes: axes over which the mean and variance are computed, e.g. "F" or "TF"
    :param str|list[str]|tuple[str]|int|list[int]|tuple[int] param_shape: shape of the scale and bias parameters.
      You can also refer to (static) axes of the input, such as the feature-dim.
      This is also the default, i.e. a param-shape of [F], independent of the axes to normalize over.
    :param bool scale: add trainable scale parameters
    :param bool bias: add trainable bias parameters
    :param float epsilon: epsilon for numerical stability
    """
    super().__init__(**kwargs)
    self.axes = axes
    self.param_shape = param_shape
    self.scale = scale
    self.bias = bias
    self.epsilon = epsilon

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axes': self.axes,
      'param_shape': self.param_shape,
      'scale': self.scale,
      'bias': self.bias,
      'epsilon': self.epsilon,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'norm',
      'from': source,
      **self.get_opts()}


class MathNorm(_ConcatInput):
  """
  Calculates sum(abs(x) ** p) ** (1./p).
  """

  def __init__(self,
               *,
               p: Union[int, float],
               axes: Any,
               keep_dims: bool = NotSpecified,
               **kwargs):
    """
    :param int|float p:
    :param str|list[str] axes:
    :param bool keep_dims:
    """
    super().__init__(**kwargs)
    self.p = p
    self.axes = axes
    self.keep_dims = keep_dims

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'p': self.p,
      'axes': self.axes,
      'keep_dims': self.keep_dims,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'math_norm',
      'from': source,
      **self.get_opts()}


class Slice(_ConcatInput):
  """
  Slicing on the input, i.e. x[start:end:step] in some axis.
  See also :class:`SliceNdLayer`, for variable start.
  See also :class:`GatherLayer`, for one single position.

  Note that __getitem__ on a TF tensor (or also Numpy ND array) is more generic,
  and supports slices in multiple axes, as well as adding new dimensions, etc.
  It even allows to get boolean values, and then applies a boolean mask.
  See TF _slice_helper (== tf.Tensor.__getitem__) for a generic implementation,
  which calls tf.strided_slice.
  If we ever need such more generic support, we might consider adding a new layer,
  like ``GenericSliceLayer``, which gets a ``splice_spec``,
  just like ``_slice_helper`` (argument to ``__getitem__``).
  But any such a slice can already be constructed with multiple individual layers,
  which perform individual slices (per axis).

  We just support slicing in a single axis here, with optional striding (slice_step).
  """

  def __init__(self,
               *,
               axis: Union[int, str],
               slice_start: Optional[int] = NotSpecified,
               slice_end: Optional[int] = NotSpecified,
               slice_step: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param int|str axis:
    :param int|None slice_start:
    :param int|None slice_end:
    :param int|None slice_step:
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.slice_start = slice_start
    self.slice_end = slice_end
    self.slice_step = slice_step

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'slice_start': self.slice_start,
      'slice_end': self.slice_end,
      'slice_step': self.slice_step,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'slice',
      'from': source,
      **self.get_opts()}


class SliceNd(_ConcatInput):
  """
  This takes out a slice-range from some axis,
  e.g. ``x[start:start + size]``.
  This layers allows a different start slice point for each batch,
  in contrast to :class:`SliceLayer`, and the start is variable.
  See also :class:`GatherNdLayer`.
  :class:`PrefixInTimeLayer` can recover the original shape (by zero-padding).
  """

  def __init__(self,
               *,
               size: Optional[int],
               min_size: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param int|None size: if None, it uses the max possible size, and it becomes a dynamic axis
    :param int|None min_size: if size is None, but we want to have a min-size, set this
    """
    super().__init__(**kwargs)
    self.size = size
    self.min_size = min_size

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'size': self.size,
      'min_size': self.min_size,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      start: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'start': start,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'slice_nd',
      'from': source,
      **args,
      **self.get_opts()}


class Gather(_ConcatInput):
  """
  Gathers slices on a specified axis from the input layer using indices from a ``position`` layer.
  If the input is a layer of the shape ``[B,D,F1]``, and position of shape ``[B,F2]``, this will yield output of the
  shape ``[B,F2,F1]`` where

  ``output[b,f2,f1] = input[b,position[b,f2],f1]``

  (if ``D`` is the axis to gather from).
  In general, all shared axes of the input and the positions will be considered as batch-axes.

  The ``position`` argument can also be an ``int``.
  In this case, this simply gives ``input[position]`` one the specified ``axis``.

  It's basically a wrapper around ``tf.gather``.
  It provides the same functionality as the deprecated ``GatherNdLayer``, but is more generic.
  See also :class:`GatherNdLayer`.
  """

  def __init__(self,
               *,
               axis: str,
               **kwargs):
    """
    :param str axis: The axis into which we gather the indices into
    """
    super().__init__(**kwargs)
    self.axis = axis

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      position: Union[LayerRef, int],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'position': position,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'gather',
      'from': source,
      **args,
      **self.get_opts()}


class GatherNd(_ConcatInput):
  """
  Warning: This layer is deprecated, use the more general :class:`GatherLayer` instead.
  :class:`GatherLayer` should be equivalent, but is more general (supports multiple batch dimensions, can specify gather
   axis) and its name is less misleading.

  This takes out a position from some axis, e.g. ``x[pos]``.
  This layers allows a different position for each batch.
  It's basically a wrapper around ``tf.gather`` (the name of this layer is misleading).
  See also :class:`GatherLayer` instead, which will replace this layer in the future.
  See also :class:`SliceNdLayer`.
  See also :class:`ScatterNdLayer`, which is the inverse operation.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      position: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'position': position,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'gather_nd',
      'from': source,
      **args,
      **self.get_opts()}


class ScatterNd(_ConcatInput):
  """
  The inverse of :class:`GatherNdLayer`.
  Mostly a wrapper for ``tf.scatter_nd``.

  The input to the layer are the ``updates``, the ``indices`` are via the ``position`` argument.
  The indices are into the newly constructed output dimension.
  The output shape is constructed via the common shape of the input, the position,
  and the the unique common axis (if not unique, we would need to introduce an option to specify it)
  is replaced by the given output dimension (currently via ``output_dim_via_time_from``).

  Examples::

    position (indices): (B,eTs)
    input (updates): (eTs,D) or (B,eTs,D) -> expanded to (B,eTs,D)
    output shape: (B,eT,D)

    position (indices): (B,dT,eTs)
    input (updates): (eTs,D) -> expanded to (B,dT,eTs,D)
    output shape: (B,dT,eT,D)

    position (indices): (dT,eTs)
    input (updates): (eTs,D) -> expanded to (dT,eTs,D)
    output shape: (dT,eTs,D)

    position (indices): (dT,eTs)
    input (updates): (B,eTs,D) -> expanded to (dT,eTs,B,D)
    output shape: (dT,eT,B,D)

  In all these examples, output_dim_via_time_from is (B,eT,F), and eTs gets replaced by eT.
  """

  def __init__(self,
               *,
               position_axis: Union[str, int],
               filter_invalid_indices: bool = NotSpecified,
               **kwargs):
    """
    :param str|int position_axis: axis in `position` to replace by the output-dim
    :param bool filter_invalid_indices: allow for indices <0 or >= output_dim, which will be discarded in the output
    """
    super().__init__(**kwargs)
    self.position_axis = position_axis
    self.filter_invalid_indices = filter_invalid_indices

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'position_axis': self.position_axis,
      'filter_invalid_indices': self.filter_invalid_indices,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      position: LayerRef,
                      output_dim_via_time_from: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'position': position,
      'output_dim_via_time_from': output_dim_via_time_from,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'scatter_nd',
      'from': source,
      **args,
      **self.get_opts()}


class Linear(_ConcatInput):
  """
  Linear/forward/fully-connected/1x1-conv layer.
  Does a linear transformation on the feature-dimension of the input
  with an optional bias term and an optional activation function.
  See also :class:`DotLayer`, :class:`ElemwiseProdLayer`, :class:`WeightedSumLayer`.
  """

  def __init__(self,
               n_out: int,
               *,
               activation: Optional[str] = NotSpecified,
               with_bias: bool = NotSpecified,
               grad_filter: Optional[float] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               bias_init: Union[str, float] = NotSpecified,
               use_transposed_weights: bool = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param str|None activation: e.g. "relu", or None
    :param bool with_bias:
    :param float|None grad_filter: if grad norm is higher than this threshold (before activation), the grad is removed
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str|float bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param bool use_transposed_weights: If True, define the weight matrix with transposed dimensions (n_out, n_in).
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.activation = activation
    self.with_bias = with_bias
    self.grad_filter = grad_filter
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.use_transposed_weights = use_transposed_weights

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'activation': self.activation,
      'with_bias': self.with_bias,
      'grad_filter': self.grad_filter,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'use_transposed_weights': self.use_transposed_weights,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'linear',
      'from': source,
      **self.get_opts()}


class Softmax(Linear):
  """
  Just a LinearLayer with activation="softmax" by default.
  """

  def __init__(self,
               *,
               activation: Any = NotSpecified,
               **kwargs):
    """
    :param activation:
    """
    super().__init__(activation=activation, **kwargs)
    self.activation = activation

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'activation': self.activation,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'softmax',
      'from': source,
      **self.get_opts()}


class Length(_Base):
  """
  Returns the length of sources as (B,), via input size_placeholder.
  """

  def __init__(self,
               *,
               add_time_axis: bool = NotSpecified,
               dtype: str = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param bool add_time_axis:
    :param str dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.add_time_axis = add_time_axis
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'add_time_axis': self.add_time_axis,
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'length',
      'from': source,
      **self.get_opts()}


class SoftmaxOverSpatial(_ConcatInput):
  """
  This applies a softmax over spatial axis/axes (currently only time axis supported).
  E.g. when the input is of shape (B,T,dim), the output will be (B,T,dim).
  It automatically masks the frames outside the seq defined by the seq-len.
  In contrast to :class:`SoftmaxLayer`, this will not do a linear transformation.
  See :class:`SeqLenMaskLayer` if you just want to apply a masking.
  """

  def __init__(self,
               *,
               axis: Optional[str] = NotSpecified,
               energy_factor: Optional[float] = NotSpecified,
               use_time_mask: bool = NotSpecified,
               log_space: bool = NotSpecified,
               **kwargs):
    """
    :param str|None axis: which axis to do the softmax over
    :param float|None energy_factor: the energy will be scaled by this factor.
      This is like a temperature for the softmax.
      In Attention-is-all-you-need, this is set to 1/sqrt(base_ctx.dim).
    :param bool use_time_mask: if True, assumes dyn seq len, and use it for masking.
      By default, if dyn seq len exists, it uses it.
    :param bool log_space: if True, returns in log space (i.e. uses log_softmax)
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.energy_factor = energy_factor
    self.use_time_mask = use_time_mask
    self.log_space = log_space

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'energy_factor': self.energy_factor,
      'use_time_mask': self.use_time_mask,
      'log_space': self.log_space,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      start: Optional[LayerRef] = NotSpecified,
                      window_start: Optional[Union[LayerRef, int]] = NotSpecified,
                      window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'start': start,
      'window_start': window_start,
      'window_size': window_size,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'softmax_over_spatial',
      'from': source,
      **args,
      **self.get_opts()}


class SeqLenMask(_ConcatInput):
  """
  Masks some values away given the seq_len_source with mask_value.
  Also see :class:`SoftmaxOverSpatialLayer`.
  Also see :class:`SwitchLayer`, which can be used to apply a generic mask.
  """

  def __init__(self,
               *,
               mask_value: float,
               axis: Union[str, int] = NotSpecified,
               **kwargs):
    """
    :param float mask_value:
    :param str|int axis:
    """
    super().__init__(**kwargs)
    self.mask_value = mask_value
    self.axis = axis

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mask_value': self.mask_value,
      'axis': self.axis,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      seq_len_source: Optional[LayerRef] = NotSpecified,
                      start: Optional[LayerRef] = NotSpecified,
                      window_start: Optional[LayerRef] = NotSpecified,
                      window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'seq_len_source': seq_len_source,
      'start': start,
      'window_start': window_start,
      'window_size': window_size,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'seq_len_mask',
      'from': source,
      **args,
      **self.get_opts()}


class RandInt(_Base):
  """
  Generates random numbers using ``tf.random.uniform``
  """

  def __init__(self,
               *,
               shape: Any,
               maxval: int,
               minval: int = NotSpecified,
               dtype: str = NotSpecified,
               seed: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param tuple[DimensionTag|int]|list[DimensionTag|int] shape: desired shape of output tensor
    :param int maxval: upper bound (exclusive) on range of random values
    :param int minval: lower bound (inclusive) on range of random values
    :param str dtype: type of the output. For random ints, int32 and int64 make sense, but could also be floats
    :param int|None seed: random seed
    """
    super().__init__(**kwargs)
    self.shape = shape
    self.maxval = maxval
    self.minval = minval
    self.dtype = dtype
    self.seed = seed

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'shape': self.shape,
      'maxval': self.maxval,
      'minval': self.minval,
      'dtype': self.dtype,
      'seed': self.seed,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'rand_int',
      'from': source,
      **self.get_opts()}


class Range(_Base):
  """
  Generic wrapper around ``tf.range``.
  See also :class:`RangeInAxisLayer`.
  """

  def __init__(self,
               *,
               limit: Union[int, float],
               start: Union[int, float] = NotSpecified,
               delta: Union[int, float] = NotSpecified,
               dtype: Optional[str] = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param int|float limit:
    :param int|float start:
    :param int|float delta:
    :param str|None dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.limit = limit
    self.start = start
    self.delta = delta
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'limit': self.limit,
      'start': self.start,
      'delta': self.delta,
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'range',
      'from': source,
      **self.get_opts()}


class RangeInAxis(_Base):
  """
  Assume that the input is e.g. (B,T,D), and you specify axis="T", you will get (B=1,T,D=1),
  where the specified axis is filled with ``tf.range``.
  See also :class:`RangeLayer`.
  """

  def __init__(self,
               *,
               axis: str,
               dtype: str = NotSpecified,
               unbroadcast: bool = NotSpecified,
               keepdims: bool = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param str axis:
    :param str dtype:
    :param bool unbroadcast:
    :param bool keepdims:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.dtype = dtype
    self.unbroadcast = unbroadcast
    self.keepdims = keepdims
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'dtype': self.dtype,
      'unbroadcast': self.unbroadcast,
      'keepdims': self.keepdims,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'range_in_axis',
      'from': source,
      **self.get_opts()}


class BatchSoftmax(_ConcatInput):
  """
  Softmax over spacial and feature axis
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'batch_softmax',
      'from': source,
      **self.get_opts()}


class Constant(_Base):
  """
  Output is a constant value.
  """

  def __init__(self,
               *,
               value: Union[int, float, bool] = NotSpecified,
               dtype: Optional[str] = NotSpecified,
               with_batch_dim: bool = NotSpecified,
               **kwargs):
    """
    :param int|float|bool value:
    :param str|None dtype:
    :param bool with_batch_dim:
    """
    super().__init__(**kwargs)
    self.value = value
    self.dtype = dtype
    self.with_batch_dim = with_batch_dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'value': self.value,
      'dtype': self.dtype,
      'with_batch_dim': self.with_batch_dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'constant',
      **self.get_opts()}


class Gating(_ConcatInput):
  """
  Splits the output into two equal parts, applies the gate_activation (sigmoid by default)
  on the one part, some other activation (e.g. tanh) on the other part and then
  element-wise multiplies them.
  Thus, the output dimension is input-dimension / 2.
  """

  def __init__(self,
               *,
               activation: Any,
               gate_activation: Any = NotSpecified,
               **kwargs):
    """
    :param activation:
    :param gate_activation:
    """
    super().__init__(**kwargs)
    self.activation = activation
    self.gate_activation = gate_activation

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'activation': self.activation,
      'gate_activation': self.gate_activation,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'gating',
      'from': source,
      **self.get_opts()}


class Window(_ConcatInput):
  """
  Adds a window dimension.
  By default, uses the time axis and goes over it with a sliding window.
  The new axis for the window is created right after the time axis.
  Will always return as batch major mode.
  E.g. if the input is (batch, time, dim), the output is (batch, time, window_size, dim).
  If you want to merge the (window_size, dim) together to (window_size * dim,),
  you can use the MergeDimsLayer, e.g. {"class": "merge_dims", "axes": "except_time"}.
  Use stride==window_size and window_right=window_size - 1 in combination with a
  MergeDimsLayer to achieve feature stacking with right-hand zero padding.

  This is not to take out a window from the time-dimension.
  See :class:`SliceLayer` or :class:`SliceNdLayer`.
  """

  def __init__(self,
               *,
               window_size: int,
               window_left: Optional[int] = NotSpecified,
               window_right: Optional[int] = NotSpecified,
               axis: str = NotSpecified,
               padding: str = NotSpecified,
               stride: int = NotSpecified,
               **kwargs):
    """
    :param int window_size:
    :param int|None window_left:
    :param int|None window_right:
    :param str axis: see Data.get_axis_from_description()
    :param str padding: "same" or "valid"
    :param int stride: return only each Nth window
    """
    super().__init__(**kwargs)
    self.window_size = window_size
    self.window_left = window_left
    self.window_right = window_right
    self.axis = axis
    self.padding = padding
    self.stride = stride

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'window_size': self.window_size,
      'window_left': self.window_left,
      'window_right': self.window_right,
      'axis': self.axis,
      'padding': self.padding,
      'stride': self.stride,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'window',
      'from': source,
      **self.get_opts()}


class Cumsum(_ConcatInput):
  """
  Basically wraps tf.cumsum. Also supports that in the RecLayer.
  """

  def __init__(self,
               *,
               axis: str = NotSpecified,
               additional_left_summand_per_element: Optional[Union[str, int, float]] = NotSpecified,
               reverse: bool = NotSpecified,
               **kwargs):
    """
    :param str axis: see :func:`Data.get_axis_from_description`
    :param str|int|float|None additional_left_summand_per_element: the order matters for tf.string
    :param bool reverse:
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.additional_left_summand_per_element = additional_left_summand_per_element
    self.reverse = reverse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'additional_left_summand_per_element': self.additional_left_summand_per_element,
      'reverse': self.reverse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'cumsum',
      'from': source,
      **self.get_opts()}


class Pad(_ConcatInput):
  """
  Adds (e.g. zero) padding in some axis or axes.
  """

  def __init__(self,
               *,
               axes: Any,
               padding: Any,
               value: Union[int, float] = NotSpecified,
               mode: str = NotSpecified,
               **kwargs):
    """
    :param str|list[str] axes: e.g. "F" etc. see :func:`Dataset.get_axes_from_description`.
    :param list[(int,int)]|(int,int)|int padding: how much to pad left/right in each axis
    :param int|float value: what constant value to pad, with mode=="constant"
    :param str mode: "constant", "reflect", "symmetric" and "replication"
    """
    super().__init__(**kwargs)
    self.axes = axes
    self.padding = padding
    self.value = value
    self.mode = mode

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axes': self.axes,
      'padding': self.padding,
      'value': self.value,
      'mode': self.mode,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'pad',
      'from': source,
      **self.get_opts()}


class MergeDims(_ConcatInput):
  """
  Merges a list of axes into a single one. (Flatten the dims.)
  E.g. input is (batch, width, height, dim) and axes=(1,2), then we get (batch, width*height, dim).
  Or input is (batch, time, height, dim) and axes="except_time", then we get (batch, time, height*dim).
  See also :class:`CombineDimsLayer`.
  When batch and time got merged, :class:`SplitBatchTimeLayer` can undo this.
  When you want to merge batch and time, but remove the padding efficiently, i.e. flatten it,
  see :class:`FlattenBatchLayer`.
  """

  def __init__(self,
               *,
               axes: Any,
               keep_order: bool = NotSpecified,
               **kwargs):
    """
    :param str|list[str]|list[int] axes: see Data.get_axes_from_description(), e.g. "except_time"
    :param bool keep_order: By default (for historical reasons), the axes are sorted, and then merged.
      Thus, the order of incoming axes will influence the result.
      E.g. inputs [B,S,F] and [B,F,S], with ``axes=["S","F"]``, will get different results,
      although the output shape is [B,S*F] in both cases.
      This is bad: In general, other layers in RETURNN might reorder the axes for various reasons,
      and all layers should behave in the same way, no matter the order.
      It is recommended to set ``keep_order=True``, such that the order defined in ``axes`` defines the behavior,
      and not the incoming axis order.
    """
    super().__init__(**kwargs)
    self.axes = axes
    self.keep_order = keep_order

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axes': self.axes,
      'keep_order': self.keep_order,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'merge_dims',
      'from': source,
      **self.get_opts()}


class Split(_ConcatInput):
  """
  Splits one axis into multiple parts, via tf.split.
  self.output is simply the input copied.
  Each part can be accessed via the sublayers "/%i".
  """

  def __init__(self,
               *,
               axis: Optional[str] = NotSpecified,
               num_splits: Optional[int] = NotSpecified,
               size_splits: Optional[List[int]] = NotSpecified,
               **kwargs):
    """
    :param str|None axis: feature axis by default
    :param int|None num_splits:
    :param list[int]|None size_splits:
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.num_splits = num_splits
    self.size_splits = size_splits

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'num_splits': self.num_splits,
      'size_splits': self.size_splits,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'split',
      'from': source,
      **self.get_opts()}


class SplitDims(_ConcatInput):
  """
  Splits one axis into multiple axes.
  E.g. if you know that your feature-dim is composed by a window,
  i.e. the input is (batch, time, window * feature),
  you can set axis="F", dims=(window, -1),
  and you will get the output (batch, time, window, feature).

  If the split axis has a dynamic length,
  exactly one of the axes that we split into need to also have a dynamic length.
  You can e.g. use this to split the input dimension into smaller "chunks" of a fixed window size.
  E.g. you could have input (batch, time, feature) and set axis="T", dims=(-1, window),
  to get output (batch, split_time, window, feature).
  In this case, the exact sequence lengths are lost and everything is padded to multiples of the window size using
  the given padding value.
  Use :class:`ReinterpretDataLayer` to receive back the original sequence lengths after merging.

  Also see :class:`SplitBatchTimeLayer`.
  Also see :class:`MergeDimsLayer` which can undo this operation.
  """

  def __init__(self,
               *,
               axis: str,
               dims: Any,
               pad_to_multiples: Optional[bool] = NotSpecified,
               pad_value: Union[int, float] = NotSpecified,
               **kwargs):
    """
    :param str axis: e.g. "F"
    :param tuple[int]|list[int] dims: what the axis should be split into. e.g. (window, -1)
    :param bool|None pad_to_multiples: If true, input will be padded to the next multiple of the product of the
      static dims, such that splitting is actually possible.
      By default this is done iff the axis has a dynamic size
    :param int|float pad_value: What pad value to use for pad_to_multiples
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.dims = dims
    self.pad_to_multiples = pad_to_multiples
    self.pad_value = pad_value

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'dims': self.dims,
      'pad_to_multiples': self.pad_to_multiples,
      'pad_value': self.pad_value,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'split_dims',
      'from': source,
      **self.get_opts()}


class SplitBatchTime(_ConcatInput):
  """
  A very specific layer which expects to get input of shape (batch * time, ...)
  and converts it into (batch, time, ...), where it recovers the seq-lens from some other layer.
  See :class:`SplitDimsLayer` for a more generic layer.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'split_batch_time',
      'from': source,
      **args,
      **self.get_opts()}


class FlattenBatch(_ConcatInput):
  """
  Merges one axis into the batch axis.
  If the axis has dynamic lengths, this would use flattening,
  i.e. recalculate the padding, i.e. the size changes.
  This basically wraps :func:`flatten_with_seq_len_mask` or :func:`flatten_with_seq_len_mask_time_major`.
  See also :class:`MergeDimsLayer`, which does not do flattening,
  i.e. the size stays the same.
  """

  def __init__(self,
               *,
               axis: str = NotSpecified,
               batch_major: bool = NotSpecified,
               **kwargs):
    """
    :param str axis:
    :param bool batch_major: if False, will flatten in time-major manner
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.batch_major = batch_major

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'batch_major': self.batch_major,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'flatten_batch',
      'from': source,
      **self.get_opts()}


class UnflattenNd(_ConcatInput):
  """
  This keeps the batch axis as-is, i.e. the flattening/unflattening did not happen on the batch axis.

  Example:

    Assumes that the input is of shape (B,T,<Ds>) which represents flattened images,
    where each image is of size width * height.
    We additionally provide these image sizes (shape (B,2)), i.e. (width,height) tuples.
    We return the unflattened images of shape (B,W,H,<Ds>), where W/H are the max width/height.

  This basically wraps :func:`TFUtil.unflatten_nd`.
  """

  def __init__(self,
               *,
               num_axes: int,
               **kwargs):
    """
    :param int num_axes:
    """
    super().__init__(**kwargs)
    self.num_axes = num_axes

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'num_axes': self.num_axes,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      sizes: LayerRef,
                      declare_same_sizes_as: Optional[Dict[int, LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'sizes': sizes,
      'declare_same_sizes_as': declare_same_sizes_as,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'unflatten_nd',
      'from': source,
      **args,
      **self.get_opts()}


class ExpandDims(_ConcatInput):
  """
  Adds some axis.
  """

  def __init__(self,
               *,
               axis: Union[str, int],
               dim: int = NotSpecified,
               **kwargs):
    """
    :param str|int axis: axis to add, e.g. "F"|"feature" or "spatial"|"time"|"T".
      if this is an integer, the input data is first converted into batch-major mode,
      and then this is counted with batch-dim.
    :param int dim: dimension of new axis (1 by default)
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.dim = dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'dim': self.dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'expand_dims',
      'from': source,
      **self.get_opts()}


class Repeat(_ConcatInput):
  """
  A wrapper around tf.repeat, but supports an additional batch axis for the durations
  The sum of the repetitions has to be non-zero for each sequence in the batch.

  This layer can only be used with Tensorflow 1.15.0 or newer.
  """

  def __init__(self,
               *,
               axis: str = NotSpecified,
               **kwargs):
    """
    :param str axis: (dynamic) axis for repetition (currently only time axis is supported)
    """
    super().__init__(**kwargs)
    self.axis = axis

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      repetitions: Union[LayerRef, int],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'repetitions': repetitions,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'repeat',
      'from': source,
      **args,
      **self.get_opts()}


class Tile(_ConcatInput):
  """
  A wrapper around tf.tile
  """

  def __init__(self,
               *,
               multiples: Dict[str, int],
               **kwargs):
    """
    :param dict[str, int] multiples: number of multiples per axis (axis provided as str)
    """
    super().__init__(**kwargs)
    self.multiples = multiples

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'multiples': self.multiples,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'tile',
      'from': source,
      **self.get_opts()}


class Cast(Copy):
  """
  Cast to some other dtype.
  """

  def __init__(self,
               *,
               dtype: str,
               **kwargs):
    """
    :param str dtype:
    """
    super().__init__(**kwargs)
    self.dtype = dtype

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dtype': self.dtype,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'cast',
      'from': source,
      **self.get_opts()}


class SwapAxes(_ConcatInput):
  """
  Swaps two axes. Basically a wrapper around :func:`TFUtil.swapaxes`.
  Note that usually, this should not be needed, and it is recommended not to be used,
  as this will be unnecessarily inefficient.
  Normally, all RETURNN layers will automatically transpose the input data into whatever format they need.

  All axes always have a special meaning (e.g. feature dim or time dim)
  or dimension tag (e.g. for time axes, including dyn seq lengths).
  If you need to change the meaning (and not actually transpose / swap axes),
  you need to use :class:`ReinterpretDataLayer`.

  See also :class:`TransposeLayer` for a more generic variant.

  See also :class:`ReinterpretDataLayer`, which does not swap/transpose axes,
  but allows to reinterpret their meaning / dim tags.
  """

  def __init__(self,
               *,
               axis1: Union[int, str],
               axis2: Union[int, str],
               **kwargs):
    """
    :param int|str axis1:
    :param int|str axis2:
    """
    super().__init__(**kwargs)
    self.axis1 = axis1
    self.axis2 = axis2

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis1': self.axis1,
      'axis2': self.axis2,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'swap_axes',
      'from': source,
      **self.get_opts()}


class Transpose(_ConcatInput):
  """
  Basically a wrapper around :func:`tf.transpose`.
  Note that usually, this should not be needed, and it is recommended not to be used,
  as this will be unnecessarily inefficient.
  Normally, all RETURNN layers will automatically transpose the input data into whatever format they need.

  All axes always have a special meaning (e.g. feature dim or time dim)
  or dimension tag (e.g. for time axes, including dyn seq lengths).
  If you need to change the meaning (and not actually transpose / swap axes),
  you need to use :class:`ReinterpretDataLayer`.

  See also :class:`ReinterpretDataLayer`, which does not transpose axes,
  but allows to reinterpret their meaning / dim tags.
  """

  def __init__(self,
               *,
               perm: Dict[str, str],
               **kwargs):
    """
    :param dict[str,str] perm: target axis -> source axis
    """
    super().__init__(**kwargs)
    self.perm = perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'perm': self.perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'transpose',
      'from': source,
      **self.get_opts()}


class ReinterpretData(_ConcatInput):
  """
  Acts like the :class:`CopyLayer` but reinterprets the role of some axes or data.
  """

  def __init__(self,
               *,
               switch_axes: Any = NotSpecified,
               set_axes: Any = NotSpecified,
               enforce_batch_major: bool = NotSpecified,
               enforce_time_major: bool = NotSpecified,
               set_sparse: Optional[bool] = NotSpecified,
               set_sparse_dim: Union[int, None, NotSpecified] = NotSpecified,
               increase_sparse_dim: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param str|list[str] switch_axes: e.g. "bt" to switch batch and time axes
    :param dict[str,int|str] set_axes:
      This can be used to overwrite the special axes like time_dim_axis or feature_dim_axis.
      For that, use keys "B","T" or "F", and a value via :func:`Data.get_axis_from_description`.
    :param bool enforce_batch_major:
    :param bool enforce_time_major:
    :param bool|None set_sparse: if bool, set sparse value to this
    :param int|None|NotSpecified set_sparse_dim: set sparse dim to this. assumes that it is sparse
    :param int|None increase_sparse_dim: add this to the dim. assumes that it is sparse
    """
    super().__init__(**kwargs)
    self.switch_axes = switch_axes
    self.set_axes = set_axes
    self.enforce_batch_major = enforce_batch_major
    self.enforce_time_major = enforce_time_major
    self.set_sparse = set_sparse
    self.set_sparse_dim = set_sparse_dim
    self.increase_sparse_dim = increase_sparse_dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'switch_axes': self.switch_axes,
      'set_axes': self.set_axes,
      'enforce_batch_major': self.enforce_batch_major,
      'enforce_time_major': self.enforce_time_major,
      'set_sparse': self.set_sparse,
      'set_sparse_dim': self.set_sparse_dim,
      'increase_sparse_dim': self.increase_sparse_dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      size_base: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'size_base': size_base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'reinterpret_data',
      'from': source,
      **args,
      **self.get_opts()}


class Conv(_ConcatInput):
  """
  A generic convolution layer which supports 1D, 2D and 3D convolution.
  Pooling can be done in the separate "pool" layer.
  """

  def __init__(self,
               n_out: int,
               *,
               filter_size: Tuple[int],
               padding: str,
               strides: Any = NotSpecified,
               dilation_rate: Any = NotSpecified,
               groups: int = NotSpecified,
               input_expand_dims: int = NotSpecified,
               input_add_feature_dim: bool = NotSpecified,
               input_split_feature_dim: Optional[int] = NotSpecified,
               auto_use_channel_first: bool = NotSpecified,
               with_bias: Union[bool, NotSpecified] = NotSpecified,
               activation: Optional[str] = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param int n_out: number of outgoing features
    :param tuple[int] filter_size: (width,), (height,width) or (depth,height,width) for 1D/2D/3D conv.
      the input data ndim must match, or you can add dimensions via input_expand_dims or input_add_feature_dim.
      it will automatically swap the batch-dim to the first axis of the input data.
    :param str padding: "same" or "valid"
    :param int|tuple[int] strides: strides for the spatial dims,
      i.e. length of this tuple should be the same as filter_size, or a single int.
    :param int|tuple[int] dilation_rate: dilation for the spatial dims
    :param int groups: grouped convolution
    :param int input_expand_dims: number of dynamic dims to add to the input
    :param bool input_add_feature_dim: will add a dim at the end and use input-feature-dim == 1,
      and use the original input feature-dim as a spatial dim.
    :param None|int input_split_feature_dim: if set, like input_add_feature_dim it will add a new feature dim
      which is of value input_split_feature_dim, and the original input feature dim
      will be divided by input_split_feature_dim, thus it must be a multiple of that value.
    :param bool auto_use_channel_first: convert the input to NCHW or not
    :param bool|NotSpecified with_bias: if True, will add a bias to the output features. False by default
    :param None|str activation: if set, will apply this function at the end
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.filter_size = filter_size
    self.padding = padding
    self.strides = strides
    self.dilation_rate = dilation_rate
    self.groups = groups
    self.input_expand_dims = input_expand_dims
    self.input_add_feature_dim = input_add_feature_dim
    self.input_split_feature_dim = input_split_feature_dim
    self.auto_use_channel_first = auto_use_channel_first
    self.with_bias = with_bias
    self.activation = activation
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'filter_size': self.filter_size,
      'padding': self.padding,
      'strides': self.strides,
      'dilation_rate': self.dilation_rate,
      'groups': self.groups,
      'input_expand_dims': self.input_expand_dims,
      'input_add_feature_dim': self.input_add_feature_dim,
      'input_split_feature_dim': self.input_split_feature_dim,
      'auto_use_channel_first': self.auto_use_channel_first,
      'with_bias': self.with_bias,
      'activation': self.activation,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      filter: Optional[LayerRef] = NotSpecified,
                      bias: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'conv',
      'from': source,
      **args,
      **self.get_opts()}


class Pool(_ConcatInput):
  """
  A generic N-D pooling layer.
  This would usually be done after a convolution for down-sampling.
  """

  def __init__(self,
               *,
               mode: str,
               pool_size: Tuple[int],
               padding: str = NotSpecified,
               dilation_rate: Any = NotSpecified,
               strides: Any = NotSpecified,
               use_channel_first: bool = NotSpecified,
               **kwargs):
    """
    :param str mode: "max" or "avg"
    :param tuple[int] pool_size: shape of the window of each reduce
    :param str padding: "valid" or "same"
    :param tuple[int]|int dilation_rate:
    :param tuple[int]|int|None strides: in contrast to tf.nn.pool, the default (if it is None) will be set to pool_size
    :param bool use_channel_first: if set, will transform input to NCHW format
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.pool_size = pool_size
    self.padding = padding
    self.dilation_rate = dilation_rate
    self.strides = strides
    self.use_channel_first = use_channel_first

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'pool_size': self.pool_size,
      'padding': self.padding,
      'dilation_rate': self.dilation_rate,
      'strides': self.strides,
      'use_channel_first': self.use_channel_first,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'pool',
      'from': source,
      **self.get_opts()}


class Dct(_ConcatInput):
  """
  Layer to perform DCT
  Wraps :func:`tf.signal.dct`. For further documentation on the input arguments, refer to
  https://www.tensorflow.org/api_docs/python/tf/signal/dct
  """

  # noinspection PyShadowingBuiltins
  def __init__(self,
               *,
               type: int = NotSpecified,
               n: Optional[int] = NotSpecified,
               norm: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param int type: DCT type to perform. Must be 1, 2, 3, or 4
    :param int|None n: length of the transform
    :param str|None norm: normalization to apply. Must be None or "ortho"
    """
    super().__init__(**kwargs)
    self.type = type
    self.n = n
    self.norm = norm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'type': self.type,
      'n': self.n,
      'norm': self.norm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dct',
      'from': source,
      **self.get_opts()}


class TransposedConv(_ConcatInput):
  """
  Transposed convolution, sometimes also called deconvolution.
  See :func:`tf.nn.conv2d_transpose` (currently we support 1D/2D).
  """

  def __init__(self,
               n_out: int,
               *,
               filter_size: List[int],
               activation: Optional[str],
               strides: Optional[List[int]] = NotSpecified,
               padding: str = NotSpecified,
               remove_padding: Any = NotSpecified,
               output_padding: Any = NotSpecified,
               with_bias: bool = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param list[int] filter_size:
    :param str|None activation:
    :param list[int]|None strides: specifies the upscaling. by default, same as filter_size
    :param str padding: "same" or "valid"
    :param list[int]|int remove_padding:
    :param list[int|None]|int|None output_padding:
    :param bool with_bias: whether to add a bias. enabled by default.
      Note that the default is different from ConvLayer!
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.filter_size = filter_size
    self.activation = activation
    self.strides = strides
    self.padding = padding
    self.remove_padding = remove_padding
    self.output_padding = output_padding
    self.with_bias = with_bias
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'filter_size': self.filter_size,
      'activation': self.activation,
      'strides': self.strides,
      'padding': self.padding,
      'remove_padding': self.remove_padding,
      'output_padding': self.output_padding,
      'with_bias': self.with_bias,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      filter: Optional[LayerRef] = NotSpecified,
                      bias: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'transposed_conv',
      'from': source,
      **args,
      **self.get_opts()}


class Reduce(_ConcatInput):
  """
  This reduces some axis by using "sum" or "max".
  It's basically a wrapper around tf.reduce_sum or tf.reduce_max.
  """

  def __init__(self,
               *,
               mode: str,
               axes: Any = NotSpecified,
               axis: Any = NotSpecified,
               keep_dims: bool = NotSpecified,
               enforce_batch_dim_axis: int = NotSpecified,
               use_time_mask: bool = NotSpecified,
               **kwargs):
    """
    :param str mode: "sum" or "max", "argmin", "min", "argmax", "mean", "logsumexp"
    :param int|list[int]|str axes: One axis or multiple axis to reduce.
      It accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature",
      and it is strongly recommended to use some of these symbolic names.
      See :func:`Data.get_axes_from_description`.
    :param int|list[int]|str axis: for compatibility, can be used instead of ``axes``
    :param bool keep_dims: if dimensions should be kept (will be 1)
    :param int enforce_batch_dim_axis: will swap the batch-dim-axis of the input with the given axis.
      e.g. 0: will convert the input into batch-major format if not already like that.
      Note that this is still not enough in some cases, e.g. when the other axes are also not as expected.
      The strong recommendation is to use a symbolic axis description.
    :param bool use_time_mask: if we reduce over the time-dim axis, use the seq len info.
      By default, in that case, it will be True.
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.axes = axes
    self.axis = axis
    self.keep_dims = keep_dims
    self.enforce_batch_dim_axis = enforce_batch_dim_axis
    self.use_time_mask = use_time_mask

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'axes': self.axes,
      'axis': self.axis,
      'keep_dims': self.keep_dims,
      'enforce_batch_dim_axis': self.enforce_batch_dim_axis,
      'use_time_mask': self.use_time_mask,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'reduce',
      'from': source,
      **self.get_opts()}


class ReduceOut(_ConcatInput):
  """
  Combination of :class:`SplitDimsLayer` applied to the feature dim
  and :class:`ReduceLayer` applied to the resulting feature dim.
  This can e.g. be used to do maxout.
  """

  def __init__(self,
               *,
               mode: str,
               num_pieces: int,
               **kwargs):
    """
    :param str mode: "sum" or "max" or "mean"
    :param int num_pieces: how many elements to reduce. The output dimension will be input.dim // num_pieces.
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.num_pieces = num_pieces

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'num_pieces': self.num_pieces,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'reduce_out',
      'from': source,
      **self.get_opts()}


class Squeeze(_ConcatInput):
  """
  Removes an axis with dimension 1.
  This is basically a wrapper around tf.squeeze.
  """

  def __init__(self,
               *,
               axis: Any,
               enforce_batch_dim_axis: Optional[int] = NotSpecified,
               allow_no_op: bool = NotSpecified,
               **kwargs):
    """
    :param int|list[int]|str axis: one axis or multiple axis to squeeze.
      this is counted with batch-dim, which by default is axis 0 (see enforce_batch_dim_axis).
      it also accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature"
    :param int|None enforce_batch_dim_axis:
    :param bool allow_no_op:
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.enforce_batch_dim_axis = enforce_batch_dim_axis
    self.allow_no_op = allow_no_op

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'enforce_batch_dim_axis': self.enforce_batch_dim_axis,
      'allow_no_op': self.allow_no_op,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'squeeze',
      'from': source,
      **self.get_opts()}


class Stack(_Base):
  """
  Stacks multiple inputs together using :func:`tf.stack`.
  """

  def __init__(self,
               *,
               axis: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param int|None axis: new axis.
      If not given, will use Data.get_default_new_axis_for_dim_tag(<spatial>),
      i.e. some reasonable default for a new spatial axis.
    """
    super().__init__(**kwargs)
    self.axis = axis

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'stack',
      'from': source,
      **self.get_opts()}


class WeightedSum(_ConcatInput):
  """
  Calculates a weighted sum, either over a complete axis of fixed dimension, or over some window.
  Can also do that for multiple axes.
  The weights are a trainable parameter matrix.
  Similar would be to use :class:`ElemwiseProdLayer` and :class:`ReduceLayer`,
  or just a :class:`DotLayer` with a :class:`VariableLayer`.
  See also :class:`LinearLayer`.
  """

  def __init__(self,
               *,
               axes: Any,
               padding: str = NotSpecified,
               size: Optional[Tuple[int]] = NotSpecified,
               keep_dims: bool = NotSpecified,
               **kwargs):
    """
    :param str|list[str] axes: the axes to do the weighted-sum over
    :param str padding: "valid" or "same", in case of keep_dims=True
    :param None|tuple[int] size: the kernel-size. if left away, the axes must be of fixed dimension,
      and we will use keep_dims=False, padding="valid" by default.
      Otherwise, if given, you must also provide padding and keep_dims=True by default.
    :param bool keep_dims: if False, the axes will be squeezed away. see also `size`.
    """
    super().__init__(**kwargs)
    self.axes = axes
    self.padding = padding
    self.size = size
    self.keep_dims = keep_dims

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axes': self.axes,
      'padding': self.padding,
      'size': self.size,
      'keep_dims': self.keep_dims,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'weighted_sum',
      'from': source,
      **self.get_opts()}


class ElemwiseProd(_ConcatInput):
  """
  Element-wise product in some axes.
  Microsoft calls this "static attention", in Deep Conv. NN with Layer-wise Context Expansion and Attention (LACE).
  The matrix/tensor to be used for the product are given as a trainable parameter.
  See also :class:`LinearLayer`.
  """

  def __init__(self,
               *,
               axes: Any,
               size: Tuple[int] = NotSpecified,
               **kwargs):
    """
    :param str|list[str] axes: e.g. "spatial", but all those axes must be of fixed dimension
    :param tuple[int] size: for double-checking, you can explicitly provide the size
    """
    super().__init__(**kwargs)
    self.axes = axes
    self.size = size

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axes': self.axes,
      'size': self.size,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'elemwise_prod',
      'from': source,
      **self.get_opts()}


class PrefixInTime(_ConcatInput):
  """
  Adds some prefix in time dimension.
  This is kind of the reverse of :class:`SliceNdLayer` does.
  """

  def __init__(self,
               *,
               prefix: Union[float, str] = NotSpecified,
               **kwargs):
    """
    :param float|str prefix: either some constant or another layer
    """
    super().__init__(**kwargs)
    self.prefix = prefix

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'prefix': self.prefix,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      repeat: Union[int, LayerRef] = NotSpecified,
                      size_base: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'repeat': repeat,
      'size_base': size_base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'prefix_in_time',
      'from': source,
      **args,
      **self.get_opts()}


class PostfixInTime(_ConcatInput):
  """
  Adds some postfix in time dimension.
  """

  def __init__(self,
               *,
               repeat: int = NotSpecified,
               **kwargs):
    """
    :param int repeat: how often to repeat the postfix
    """
    super().__init__(**kwargs)
    self.repeat = repeat

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'repeat': self.repeat,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      postfix: Union[float, int, LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'postfix': postfix,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'postfix_in_time',
      'from': source,
      **args,
      **self.get_opts()}


class TimeChunking(_ConcatInput):
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.
  """

  def __init__(self,
               *,
               chunk_size: int,
               chunk_step: int,
               **kwargs):
    """
    :param int chunk_size:
    :param int chunk_step:
    """
    super().__init__(**kwargs)
    self.chunk_size = chunk_size
    self.chunk_step = chunk_step

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'chunk_size': self.chunk_size,
      'chunk_step': self.chunk_step,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'time_chunking',
      'from': source,
      **self.get_opts()}


class TimeUnChunking(_ConcatInput):
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      chunking_layer: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'chunking_layer': chunking_layer,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'time_unchunking',
      'from': source,
      **args,
      **self.get_opts()}


class Dot(_Base):
  """
  This performs a dot-product of two sources.
  The underlying matmul expects shapes (shared..., I, J) * (shared..., J, K) -> (shared..., I, K).
  We say that J is the axis to be reduced,
  I is the var-dim of source 1, and K is the var-dim of source 2.
  I, J, K can also be multiple axes from the sources.
  The var-dims don't need to exist.
  All other axes (shared...) are expected to match.
  """

  def __init__(self,
               *,
               red1: Any = NotSpecified,
               red2: Any = NotSpecified,
               var1: Any = NotSpecified,
               var2: Any = NotSpecified,
               add_var2_if_empty: bool = NotSpecified,
               debug: bool = NotSpecified,
               **kwargs):
    """
    :param str|int|tuple[str|int]|list[str|int] red1: reduce axes of first source
    :param str|int|tuple[str|int]|list[str|int] red2: reduce axes of second source
    :param str|int|tuple[str|int]|list[str|int]|None var1: var axes of first source
    :param str|int|tuple[str|int]|list[str|int]|None var2: var axes of second source
    :param bool add_var2_if_empty: if var2=None, add dim=1 at the end
    :param bool debug: will print debug shapes, etc.
    """
    super().__init__(**kwargs)
    self.red1 = red1
    self.red2 = red2
    self.var1 = var1
    self.var2 = var2
    self.add_var2_if_empty = add_var2_if_empty
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'red1': self.red1,
      'red2': self.red2,
      'var1': self.var1,
      'var2': self.var2,
      'add_var2_if_empty': self.add_var2_if_empty,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dot',
      'from': source,
      **self.get_opts()}


class ShiftAxis(_ConcatInput):
  """
  Shifts the dimensions in an axis around.
  This layer may change the axis-dimension.

  This name might be confusing. No axis will be shifted here. See :class:`SwapAxesLayer` for that.
  """

  def __init__(self,
               *,
               axis: Union[str, int],
               amount: int,
               pad: bool = NotSpecified,
               adjust_size_info: bool = NotSpecified,
               **kwargs):
    """
    :param str|int axis: single axis to shift
    :param int amount: number of elements to shift
                   (<0 for left-shift, >0 for right-shift)
    :param bool pad: preserve shape by padding
    :param bool adjust_size_info: whether to adjust the size_placeholder
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.amount = amount
    self.pad = pad
    self.adjust_size_info = adjust_size_info

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'amount': self.amount,
      'pad': self.pad,
      'adjust_size_info': self.adjust_size_info,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'shift_axis',
      'from': source,
      **self.get_opts()}


class Resize(_ConcatInput):
  """
  Resizes the input, i.e. upsampling or downsampling.
  Supports different kinds, such as linear interpolation or nearest-neighbor.
  """

  def __init__(self,
               *,
               factor: int,
               axis: Union[str, int],
               kind: str = NotSpecified,
               fill_value: Optional[Union[int, float]] = NotSpecified,
               fill_dropout: float = NotSpecified,
               **kwargs):
    """
    :param int factor:
    :param str|int axis: the axis to resize, counted with batch-dim. can also be "T" for time
    :param str kind: "linear", "nn"/"nearest_neighbor", "cubic", "fill"
    :param None|int|float fill_value: if kind=="fill"
    :param float fill_dropout: if set, will dropout in the same axis
    """
    super().__init__(**kwargs)
    self.factor = factor
    self.axis = axis
    self.kind = kind
    self.fill_value = fill_value
    self.fill_dropout = fill_dropout

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'factor': self.factor,
      'axis': self.axis,
      'kind': self.kind,
      'fill_value': self.fill_value,
      'fill_dropout': self.fill_dropout,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'resize',
      'from': source,
      **self.get_opts()}


class CombineDims(MergeDims):
  """
  Combines multiple dimensions.
  See also :class:`MergeDimsLayer`. This is deprecated in favor of :class:`MergeDimsLayer`.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'combine_dims',
      'from': source,
      **self.get_opts()}


class Remove(_Base):
  """
  Currently, assumes sparse data, and removes a specific symbol from the data.

  It is recommended to use :class:`MaskedComputationLayer` in combination with e.g.
  a :class:CompareLayer` instead, as this provides more flexibility.
  """

  def __init__(self,
               *,
               symbol: int,
               **kwargs):
    """
    :param int symbol:
    """
    super().__init__(**kwargs)
    self.symbol = symbol

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'symbol': self.symbol,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'remove',
      'from': source,
      **self.get_opts()}


class Combine(_Base):
  """
  Applies a binary operation, such as addition, to all sources while accumulating the partial results.
  In the first step, the binary operation is performed on the first two sources.
  After the first step, the previous results is always the left-hand operator.

  Its basic working is similar to the `reduce` function used in functional programming.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.
  """

  # noinspection PyShadowingBuiltins
  def __init__(self,
               *,
               kind: str,
               activation: Optional[str] = NotSpecified,
               with_bias: bool = NotSpecified,
               eval: Union[str, callable] = NotSpecified,
               eval_locals: Optional[Dict[str]] = NotSpecified,
               eval_for_output_loss: bool = NotSpecified,
               **kwargs):
    """
    :param str kind:
      currently accepted values are `average`, `add`, `sub`, `mul`, `truediv`, `logical_and`, `logical_or`, or `eval`
    :param str|None activation: if provided, activation function to apply, e.g. "tanh" or "relu"
    :param bool with_bias: if given, will add a trainable bias tensor
    :param str|callable eval: for kind="eval", will eval this string. or function. see :func:`_op_kind_eval`
    :param dict[str]|None eval_locals: locals for eval
    :param bool eval_for_output_loss: will do the same eval on layer.output_loss
    """
    super().__init__(**kwargs)
    self.kind = kind
    self.activation = activation
    self.with_bias = with_bias
    self.eval = eval
    self.eval_locals = eval_locals
    self.eval_for_output_loss = eval_for_output_loss

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'kind': self.kind,
      'activation': self.activation,
      'with_bias': self.with_bias,
      'eval': self.eval,
      'eval_locals': self.eval_locals,
      'eval_for_output_loss': self.eval_for_output_loss,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'combine',
      'from': source,
      **self.get_opts()}


class Eval(Combine):
  """
  Evaluates some string.
  The :class:`CombineLayer` provides this functionality, thus this is just a special case of it.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  The output type is defined as a broadcasted extension of all sources.
  You can overwrite it by (partially) specifying `out_type`.
  `out_type` can also be a generic Python function, returning a `Data` instance.
  """

  # noinspection PyShadowingBuiltins
  def __init__(self,
               *,
               eval: str,
               **kwargs):
    """
    :param str eval: will eval this string. see :func:`_op_kind_eval`
    """
    super().__init__(kind="eval", eval=eval, **kwargs)
    self.eval = eval

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'eval': self.eval,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'eval',
      'from': source,
      **self.get_opts()}


class Compare(_Base):
  """
  Compares element-wise the tokens of all input sequences among themselves and/or with a specified given value.
  The comparisons are performed in a chain according to the order in which they are listed.

  Example::

      {"class": "compare", "from": ["i1", "i2"], "value": val, "kind": "less"}

  computes i1 < i2 < val and it is true only if the whole chain of operations is true.
  The final result is the logical "and" of all comparisons. Note that `value` is the last element to be compared to.

  A common example usage is the `end` layer in a rec subnetwork to specify the stopping criterion,
  e.g. the last generated token is equal to the end-of-sentence token::

      "output": {"class": "rec", "from": [], "unit": {
          .
          .
          .
          "end": {"class": "compare", "from": "output", "value": end_of_sentence_id}
      }, "target": "classes0"}

  """

  def __init__(self,
               *,
               kind: str = NotSpecified,
               value: Optional[Union[float, int]] = NotSpecified,
               **kwargs):
    """
    :param str kind: which comparison operation to use, e.g. "equal", "greater", "less"
      or other supported TF comparison ops
    :param float|int|None value: if specified, will also compare to this
    """
    super().__init__(**kwargs)
    self.kind = kind
    self.value = value

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'kind': self.kind,
      'value': self.value,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'compare',
      'from': source,
      **self.get_opts()}


class Switch(_Base):
  """
  Wrapper around ``tf.where()`` (or more generically :func:`TFUtil.where_bc`),
  or statically choose a single source if the condition is a callable (...)->bool.
  (``tf.cond`` is not useful here, as the sources would have been already constructed and computed.)

  This layer is also useful for applying any kind of generic masking to the frames.
  E.g. one could have a layer called "mask" computing a boolean mask for the values stored in another layer "input".
  Then use this layer with condition="mask", true_from="input", false_from=mask_value,
  to mask out all frames where the mask is false with the mask_value.

  See also :class:`CondLayer`.
  See also :class:`SeqLenMaskLayer` if you just want to mask using the sequence lengths.
  """

  def make_layer_dict(self,
                      *,
                      condition: Union[LayerRef, bool],
                      true_from: Optional[Union[LayerRef, float, int]],
                      false_from: Optional[Union[LayerRef, float, int]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'condition': condition,
      'true_from': true_from,
      'false_from': false_from,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'switch',
      **args,
      **self.get_opts()}


class SearchSorted(_Base):
  """
  Basically wraps :func:`tf.searchsorted`.

  Takes a tensor `sorted_sequence` that is sorted along one axis, and a tensor `values`.
  Will compute an output tensor with the same axes as `values`,
  where each entry is the index of the value within the sorted sequence.
  All (batch) axes of `sorted_sequence` except for the axis it is sorted along must be present in `values`.
  """

  def __init__(self,
               *,
               axis: str = NotSpecified,
               side: str = NotSpecified,
               **kwargs):
    """
    :param str axis: the axis along which `sorted_sequence` is sorted
    :param str side: "left" or "right".
      When one of the `values` exactly matches an element of the `sorted_sequence`,
      whether to choose the lower or higher index.
    """
    super().__init__(**kwargs)
    self.axis = axis
    self.side = side

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'axis': self.axis,
      'side': self.side,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      sorted_sequence: LayerRef,
                      values: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'sorted_sequence': sorted_sequence,
      'values': values,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'search_sorted',
      'from': source,
      **args,
      **self.get_opts()}


class Variable(_Base):
  """
  Represents a variable. Can add batch/time dimension if wanted. Can be trainable.
  See defaults.
  """

  def __init__(self,
               *,
               shape: Any,
               dtype: str = NotSpecified,
               add_batch_axis: bool = NotSpecified,
               add_time_axis: bool = NotSpecified,
               trainable: bool = NotSpecified,
               init: Union[str, float, int] = NotSpecified,
               **kwargs):
    """
    :param tuple[int]|list[int] shape:
    :param str dtype:
    :param bool add_batch_axis:
    :param bool add_time_axis:
    :param bool trainable:
    :param str|float|int init: see :func:`TFUtil.get_initializer`
    """
    super().__init__(trainable=trainable, **kwargs)
    self.shape = shape
    self.dtype = dtype
    self.add_batch_axis = add_batch_axis
    self.add_time_axis = add_time_axis
    self.trainable = trainable
    self.init = init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'shape': self.shape,
      'dtype': self.dtype,
      'add_batch_axis': self.add_batch_axis,
      'add_time_axis': self.add_time_axis,
      'trainable': self.trainable,
      'init': self.init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'variable',
      **self.get_opts()}


class AccumulateMean(Reduce):
  """
  Accumulates the mean of the input (in training) (over batch-dim and time-dim by default).
  It's similar to :class:`ReduceLayer`
  """

  def __init__(self,
               *,
               exp_average: float,
               axes: Any = NotSpecified,
               initial_value: float = NotSpecified,
               is_prob_distribution: bool = NotSpecified,
               **kwargs):
    """
    :param float exp_average: momentum in exponential average calculation
    :param int|list[str]|str axes: the axes to reduce. must contain batch and time.
    :param float initial_value: how to initialize the variable which accumulates the mean
    :param bool is_prob_distribution: if provided, better default for initial_value
    """
    super().__init__(mode="mean", keep_dims=False, axes=axes, **kwargs)
    self.exp_average = exp_average
    self.axes = axes
    self.initial_value = initial_value
    self.is_prob_distribution = is_prob_distribution

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'exp_average': self.exp_average,
      'axes': self.axes,
      'initial_value': self.initial_value,
      'is_prob_distribution': self.is_prob_distribution,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'accumulate_mean',
      'from': source,
      **self.get_opts()}


class Loss(_Base):
  """
  This layers wraps a :class:`Loss` calculation as a layer.
  I.e. the loss will be calculated and returned by the layer.
  But this loss will not be used as a loss by the updater.
  If you want to use it as a loss, you can use the :class:`AsIsLoss`,
  i.e. write ``"loss": "as_is"``.

  Note that the loss options for the wrapped loss need to be provided via ``loss_opts_``,
  and it does not apply any reduce function.

  .. note::

    The ``LossLayer`` might be deprecated in the future in favor of implementing the losses as actual layers.

    If you want to define a loss inside the network, it is recommended to define it explicitly.
    An example could be:

    ``"se_loss": {"class": "eval", "eval": "(source(0) - source(1)) ** 2", "from": ["output", "data:classes"]}``

    Followed by an e.g. mean reduce if needed:

    ``"mse_loss": {"class": "reduce", "mode": "mean", "axis": "F", "from": "se_loss"}``


  """

  def __init__(self,
               *,
               loss_: Loss,
               use_error: bool = NotSpecified,
               **kwargs):
    """
    ``loss_`` and related params have the postfix ``_`` to distinguish them
    from the loss options, which are used by the network and updater for training.
    Some of these (e.g. ``loss_opts_``) are handled in :func:`transform_config_dict`.

    :param Loss loss_:
    :param bool use_error: whether to output the loss error instead of the loss value
    """
    super().__init__(**kwargs)
    self.loss_ = loss_
    self.use_error = use_error

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'loss_': self.loss_,
      'use_error': self.use_error,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      target_: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'target_': target_,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'loss',
      'from': source,
      **args,
      **self.get_opts()}


class ForcedAlignment(_ConcatInput):
  """
  Calculates a forced alignment, via Viterbi algorithm.
  """

  def __init__(self,
               *,
               topology: str,
               input_type: str,
               **kwargs):
    """
    :param str topology: e.g. "ctc" or "rna" (RNA is CTC without label loop)
    :param str input_type: "log_prob" or "prob"
    """
    super().__init__(**kwargs)
    self.topology = topology
    self.input_type = input_type

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'topology': self.topology,
      'input_type': self.input_type,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      align_target: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'align_target': align_target,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'forced_align',
      'from': source,
      **args,
      **self.get_opts()}


class FastBaumWelch(_ConcatInput):
  """
  Calls :func:`fast_baum_welch` or :func:`fast_baum_welch_by_sprint_automata`.
  We expect that our input are +log scores, e.g. use log-softmax.
  """

  def __init__(self,
               *,
               align_target: str,
               align_target_key: Optional[str] = NotSpecified,
               ctc_opts: Dict[str] = NotSpecified,
               sprint_opts: Dict[str] = NotSpecified,
               input_type: str = NotSpecified,
               tdp_scale: float = NotSpecified,
               am_scale: float = NotSpecified,
               min_prob: float = NotSpecified,
               **kwargs):
    """
    :param str align_target: e.g. "sprint" or "staircase"
    :param str|None align_target_key: e.g. "classes", used for e.g. align_target "ctc"
    :param dict[str] ctc_opts: used for align_target "ctc"
    :param dict[str] sprint_opts: used for Sprint (RASR) for align_target "sprint"
    :param str input_type: "log_prob" or "prob"
    :param float tdp_scale:
    :param float am_scale:
    :param float min_prob: clips the minimum prob (value in [0,1])
    """
    super().__init__(**kwargs)
    self.align_target = align_target
    self.align_target_key = align_target_key
    self.ctc_opts = ctc_opts
    self.sprint_opts = sprint_opts
    self.input_type = input_type
    self.tdp_scale = tdp_scale
    self.am_scale = am_scale
    self.min_prob = min_prob

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'align_target': self.align_target,
      'align_target_key': self.align_target_key,
      'ctc_opts': self.ctc_opts,
      'sprint_opts': self.sprint_opts,
      'input_type': self.input_type,
      'tdp_scale': self.tdp_scale,
      'am_scale': self.am_scale,
      'min_prob': self.min_prob,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      staircase_seq_len_source: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'staircase_seq_len_source': staircase_seq_len_source,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'fast_bw',
      'from': source,
      **args,
      **self.get_opts()}


class SyntheticGradient(_ConcatInput):
  """
  This is a generalized way to be able to replace the true gradient with any kind of predicted gradient.
  This enabled to implement the idea from here:
    Decoupled Neural Interfaces using Synthetic Gradients, https://arxiv.org/abs/1608.05343
  """

  def __init__(self,
               *,
               meta_loss_scale: float = NotSpecified,
               **kwargs):
    """
    :param float meta_loss_scale:
    """
    super().__init__(**kwargs)
    self.meta_loss_scale = meta_loss_scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'meta_loss_scale': self.meta_loss_scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      gradient: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'gradient': gradient,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'synthetic_gradient',
      'from': source,
      **args,
      **self.get_opts()}


class TikhonovRegularization(Copy):
  """
  Adds the Tikhonov regularization as a meta-loss (see :class:`TFUtil.MetaLosses`).
  """

  def __init__(self,
               *,
               meta_loss_scale: float = NotSpecified,
               **kwargs):
    """
    :param float meta_loss_scale:
    """
    super().__init__(**kwargs)
    self.meta_loss_scale = meta_loss_scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'meta_loss_scale': self.meta_loss_scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'tikhonov_regularization',
      'from': source,
      **self.get_opts()}


class AllophoneStateIdxParser(_Base):
  """
  This is very much Sprint/RASR specific.
  We get allophone state indices and return (center, left_1, right_1, ..., state, boundary).
  The index is defined by NoTyingDense (ClassicStateTying.cc).
  In the Sprint config, this is via option --*.state-tying.type=no-tying-dense.
  """

  def __init__(self,
               *,
               num_phone_classes: int,
               num_states: int = NotSpecified,
               context_len: int = NotSpecified,
               **kwargs):
    """
    :param int num_phone_classes: number of phonemes + 1, with special 0 phone == no context
    :param int num_states: number of HMM states
    :param int context_len: left/right context len
    """
    super().__init__(**kwargs)
    self.num_phone_classes = num_phone_classes
    self.num_states = num_states
    self.context_len = context_len

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'num_phone_classes': self.num_phone_classes,
      'num_states': self.num_states,
      'context_len': self.context_len,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'allophone_state_idx_parser',
      'from': source,
      **self.get_opts()}


class FramewiseStatistics(_Base):
  """
  Collects various statistics (such as FER, etc) on the sources.
  The tensors will get stored in self.stats which will be collected by TFEngine.
  """

  def __init__(self,
               *,
               sil_label_idx: Any,
               histogram_num_bins: Any = NotSpecified,
               **kwargs):
    """
    :param sil_label_idx:
    :param histogram_num_bins:
    """
    super().__init__(**kwargs)
    self.sil_label_idx = sil_label_idx
    self.histogram_num_bins = histogram_num_bins

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'sil_label_idx': self.sil_label_idx,
      'histogram_num_bins': self.histogram_num_bins,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'framewise_statistics',
      'from': source,
      **self.get_opts()}


class Print(_Base):
  """
  Prints the sources to console/log, via :func:`TFUtil.py_print`.
  """

  def __init__(self,
               *,
               summarize: Optional[int] = NotSpecified,
               extra_print_args: Union[List, Tuple] = NotSpecified,
               **kwargs):
    """
    :param int|None summarize: passed to :func:`py_print`
    :param list|tuple extra_print_args:
    """
    super().__init__(**kwargs)
    self.summarize = summarize
    self.extra_print_args = extra_print_args

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'summarize': self.summarize,
      'extra_print_args': self.extra_print_args,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'print',
      'from': source,
      **self.get_opts()}


class HDFDump(_Base):
  """
  Dumps into HDF file, compatible to :class:`HDFDataset`.

  The HDF will be written to disk under the specified filename, if there was no error,
  by default at graph reset, via :func:`TFNetwork.register_graph_reset_callback`.
  Or after the dataset iteration run loop, with dump_per_run,
  via :func:`TFNetwork.register_run_finished_callback`.

  Common usage would be to add this to your network with "is_output_layer": True,
  such that you don't need to make other layers depend on it.

  It currently uses :class:`SimpleHDFWriter` internally.
  """

  def __init__(self,
               *,
               filename: Any,
               dump_whole_batches: bool = NotSpecified,
               labels: Optional[List[str]] = NotSpecified,
               extend_existing_file: bool = NotSpecified,
               dump_per_run: bool = NotSpecified,
               **kwargs):
    """
    :param str|(()->str) filename:
    :param bool dump_whole_batches: dumps the whole batch as a single sequence into the HDF
    :param list[str]|None labels:
    :param bool extend_existing_file: True also means we expect that it exists
    :param bool dump_per_run: write via :func:`TFNetwork.register_run_finished_callback`
    """
    super().__init__(**kwargs)
    self.filename = filename
    self.dump_whole_batches = dump_whole_batches
    self.labels = labels
    self.extend_existing_file = extend_existing_file
    self.dump_per_run = dump_per_run

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'filename': self.filename,
      'dump_whole_batches': self.dump_whole_batches,
      'labels': self.labels,
      'extend_existing_file': self.extend_existing_file,
      'dump_per_run': self.dump_per_run,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      extra: Optional[Dict[str, LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'extra': extra,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'hdf_dump',
      'from': source,
      **args,
      **self.get_opts()}


class ImageSummary(_Base):
  """
  Creates image summaries which can be viewed in TensorBoard.
  This layer expects the source to be in (T-decoder, T-encoder, B, 1).
  """

  def __init__(self,
               *,
               max_outputs: Any = NotSpecified,
               **kwargs):
    """
    :param max_outputs: number of images to generate per step
    """
    super().__init__(**kwargs)
    self.max_outputs = max_outputs

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'max_outputs': self.max_outputs,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'image_summary',
      'from': source,
      **self.get_opts()}


class Rec(_ConcatInput):
  """
  Recurrent layer, has support for several implementations of LSTMs (via ``unit`` argument),
  see :ref:`tf_lstm_benchmark` (http://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html),
  and also GRU, or simple RNN.
  Via `unit` parameter, you specify the operation/model performed in the recurrence.
  It can be a string and specify a RNN cell, where all TF cells can be used,
  and the `"Cell"` suffix can be omitted; and case is ignored.
  Some possible LSTM implementations are (in all cases for both CPU and GPU):

   * BasicLSTM (the cell), via official TF, pure TF implementation
   * LSTMBlock (the cell), via tf.contrib.rnn.
   * LSTMBlockFused, via tf.contrib.rnn. should be much faster than BasicLSTM
   * CudnnLSTM, via tf.contrib.cudnn_rnn. This is experimental yet.
   * NativeLSTM, our own native LSTM. should be faster than LSTMBlockFused.
   * NativeLstm2, improved own native LSTM, should be the fastest and most powerful.

  We default to the current tested fastest one, i.e. NativeLSTM.
  Note that they are currently not compatible to each other, i.e. the way the parameters are represented.

  A subnetwork can also be given which will be evaluated step-by-step,
  which can use attention over some separate input,
  which can be used to implement a decoder in a sequence-to-sequence scenario.
  The subnetwork will get the extern data from the parent net as templates,
  and if there is input to the RecLayer,
  then it will be available as the "source" data key in the subnetwork.
  The subnetwork is specified as a `dict` for the `unit` parameter.
  In the subnetwork, you can access outputs from layers from the previous time step when they
  are referred to with the "prev:" prefix.

  Example::

      {
          "class": "rec",
          "from": "input",
          "unit": {
            # Recurrent subnet here, operate on a single time-step:
            "output": {
              "class": "linear",
              "from": ["prev:output", "data:source"],
              "activation": "relu",
              "n_out": n_out},
          },
          "n_out": n_out},
      }

  More examples can be seen in :mod:`test_TFNetworkRecLayer` and :mod:`test_TFEngine`.

  The subnetwork can automatically optimize the inner recurrent loop
  by moving layers out of the loop if possible.
  It will try to do that greedily. This can be disabled via the option `optimize_move_layers_out`.
  It assumes that those layers behave the same with time-dimension or without time-dimension and used per-step.
  Examples for such layers are :class:`LinearLayer`, :class:`RnnCellLayer`
  or :class:`SelfAttentionLayer` with option `attention_left_only`.

  This layer can also be inside another RecLayer. In that case, it behaves similar to :class:`RnnCellLayer`.
  (This support is somewhat incomplete yet. It should work for the native units such as NativeLstm.)

  Also see :ref:`recurrency`.
  """

  def __init__(self,
               n_out: int,
               *,
               unit: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               direction: Optional[int] = NotSpecified,
               input_projection: bool = NotSpecified,
               max_seq_len: Optional[Union[str, int]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               optimize_move_layers_out: Optional[bool] = NotSpecified,
               cheating: bool = NotSpecified,
               unroll: bool = NotSpecified,
               back_prop: Optional[bool] = NotSpecified,
               use_global_rec_step_offset: bool = NotSpecified,
               include_eos: bool = NotSpecified,
               debug: Optional[bool] = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param str|_SubnetworkRecCell unit: the RNNCell/etc name, e.g. "nativelstm". see comment below.
      alternatively a whole subnetwork, which will be executed step by step,
      and which can include "prev" in addition to "from" to refer to previous steps.
      The subnetwork is specified as a net dict in the config.
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param int|None direction: None|1 -> forward, -1 -> backward
    :param bool input_projection: True -> input is multiplied with matrix. False only works if same input dim
    :param int|tf.Tensor|None max_seq_len: if unit is a subnetwork. str will be evaluated. see code
    :param str forward_weights_init: see :func:`TFUtil.get_initializer`
    :param str recurrent_weights_init: see :func:`TFUtil.get_initializer`
    :param str bias_init: see :func:`TFUtil.get_initializer`
    :param bool|None optimize_move_layers_out: will automatically move layers out of the loop when possible
    :param bool cheating: Unused, is now part of ChoiceLayer
    :param bool unroll: if possible, unroll the loop (implementation detail)
    :param bool|None back_prop: for tf.while_loop. the default will use self.network.train_flag
    :param bool use_global_rec_step_offset:
    :param bool include_eos: for search, whether we should include the frame where "end" is True
    :param bool|None debug:
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.unit = unit
    self.unit_opts = unit_opts
    self.direction = direction
    self.input_projection = input_projection
    self.max_seq_len = max_seq_len
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init
    self.optimize_move_layers_out = optimize_move_layers_out
    self.cheating = cheating
    self.unroll = unroll
    self.back_prop = back_prop
    self.use_global_rec_step_offset = use_global_rec_step_offset
    self.include_eos = include_eos
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'unit': self.unit,
      'unit_opts': self.unit_opts,
      'direction': self.direction,
      'input_projection': self.input_projection,
      'max_seq_len': self.max_seq_len,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
      'optimize_move_layers_out': self.optimize_move_layers_out,
      'cheating': self.cheating,
      'unroll': self.unroll,
      'back_prop': self.back_prop,
      'use_global_rec_step_offset': self.use_global_rec_step_offset,
      'include_eos': self.include_eos,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]] = (),
                      *,
                      initial_state: Optional[Union[LayerRef, str, float, int, Tuple]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': initial_state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'rec',
      'from': source,
      **args,
      **self.get_opts()}


class RnnCell(_ConcatInput):
  """
  Wrapper around tf.contrib.rnn.RNNCell.
  This will operate a single step, i.e. there is no time dimension,
  i.e. we expect a (batch,n_in) input, and our output is (batch,n_out).
  This is expected to be used inside a RecLayer.
  (But it can also handle the case to be optimized out of the rec loop,
   i.e. outside a RecLayer, with a time dimension.)
  """

  def __init__(self,
               n_out: int,
               *,
               unit: str,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               initial_output: None = NotSpecified,
               weights_init: Any = NotSpecified,
               **kwargs):
    """
    :param int n_out: so far, only output shape (batch,n_out) supported
    :param str|tf.contrib.rnn.RNNCell unit: e.g. "BasicLSTM" or "LSTMBlock"
    :param dict[str]|None unit_opts: passed to the cell.__init__
    :param None initial_output: the initial output is defined implicitly via initial state, thus don't set this
    :param weights_init:
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.unit = unit
    self.unit_opts = unit_opts
    self.initial_output = initial_output
    self.weights_init = weights_init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'unit': self.unit,
      'unit_opts': self.unit_opts,
      'initial_output': self.initial_output,
      'weights_init': self.weights_init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      initial_state: Any = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': initial_state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'rnn_cell',
      'from': source,
      **args,
      **self.get_opts()}


class GetLastHiddenState(_Base):
  """
  Will combine (concat or add or so) all the last hidden states from all sources.
  """

  def __init__(self,
               *,
               combine: str = NotSpecified,
               key: Optional[Union[str, int]] = NotSpecified,
               **kwargs):
    """
    :param str combine: "concat" or "add"
    :param str|int|None key: for the state, which could be a namedtuple. see :func:`RnnCellLayer.get_state_by_key`
    """
    super().__init__(**kwargs)
    self.combine = combine
    self.key = key

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'combine': self.combine,
      'key': self.key,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'get_last_hidden_state',
      'from': source,
      **self.get_opts()}


class GetRecAccumulatedOutput(_Base):
  """
  For :class:`RecLayer` with a subnet.
  If some layer is explicitly marked as an additional output layer (via 'is_output_layer': True),
  you can get that subnet layer output via this accessor.
  Retrieves the accumulated output.

  Note that this functionality is obsolete now. You can simply access such an sub layer
  via the generic sub layer access mechanism. I.e. instead of::

    "sub_layer": {"class": "get_rec_accumulated", "from": "rec_layer", "sub_layer": "hidden"}

  You can do::

    "sub_layer": {"class": "copy", "from": "rec_layer/hidden"}
  """

  def __init__(self,
               *,
               sub_layer: str,
               **kwargs):
    """
    :param str sub_layer: layer of subnet in RecLayer source, which has 'is_output_layer': True
    """
    super().__init__(**kwargs)
    self.sub_layer = sub_layer

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'sub_layer': self.sub_layer,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'get_rec_accumulated',
      'from': source,
      **self.get_opts()}


class BaseChoice(_Base):
  """
  This is a base-class for any layer which defines a new search choice,
  i.e. which defines ``self.search_choices``.
  """

  def __init__(self,
               *,
               beam_size: Optional[int],
               search: Union[NotSpecified, bool] = NotSpecified,
               **kwargs):
    """
    :param int|None beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
    :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
      If not specified, it will depend on `network.search_flag`.
    """
    super().__init__(**kwargs)
    self.beam_size = beam_size
    self.search = search

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'beam_size': self.beam_size,
      'search': self.search,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Choice(BaseChoice):
  """
  This layer represents a choice to be made in search during inference,
  such as choosing the top-k outputs from a log-softmax for beam search.
  During training, this layer can return the true label.
  This is supposed to be used inside the rec layer.
  This can be extended in various ways.

  We present the scores in +log space, and we will add them up along the path.
  Assume that we get input (batch,dim) from a (log-)softmax.
  Assume that each batch is already a choice via search.
  In search with a beam size of N, we would output
  sparse (batch=N,) and scores for each.

  In case of multiple sources, this layer computes the top-k combinations of choices. The score of such a combination
  is determined by adding up the (log-space) scores of the choices for the individual sources. In this case, the
  'target' parameter of the layer has to be set to a list of targets corresponding to the sources respectively. Because
  computing all possible combinations of source scores is costly, the sources are pruned beforehand using the beam
  sizes set by the 'source_beam_sizes' parameter. The choices made for the different sources can be accessed via the
  sublayers '<choice layer name>/out_0', '<choice layer name>/out_1' and so on.
  Note, that the way scores are combined assumes the sources to be independent. If you want to model a dependency,
  use separate ChoiceLayers and let the input of one depend on the output of the other.
  """

  def __init__(self,
               *,
               beam_size: int,
               keep_beams: bool = NotSpecified,
               search: Union[NotSpecified, bool] = NotSpecified,
               input_type: str = NotSpecified,
               prob_scale: float = NotSpecified,
               base_beam_score_scale: float = NotSpecified,
               random_sample_scale: float = NotSpecified,
               length_normalization: bool = NotSpecified,
               length_normalization_exponent: Any = NotSpecified,
               custom_score_combine: Optional[callable] = NotSpecified,
               source_beam_sizes: Optional[List[int]] = NotSpecified,
               scheduled_sampling: Optional[Dict] = NotSpecified,
               cheating: Union[bool, str] = NotSpecified,
               **kwargs):
    """
    :param int beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
    :param bool keep_beams: specifies that we keep the beam_in entries,
      i.e. we just expand, i.e. we just search on the dim. beam_size must be a multiple of beam_in.
    :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
      If not specified, it will depend on `network.search_flag`.
    :param str input_type: "prob" or "log_prob", whether the input is in probability space, log-space, etc.
      or "regression", if it is a prediction of the data as-is. If there are several inputs, same format
      for all is assumed.
    :param float prob_scale: factor for prob (score in +log space from source)
    :param float base_beam_score_scale: factor for beam base score (i.e. prev prob scores)
    :param float random_sample_scale: if >0, will add Gumbel scores. you might want to set base_beam_score_scale=0
    :param bool length_normalization: evaluates score_t/len in search
    :param length_normalization_exponent:
    :param callable|None custom_score_combine:
    :param list[int]|None source_beam_sizes: If there are several sources, they are pruned with these beam sizes
       before combination. If None, 'beam_size' is used for all sources. Has to have same length as number of sources.
    :param dict|None scheduled_sampling:
    :param bool|str cheating: if True, will always add the true target in the beam.
      if "exclusive", enables cheating_exclusive. see :func:`TFUtil.beam_search`.
    """
    super().__init__(beam_size=beam_size, search=search, **kwargs)
    self.beam_size = beam_size
    self.keep_beams = keep_beams
    self.search = search
    self.input_type = input_type
    self.prob_scale = prob_scale
    self.base_beam_score_scale = base_beam_score_scale
    self.random_sample_scale = random_sample_scale
    self.length_normalization = length_normalization
    self.length_normalization_exponent = length_normalization_exponent
    self.custom_score_combine = custom_score_combine
    self.source_beam_sizes = source_beam_sizes
    self.scheduled_sampling = scheduled_sampling
    self.cheating = cheating

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'beam_size': self.beam_size,
      'keep_beams': self.keep_beams,
      'search': self.search,
      'input_type': self.input_type,
      'prob_scale': self.prob_scale,
      'base_beam_score_scale': self.base_beam_score_scale,
      'random_sample_scale': self.random_sample_scale,
      'length_normalization': self.length_normalization,
      'length_normalization_exponent': self.length_normalization_exponent,
      'custom_score_combine': self.custom_score_combine,
      'source_beam_sizes': self.source_beam_sizes,
      'scheduled_sampling': self.scheduled_sampling,
      'cheating': self.cheating,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      target: LayerRef,
                      explicit_search_sources: Optional[List[LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'target': target,
      'explicit_search_sources': explicit_search_sources,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'choice',
      'from': source,
      **args,
      **self.get_opts()}


class Decide(BaseChoice):
  """
  This is kind of the counter-part to the choice layer.
  This only has an effect in search mode.
  E.g. assume that the input is of shape (batch * beam, time, dim)
  and has search_sources set.
  Then this will output (batch, time, dim) where the beam with the highest score is selected.
  Thus, this will do a decision based on the scores.
  In will convert the data to batch-major mode.
  """

  def __init__(self,
               *,
               length_normalization: bool = NotSpecified,
               **kwargs):
    """
    :param bool length_normalization: performed on the beam scores
    """
    super().__init__(beam_size=1, **kwargs)
    self.length_normalization = length_normalization

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'length_normalization': self.length_normalization,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'decide',
      'from': source,
      **self.get_opts()}


class DecideKeepBeam(BaseChoice):
  """
  This just marks the search choices as decided, but does not change them (in contrast to :class:`DecideLayer`).
  You can use this to get out some values as-is, without having them resolved to the final choices.

  For internal usage only.
  """

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'decide_keep_beam',
      'from': source,
      **self.get_opts()}


class ChoiceGetBeamScores(_Base):
  """
  Gets beam scores from :class:`SearchChoices`.
  This requires that the source has search choices.

  .. note::

    This layer might be deprecated in the future.

  """

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'choice_get_beam_scores',
      'from': source,
      **self.get_opts()}


class ChoiceGetSrcBeams(_Base):
  """
  Gets source beam indices from :class:`SearchChoices`.
  This requires that the source has search choices.
  """

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'choice_get_src_beams',
      'from': source,
      **self.get_opts()}


class AttentionBase(_ConcatInput):
  """
  This is the base class for attention.
  This layer would get constructed in the context of one single decoder step.
  We get the whole encoder output over all encoder frames (the base), e.g. (batch,enc_time,enc_dim),
  and some current decoder context, e.g. (batch,dec_att_dim),
  and we are supposed to return the attention output, e.g. (batch,att_dim).

  Some sources:
  * Bahdanau, Bengio, Montreal, Neural Machine Translation by Jointly Learning to Align and Translate, 2015,
    https://arxiv.org/abs/1409.0473
  * Luong, Stanford, Effective Approaches to Attention-based Neural Machine Translation, 2015,
    https://arxiv.org/abs/1508.04025
    -> dot, general, concat, location attention; comparison to Bahdanau
  * https://github.com/ufal/neuralmonkey/blob/master/neuralmonkey/decoders/decoder.py
  * https://google.github.io/seq2seq/
    https://github.com/google/seq2seq/blob/master/seq2seq/contrib/seq2seq/decoder.py
    https://github.com/google/seq2seq/blob/master/seq2seq/decoders/attention_decoder.py
  * https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/attention.py
  """

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class GlobalAttentionContextBase(AttentionBase):
  """
  Base class for other attention types, which use a global context.
  """

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class GenericAttention(AttentionBase):
  """
  The weighting for the base is specified explicitly here.
  This can e.g. be used together with :class:`SoftmaxOverSpatialLayer`.
  Note that we do not do any masking here. E.g. :class:`SoftmaxOverSpatialLayer` does that.

  Note that :class:`DotLayer` is similar, just using a different terminology.
  Reduce axis: weights: time-axis; base: time-axis.
    Note that if the last layer was :class:`SoftmaxOverSpatialLayer`, we should use the same time-axis.
    Also we should do a check whether these time axes really match.
  Common axes (should match): batch-axis, all from base excluding base feature axis and excluding time axis.
  Keep axes: base: feature axis; weights: all remaining, e.g. extra time.
  """

  def __init__(self,
               *,
               auto_squeeze: bool = NotSpecified,
               **kwargs):
    """
    :param bool auto_squeeze: auto-squeeze any weight-axes with dim=1 away
    """
    super().__init__(**kwargs)
    self.auto_squeeze = auto_squeeze

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'auto_squeeze': self.auto_squeeze,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      *,
                      weights: LayerRef,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'weights': weights,
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'generic_attention',
      **args,
      **self.get_opts()}


class DotAttention(GlobalAttentionContextBase):
  """
  Classic global attention: Dot-product as similarity measure between base_ctx and source.
  """

  def __init__(self,
               *,
               energy_factor: Optional[float] = NotSpecified,
               **kwargs):
    """
    :param float|None energy_factor: the energy will be scaled by this factor.
      This is like a temperature for the softmax.
      In Attention-is-all-you-need, this is set to 1/sqrt(base_ctx.dim).
    """
    super().__init__(**kwargs)
    self.energy_factor = energy_factor

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'energy_factor': self.energy_factor,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      base_ctx: LayerRef,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'base_ctx': base_ctx,
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'dot_attention',
      'from': source,
      **args,
      **self.get_opts()}


class ConcatAttention(GlobalAttentionContextBase):
  """
  Additive attention / tanh-concat attention as similarity measure between base_ctx and source.
  This is used by Montreal, where as Stanford compared this to the dot-attention.
  The concat-attention is maybe more standard for machine translation at the moment.
  """

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      base_ctx: LayerRef,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'base_ctx': base_ctx,
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'concat_attention',
      'from': source,
      **args,
      **self.get_opts()}


class GaussWindowAttention(AttentionBase):
  """
  Interprets the incoming source as the location (float32, shape (batch,))
  and returns a gauss-window-weighting of the base around the location.
  The window size is fixed (TODO: but the variance can optionally be dynamic).
  """

  def __init__(self,
               *,
               window_size: int,
               std: float = NotSpecified,
               inner_size: Optional[int] = NotSpecified,
               inner_size_step: float = NotSpecified,
               **kwargs):
    """
    :param int window_size: the window size where the Gaussian window will be applied on the base
    :param float std: standard deviation for Gauss
    :param int|None inner_size: if given, the output will have an additional dimension of this size,
      where t is shifted by +/- inner_size_step around.
      e.g. [t-1,t-0.5,t,t+0.5,t+1] would be the locations with inner_size=5 and inner_size_step=0.5.
    :param float inner_size_step: see inner_size above
    """
    super().__init__(**kwargs)
    self.window_size = window_size
    self.std = std
    self.inner_size = inner_size
    self.inner_size_step = inner_size_step

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'window_size': self.window_size,
      'std': self.std,
      'inner_size': self.inner_size,
      'inner_size_step': self.inner_size_step,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'gauss_window_attention',
      'from': source,
      **args,
      **self.get_opts()}


class SelfAttention(_ConcatInput):
  """
  Applies self-attention on the input. I.e., with input `x`,
  it will basically calculate

      att(Q x, K x, V x),

  where `att` is multi-head dot-attention for now, `Q`, `K`, `V` are matrices.
  The attention will be over the time-dimension.
  If there is no time-dimension, we expect to be inside a :class:`RecLayer`;
  also, this is only valid with `attention_to_past_only=True`.

  See also `dot_product_attention` here:
    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py
  """

  def __init__(self,
               *,
               num_heads: int,
               total_key_dim: int,
               forward_weights_init: str = NotSpecified,
               attention_dropout: float = NotSpecified,
               attention_left_only: bool = NotSpecified,
               initial_state: Optional[Union[str, float, int]] = NotSpecified,
               restrict_state_to_last_seq: bool = NotSpecified,
               state_var_lengths: Any = NotSpecified,
               **kwargs):
    """
    :param int num_heads:
    :param int total_key_dim: i.e. key_dim == total_key_dim // num_heads
    :param str forward_weights_init: see :func:`TFUtil.get_initializer`
    :param float attention_dropout:
    :param bool attention_left_only: will mask out the future. see Attention is all you need.
    :param str|float|int|None initial_state: see RnnCellLayer.get_rec_initial_state_inner().
    :param bool restrict_state_to_last_seq: see code comment below
    :param None|tf.Tensor|()->tf.Tensor state_var_lengths:
      if passed, a Tensor containing the number of keys in the state_var for
      each batch-entry, used for decoding in RASR.
    """
    super().__init__(**kwargs)
    self.num_heads = num_heads
    self.total_key_dim = total_key_dim
    self.forward_weights_init = forward_weights_init
    self.attention_dropout = attention_dropout
    self.attention_left_only = attention_left_only
    self.initial_state = initial_state
    self.restrict_state_to_last_seq = restrict_state_to_last_seq
    self.state_var_lengths = state_var_lengths

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'num_heads': self.num_heads,
      'total_key_dim': self.total_key_dim,
      'forward_weights_init': self.forward_weights_init,
      'attention_dropout': self.attention_dropout,
      'attention_left_only': self.attention_left_only,
      'initial_state': self.initial_state,
      'restrict_state_to_last_seq': self.restrict_state_to_last_seq,
      'state_var_lengths': self.state_var_lengths,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      key_shift: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'key_shift': key_shift,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'self_attention',
      'from': source,
      **args,
      **self.get_opts()}


class PositionalEncoding(_ConcatInput):
  """
  Provides positional encoding in the form of (batch, time, n_out) or (time, batch, n_out)
  where n_out is the number of channels, if it is run outside a :class:`RecLayer`,
  and (batch, n_out) or (n_out, batch)
  if run inside a :class:`RecLayer`, where it will depend on the current time frame.

  Assumes one source input with a time dimension if outside a :class:`RecLayer`.
  With `add_to_input`, it will calculate `x + input`, and the output shape is the same as the input

  The positional encoding is the same as in Tensor2Tensor.
  See :func:`TFUtil.get_positional_encoding`.
  """

  def __init__(self,
               n_out: int,
               *,
               add_to_input: bool = NotSpecified,
               constant: int = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param bool add_to_input: will add the signal to the input
    :param int constant: if positive, always output the corresponding positional encoding.
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.add_to_input = add_to_input
    self.constant = constant

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'add_to_input': self.add_to_input,
      'constant': self.constant,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      offset: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'offset': offset,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'positional_encoding',
      'from': source,
      **args,
      **self.get_opts()}


class KenLmState(_ConcatInput):
  """
  Get next word (or subword) each frame,
  accumulates string,
  keeps state of seen string so far,
  returns score (+log space, natural base e) of sequence,
  using KenLM (http://kheafield.com/code/kenlm/) (see :mod:`TFKenLM`).
  EOS (</s>) token must be used explicitly.
  """

  def __init__(self,
               *,
               lm_file: Any,
               vocab_file: Optional[str] = NotSpecified,
               vocab_unknown_label: str = NotSpecified,
               bpe_merge_symbol: Optional[str] = NotSpecified,
               input_step_offset: int = NotSpecified,
               dense_output: bool = NotSpecified,
               debug: bool = NotSpecified,
               **kwargs):
    """
    :param str|()->str lm_file: ARPA file or so. whatever KenLM supports
    :param str|None vocab_file: if the inputs are symbols, this must be provided. see :class:`Vocabulary`
    :param str vocab_unknown_label: for the vocabulary
    :param str|None bpe_merge_symbol: e.g. "@@" if you want to apply BPE merging
    :param int input_step_offset: if provided, will consider the input only from this step onwards
    :param bool dense_output: whether we output the score for all possible succeeding tokens
    :param bool debug: prints debug info
    """
    super().__init__(**kwargs)
    self.lm_file = lm_file
    self.vocab_file = vocab_file
    self.vocab_unknown_label = vocab_unknown_label
    self.bpe_merge_symbol = bpe_merge_symbol
    self.input_step_offset = input_step_offset
    self.dense_output = dense_output
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'lm_file': self.lm_file,
      'vocab_file': self.vocab_file,
      'vocab_unknown_label': self.vocab_unknown_label,
      'bpe_merge_symbol': self.bpe_merge_symbol,
      'input_step_offset': self.input_step_offset,
      'dense_output': self.dense_output,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'kenlm',
      'from': source,
      **self.get_opts()}


class EditDistanceTable(_Base):
  """
  Given a source and a target, calculates the edit distance table between them.
  Source can be inside a recurrent loop.
  It uses :func:`TFNativeOp.next_edit_distance_row`.

  Usually, if you are inside a rec layer, and "output" is the :class:`ChoiceLayer`,
  you would use "from": "output"
  and "target": "layer:base:data:target" (make sure it has the time dimension).

  See also :class:`OptimalCompletionsLayer`.
  """

  def __init__(self,
               *,
               debug: bool = NotSpecified,
               blank_idx: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param bool debug:
    :param int|None blank_idx: if given, will keep the same row for this source label
    """
    super().__init__(**kwargs)
    self.debug = debug
    self.blank_idx = blank_idx

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'debug': self.debug,
      'blank_idx': self.blank_idx,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'edit_distance_table',
      'from': source,
      **self.get_opts()}


class OptimalCompletions(_Base):
  """
  We expect to get the inputs from :class:`EditDistanceTableLayer`, esp from the prev frame, like this:
  "opt_completions": {"class": "optimal_completions", "from": "prev:edit_dist_table"}.

  You can also then define this further layer:
  "opt_completion_soft_targets": {
    "class": "eval", "eval": "tf.nn.softmax(tf.cast(source(0), tf.float32))",
    "from": "opt_completions", "out_type": {"dtype": "float32"}},
  and use that as the :class:`CrossEntropyLoss` soft targets
  for the input of the "output" :class:`ChoiceLayer`, e.g. "output_prob".
  This makes most sense when you enable beam search (even, or esp, during training).
  Note that you probably want to have this all before the last choice, where you still have more beams open.
  """

  def __init__(self,
               *,
               debug: bool = NotSpecified,
               blank_idx: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param bool debug:
    :param int|None blank_idx:
    """
    super().__init__(**kwargs)
    self.debug = debug
    self.blank_idx = blank_idx

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'debug': self.debug,
      'blank_idx': self.blank_idx,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'optimal_completions',
      'from': source,
      **self.get_opts()}


class Unmask(_Base):
  """
  This is meant to be used together with :class:`MaskedComputationLayer`,
  which operates on input [B,T,D], and given a mask, returns [B,T',D'].
  This layer :class:`UnmaskLayer` is supposed to undo the masking,
  i.e. to recover the original time dimension, i.e. given [B,T',D'], we output [B,T,D'].
  This is done by repeating the output for the non-masked frames,
  via the last masked frame.

  If this layer is inside a recurrent loop, i.e. we get [B,D'] as input,
  this is a no-op, and we just return the input as is.
  In that case, the repetition logic is handled via :class:`MaskedComputationLayer`.
  """

  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      mask: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'mask': mask,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'unmask',
      'from': source,
      **args,
      **self.get_opts()}


class TwoDLSTM(_Base):
  """
  2D LSTM.

  Currently only from left-to-right in the time axis.
  Can be inside a recurrent loop, or outside.
  """

  def __init__(self,
               *,
               pooling: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               **kwargs):
    """
    :param str pooling: defines how the 1D return value is computed based on the 2D lstm result. Either 'last' or 'max'
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param str forward_weights_init: see :func:`TFUtil.get_initializer`
    :param str recurrent_weights_init: see :func:`TFUtil.get_initializer`
    :param str bias_init: see :func:`TFUtil.get_initializer`
    """
    super().__init__(**kwargs)
    self.pooling = pooling
    self.unit_opts = unit_opts
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'pooling': self.pooling,
      'unit_opts': self.unit_opts,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'twod_lstm',
      'from': source,
      **self.get_opts()}


class RelativePositionalEncoding(_ConcatInput):
  """
  Relative positioning term as introduced by Shaw et al., 2018

  Usually added to Self-Attention using key_shift.
  Parts of the code are adapted from Tensor2Tensor (https://github.com/tensorflow/tensor2tensor).

  Example usage::

      d[output + '_rel_pos'] = {"class": "relative_positional_encoding",
                                "from": [output + '_self_att_laynorm'],
                                "n_out": self.EncKeyTotalDim // self.AttNumHeads,
                                "forward_weights_init": self.ff_init}
      d[output + '_self_att_att'] = {"class": "self_attention",
                                     "num_heads": self.AttNumHeads,
                                     "total_key_dim": self.EncKeyTotalDim,
                                     "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                     "attention_left_only": False, "attention_dropout": self.attention_dropout,
                                     "forward_weights_init": self.ff_init,
                                     "key_shift": output + '_rel_pos'}

  """

  def __init__(self,
               n_out: int,
               *,
               forward_weights_init: str = NotSpecified,
               clipping: int = NotSpecified,
               fixed: bool = NotSpecified,
               **kwargs):
    """
    :param int n_out: Feature dimension of encoding.
    :param str forward_weights_init: see :func:`TFUtil.get_initializer`
    :param int clipping: After which distance to fallback to the last encoding
    :param bool fixed: Uses sinusoid positional encoding instead of learned parameters
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.forward_weights_init = forward_weights_init
    self.clipping = clipping
    self.fixed = fixed

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'forward_weights_init': self.forward_weights_init,
      'clipping': self.clipping,
      'fixed': self.fixed,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'relative_positional_encoding',
      'from': source,
      **self.get_opts()}
