"""
This file is auto-generated by _generate_layers.py.
RETURNN: 1.20211024.182640+git.8b24699

These are the RETURNN layers directly wrapped.
Note that we intentionally exclude some layers or options for more consistency.
Please file an issue if you miss something.
"""

from __future__ import annotations
from typing import Union, Optional, Tuple, List, Dict, Any
from returnn.util.basic import NotSpecified
from returnn.tf.util.basic import DimensionTag
from .base import NameCtx, ILayerMaker, _ReturnnWrappedLayerBase, Layer, LayerRef, LayerDictRaw


class _Base(_ReturnnWrappedLayerBase):
  """
  This is the base class for all layers.
  Every layer by default has a list of source layers `sources` and defines `self.output` which is of type :class:`Data`.
  It shares some common functionality across all layers, such as explicitly defining the output format,
  some parameter regularization, and more.

  If you want to implement your own layer::

      class YourOwnLayer(_ConcatInputLayer):  # e.g. either _ConcatInputLayer or LayerBase as a base
          " some docstring "
          layer_class = "your_layer_name"

          def __init__(self, your_kwarg1, your_opt_kwarg2=None, **kwargs):
              " docstring, document the args! "
              super(YourOwnLayer, self).__init__(**kwargs)
              # Now we need to set self.output, which must be of type :class:`Data`.
              # It is set at this point to whatever we got from `self.get_out_data_from_opts()`,
              # so it is enough if we set self.output.placeholder and self.output.size_placeholder,
              # but we could also reset self.output.
              self.output.placeholder = self.input_data.placeholder + 42  # whatever you want to do
              # If you don't modify the sizes (e.g. sequence-length), just copy the input sizes.
              self.output.size_placeholder = self.input_data.size_placeholder.copy()

          @classmethod
          def get_out_data_from_opts(cls, **kwargs):
              " This is supposed to return a :class:`Data` instance as a template, given the arguments. "
              # example, just the same as the input:
              return get_concat_sources_data_template(kwargs["sources"], name="%s_output" % kwargs["name"])

  """
  returnn_layer_class = None
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               param_device: Optional[str] = NotSpecified,
               only_on_eval: bool = NotSpecified,
               only_on_search: bool = NotSpecified,
               copy_output_loss_from_source_idx: Optional[int] = NotSpecified,
               l2: Optional[float] = NotSpecified,
               darc1: Optional[float] = NotSpecified,
               spatial_smoothing: Optional[float] = NotSpecified,
               param_variational_noise: Optional[float] = NotSpecified,
               updater_opts: Optional[Dict[str]] = NotSpecified,
               initial_output: Union[str, float] = NotSpecified,
               collocate_with: Optional[List[str]] = NotSpecified,
               trainable: Optional[bool] = NotSpecified,
               custom_param_importer: Optional[Union[str, callable]] = NotSpecified,
               register_as_extern_data: Optional[str] = NotSpecified,
               ):
    """
    Usually the arguments, when specified in the network dict,
    are going through :func:`transform_config_dict`, before they are passed to here.
    See :func:`TFNetwork.construct_from_dict`.

    :param str|None param_device: e.g. "CPU", etc. any valid name for tf.device.
      see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/device_name_utils.h
    :param bool only_on_eval: if True, this layer will only be calculated in eval
    :param bool only_on_search: if True, this layer will only be calculated when search is done
    :param int|None copy_output_loss_from_source_idx: if set, will copy output_loss from this source
    :param float|None l2: for constraints
    :param float|None darc1: for constraints. see Generalization in Deep Learning, https://arxiv.org/abs/1710.05468
    :param float|None spatial_smoothing: see :func:`returnn.tf.util.basic.spatial_smoothing_energy`
    :param float|None param_variational_noise: adds variational noise to the params during training
    :param dict[str]|None updater_opts: accepts similar opts as TFUpdater, e.g. "optimizer", "learning_rate", ...
    :param str|float initial_output: used for recurrent layer, see self.get_rec_initial_output()
    :param list[str]|None collocate_with: in the rec layer, collocate with the specified other layers
    :param bool|None trainable: whether the parameters of this layer will be trained.
      default (None) inherits from the parent layer if there is one, or otherwise True.
    :param str|callable|None custom_param_importer: used by :func:`set_param_values_by_dict`
    :param str|None register_as_extern_data: registers output in network.extern_data
    """
    super().__init__()
    self.param_device = param_device
    self.only_on_eval = only_on_eval
    self.only_on_search = only_on_search
    self.copy_output_loss_from_source_idx = copy_output_loss_from_source_idx
    self.l2 = l2
    self.darc1 = darc1
    self.spatial_smoothing = spatial_smoothing
    self.param_variational_noise = param_variational_noise
    self.updater_opts = updater_opts
    self.initial_output = initial_output
    self.collocate_with = collocate_with
    self.trainable = trainable
    self.custom_param_importer = custom_param_importer
    self.register_as_extern_data = register_as_extern_data

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'param_device': self.param_device,
      'only_on_eval': self.only_on_eval,
      'only_on_search': self.only_on_search,
      'copy_output_loss_from_source_idx': self.copy_output_loss_from_source_idx,
      'L2': self.l2,
      'darc1': self.darc1,
      'spatial_smoothing': self.spatial_smoothing,
      'param_variational_noise': self.param_variational_noise,
      'updater_opts': self.updater_opts,
      'initial_output': self.initial_output,
      'collocate_with': self.collocate_with,
      'trainable': self.trainable,
      'custom_param_importer': self.custom_param_importer,
      'register_as_extern_data': self.register_as_extern_data,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return opts

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Source(_Base):
  """
  This gives access to some entry from network.extern_data (:class:`ExternData`).
  """
  returnn_layer_class = 'source'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               data_key: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param str|None data_key:
    """
    super().__init__(**kwargs)
    self.data_key = data_key

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'data_key': self.data_key,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'source',
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def external_data(
                  *,
                  data_key: Optional[str] = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This gives access to some entry from network.extern_data (:class:`ExternData`).

  :param str|None data_key:
  :param str|None name:
  """
  mod = Source(
    data_key=data_key,
    )
  return mod(
    name=name)


class _ConcatInput(_Base):
  """
  Base layer which concatenates all incoming source layers in the feature dimension,
  and provides that as `self.input_data`, which is of type :class:`Data`.
  This is the most common thing what many layers do with the input sources.
  If there is only a single source, will not do anything.
  This layer also optionally can do dropout on the input.
  """
  returnn_layer_class = None
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               dropout: float = NotSpecified,
               dropout_noise_shape: Any = NotSpecified,
               dropout_on_forward: bool = NotSpecified,
               mask: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param float dropout: 0.0 means to apply no dropout. dropout will only be applied during training
    :param dict[str|tuple,int|None] dropout_noise_shape: see :func:`returnn.tf.util.data.get_bc_shape`
    :param bool dropout_on_forward: apply dropout during inference
    :param str|None mask: "dropout" or "unity" or None. this is obsolete and only here for historical reasons
    """
    super().__init__(**kwargs)
    self.dropout = dropout
    self.dropout_noise_shape = dropout_noise_shape
    self.dropout_on_forward = dropout_on_forward
    self.mask = mask

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dropout': self.dropout,
      'dropout_noise_shape': self.dropout_noise_shape,
      'dropout_on_forward': self.dropout_on_forward,
      'mask': self.mask,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Copy(_ConcatInput):
  """
  This layer does nothing, it copies its input.
  If multiple sources are provided, they are concatenated in the feature-dim.
  """
  returnn_layer_class = 'copy'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'copy',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def copy(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This layer does nothing, it copies its input.
  If multiple sources are provided, they are concatenated in the feature-dim.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|None name:
  """
  mod = Copy()
  return mod(source, name=name)


class Dropout(Copy):
  """
  Just the same as :class:`CopyLayer`, because that one already supports dropout.
  """
  returnn_layer_class = 'dropout'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dropout',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def dropout(
            source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
            *,
            dropout: float = NotSpecified,
            dropout_noise_shape: Any = NotSpecified,
            dropout_on_forward: bool = NotSpecified,
            mask: Optional[str] = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Just the same as :class:`CopyLayer`, because that one already supports dropout.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float dropout: 0.0 means to apply no dropout. dropout will only be applied during training
  :param dict[str|tuple,int|None] dropout_noise_shape: see :func:`returnn.tf.util.data.get_bc_shape`
  :param bool dropout_on_forward: apply dropout during inference
  :param str|None mask: "dropout" or "unity" or None. this is obsolete and only here for historical reasons
  :param str|None name:
  """
  mod = Dropout(
    dropout=dropout,
    dropout_noise_shape=dropout_noise_shape,
    dropout_on_forward=dropout_on_forward,
    mask=mask,
    )
  return mod(source, name=name)


class ScaledGradient(Copy):
  """
  Just tf.identity in the forward pass.
  Scales the gradient by some factor in backprop.
  Can be used as gradient reversal layer (with negative factor).
  Uses :func:`returnn.tf.util.basic.scaled_gradient`, or :func:`tf.stop_gradient`
  """
  returnn_layer_class = 'scaled_grad'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               scale: float,
               **kwargs):
    """
    :param float scale: if 0., will use tf.stop_gradient
    """
    super().__init__(**kwargs)
    self.scale = scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'scale': self.scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'scaled_grad',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def scaled_gradient(
                    source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                    *,
                    scale: float,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Just tf.identity in the forward pass.
  Scales the gradient by some factor in backprop.
  Can be used as gradient reversal layer (with negative factor).
  Uses :func:`returnn.tf.util.basic.scaled_gradient`, or :func:`tf.stop_gradient`

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float scale: if 0., will use tf.stop_gradient
  :param str|None name:
  """
  mod = ScaledGradient(
    scale=scale,
    )
  return mod(source, name=name)


class Activation(_ConcatInput):
  """
  This layer just applies an activation function.
  See :func:`returnn.tf.util.basic.get_activation_function` about supported functions.
  Also see :class:`EvalLayer` and :class:`CombineLayer` for similar layers.
  """
  returnn_layer_class = 'activation'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               activation: str,
               **kwargs):
    """
    :param str activation: e.g. "relu", "tanh", etc
    """
    super().__init__(**kwargs)
    self.activation = activation

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'activation': self.activation,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'activation',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def activation(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               activation: str,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This layer just applies an activation function.
  See :func:`returnn.tf.util.basic.get_activation_function` about supported functions.
  Also see :class:`EvalLayer` and :class:`CombineLayer` for similar layers.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str activation: e.g. "relu", "tanh", etc
  :param str|None name:
  """
  mod = Activation(
    activation=activation,
    )
  return mod(source, name=name)


class BatchNorm(Copy):
  """
  Implements batch-normalization (https://arxiv.org/abs/1502.03167) as a separate layer.

  Also see :class:`NormLayer`.
  """
  returnn_layer_class = 'batch_norm'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               use_shift: bool = NotSpecified,
               use_std: bool = NotSpecified,
               use_sample: float = NotSpecified,
               force_sample: bool = NotSpecified,
               momentum: float = NotSpecified,
               epsilon: float = NotSpecified,
               update_sample_only_in_training: bool = NotSpecified,
               delay_sample_update: bool = NotSpecified,
               param_version: int = NotSpecified,
               gamma_init: Union[str, float] = NotSpecified,
               beta_init: Union[str, float] = NotSpecified,
               masked_time: bool = NotSpecified,
               **kwargs):
    """
    The default settings for these variables are set in the function "batch_norm" of the LayerBase. If you do not want
    to change them you can leave them undefined here.
    With our default settings:

    - In training: use_sample=0, i.e. not using running average, using current batch mean/var.
    - Not in training (e.g. eval): use_sample=1, i.e. using running average, not using current batch mean/var.
    - The running average includes the statistics of the current batch.
    - The running average is also updated when not training.

    :param bool use_shift:
    :param bool use_std:
    :param float use_sample: defaults to 0.0 which is used in training
    :param bool force_sample: even in eval, use the use_sample factor
    :param float momentum: for the running average of sample_mean and sample_std
    :param float epsilon:
    :param bool update_sample_only_in_training:
    :param bool delay_sample_update:
    :param int param_version: 0 or 1
    :param str|float gamma_init: see :func:`returnn.tf.util.basic.get_initializer`, for the scale
    :param str|float beta_init: see :func:`returnn.tf.util.basic.get_initializer`, for the mean
    :param bool masked_time: flatten and mask input tensor
    """
    super().__init__(**kwargs)
    self.use_shift = use_shift
    self.use_std = use_std
    self.use_sample = use_sample
    self.force_sample = force_sample
    self.momentum = momentum
    self.epsilon = epsilon
    self.update_sample_only_in_training = update_sample_only_in_training
    self.delay_sample_update = delay_sample_update
    self.param_version = param_version
    self.gamma_init = gamma_init
    self.beta_init = beta_init
    self.masked_time = masked_time

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'use_shift': self.use_shift,
      'use_std': self.use_std,
      'use_sample': self.use_sample,
      'force_sample': self.force_sample,
      'momentum': self.momentum,
      'epsilon': self.epsilon,
      'update_sample_only_in_training': self.update_sample_only_in_training,
      'delay_sample_update': self.delay_sample_update,
      'param_version': self.param_version,
      'gamma_init': self.gamma_init,
      'beta_init': self.beta_init,
      'masked_time': self.masked_time,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'batch_norm',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def batch_norm(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               use_shift: bool = NotSpecified,
               use_std: bool = NotSpecified,
               use_sample: float = NotSpecified,
               force_sample: bool = NotSpecified,
               momentum: float = NotSpecified,
               epsilon: float = NotSpecified,
               update_sample_only_in_training: bool = NotSpecified,
               delay_sample_update: bool = NotSpecified,
               param_version: int = NotSpecified,
               gamma_init: Union[str, float] = NotSpecified,
               beta_init: Union[str, float] = NotSpecified,
               masked_time: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Implements batch-normalization (https://arxiv.org/abs/1502.03167) as a separate layer.

  Also see :class:`NormLayer`.

  The default settings for these variables are set in the function "batch_norm" of the LayerBase. If you do not want
  to change them you can leave them undefined here.
  With our default settings:

  - In training: use_sample=0, i.e. not using running average, using current batch mean/var.
  - Not in training (e.g. eval): use_sample=1, i.e. using running average, not using current batch mean/var.
  - The running average includes the statistics of the current batch.
  - The running average is also updated when not training.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param bool use_shift:
  :param bool use_std:
  :param float use_sample: defaults to 0.0 which is used in training
  :param bool force_sample: even in eval, use the use_sample factor
  :param float momentum: for the running average of sample_mean and sample_std
  :param float epsilon:
  :param bool update_sample_only_in_training:
  :param bool delay_sample_update:
  :param int param_version: 0 or 1
  :param str|float gamma_init: see :func:`returnn.tf.util.basic.get_initializer`, for the scale
  :param str|float beta_init: see :func:`returnn.tf.util.basic.get_initializer`, for the mean
  :param bool masked_time: flatten and mask input tensor
  :param str|None name:
  """
  mod = BatchNorm(
    use_shift=use_shift,
    use_std=use_std,
    use_sample=use_sample,
    force_sample=force_sample,
    momentum=momentum,
    epsilon=epsilon,
    update_sample_only_in_training=update_sample_only_in_training,
    delay_sample_update=delay_sample_update,
    param_version=param_version,
    gamma_init=gamma_init,
    beta_init=beta_init,
    masked_time=masked_time,
    )
  return mod(source, name=name)


class LayerNorm(_ConcatInput):
  """
  Applies `layer-normalization <https://arxiv.org/abs/1607.06450>`__.

  Note that we *just* normalize over the feature-dim axis here.
  This is consistent to the default behavior of :class:`tf.keras.layers.LayerNormalization`
  and also how it is commonly used in many models, including Transformer.

  However, there are cases where it would be common to normalize over all axes except batch-dim,
  or all axes except batch and time.
  For a more generic variant, see :class:`NormLayer`.
  """
  returnn_layer_class = 'layer_norm'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               epsilon: float = NotSpecified,
               **kwargs):
    """
    :param float epsilon:
    """
    super().__init__(**kwargs)
    self.epsilon = epsilon

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'epsilon': self.epsilon,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'layer_norm',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def layer_norm(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               epsilon: float = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Applies `layer-normalization <https://arxiv.org/abs/1607.06450>`__.

  Note that we *just* normalize over the feature-dim axis here.
  This is consistent to the default behavior of :class:`tf.keras.layers.LayerNormalization`
  and also how it is commonly used in many models, including Transformer.

  However, there are cases where it would be common to normalize over all axes except batch-dim,
  or all axes except batch and time.
  For a more generic variant, see :class:`NormLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float epsilon:
  :param str|None name:
  """
  mod = LayerNorm(
    epsilon=epsilon,
    )
  return mod(source, name=name)


class Norm(_ConcatInput):
  """
  Normalize over specified axes, e.g. time and/or feature axis.

  Note: For calculating a norm, see :class:`MathNormLayer` instead.

  In case of just feature (``axes="F"``),
  this corresponds to `layer normalization <https://arxiv.org/abs/1607.06450>`__ (see :class:`LayerNormLayer`).
  In case of time and feature (``axes="TF"``) for a 3D input,
  or more general all except batch (``axes="except_batch"``),
  this corresponds to `group normalization <https://arxiv.org/abs/1803.08494>`__ with G=1,
  or non-standard layer normalization.
  (The definition of layer-normalization is not clear on what axes should be normalized over.
  In many other frameworks, the default axis is just the last axis,
  which is usually the feature axis.
  However, in certain implementations and models,
  it is also common to normalize over all axes except batch.)

  The statistics are calculated just on the input.
  There are no running statistics (in contrast to batch normalization, see :class:`BatchNormLayer`).

  For some discussion on the definition of layer-norm vs group-norm,
  also see
  `here <https://stats.stackexchange.com/questions/485550/is-group-norm-with-g-1-equiv-to-layer-norm>`__
  and `here <https://github.com/tensorflow/addons/issues/2143>`__.
  """
  returnn_layer_class = 'norm'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               param_shape: Any = NotSpecified,
               scale: bool = NotSpecified,
               bias: bool = NotSpecified,
               epsilon: float = NotSpecified,
               **kwargs):
    """
    :param str|list[str]|tuple[str]|int|list[int]|tuple[int] param_shape: shape of the scale and bias parameters.
      You can also refer to (static) axes of the input, such as the feature-dim.
      This is also the default, i.e. a param-shape of [F], independent of the axes to normalize over.
    :param bool scale: add trainable scale parameters
    :param bool bias: add trainable bias parameters
    :param float epsilon: epsilon for numerical stability
    """
    super().__init__(**kwargs)
    self.param_shape = param_shape
    self.scale = scale
    self.bias = bias
    self.epsilon = epsilon

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'param_shape': self.param_shape,
      'scale': self.scale,
      'bias': self.bias,
      'epsilon': self.epsilon,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axes: Any,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axes': axes,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'norm',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def norm(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         axes: Any,
         param_shape: Any = NotSpecified,
         scale: bool = NotSpecified,
         bias: bool = NotSpecified,
         epsilon: float = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Normalize over specified axes, e.g. time and/or feature axis.

  Note: For calculating a norm, see :class:`MathNormLayer` instead.

  In case of just feature (``axes="F"``),
  this corresponds to `layer normalization <https://arxiv.org/abs/1607.06450>`__ (see :class:`LayerNormLayer`).
  In case of time and feature (``axes="TF"``) for a 3D input,
  or more general all except batch (``axes="except_batch"``),
  this corresponds to `group normalization <https://arxiv.org/abs/1803.08494>`__ with G=1,
  or non-standard layer normalization.
  (The definition of layer-normalization is not clear on what axes should be normalized over.
  In many other frameworks, the default axis is just the last axis,
  which is usually the feature axis.
  However, in certain implementations and models,
  it is also common to normalize over all axes except batch.)

  The statistics are calculated just on the input.
  There are no running statistics (in contrast to batch normalization, see :class:`BatchNormLayer`).

  For some discussion on the definition of layer-norm vs group-norm,
  also see
  `here <https://stats.stackexchange.com/questions/485550/is-group-norm-with-g-1-equiv-to-layer-norm>`__
  and `here <https://github.com/tensorflow/addons/issues/2143>`__.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|list[str] axes: axes over which the mean and variance are computed, e.g. "F" or "TF"
  :param str|list[str]|tuple[str]|int|list[int]|tuple[int] param_shape: shape of the scale and bias parameters.
    You can also refer to (static) axes of the input, such as the feature-dim.
    This is also the default, i.e. a param-shape of [F], independent of the axes to normalize over.
  :param bool scale: add trainable scale parameters
  :param bool bias: add trainable bias parameters
  :param float epsilon: epsilon for numerical stability
  :param str|None name:
  """
  mod = Norm(
    param_shape=param_shape,
    scale=scale,
    bias=bias,
    epsilon=epsilon,
    )
  return mod(
    source,
    axes=axes,
    name=name)


class MathNorm(_ConcatInput):
  """
  Calculates sum(abs(x) ** p) ** (1./p).
  """
  returnn_layer_class = 'math_norm'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               p: Union[int, float],
               keep_dims: bool = NotSpecified,
               **kwargs):
    """
    :param int|float p:
    :param bool keep_dims:
    """
    super().__init__(**kwargs)
    self.p = p
    self.keep_dims = keep_dims

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'p': self.p,
      'keep_dims': self.keep_dims,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axes: Any,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axes': axes,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'math_norm',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def math_norm(
              source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
              *,
              p: Union[int, float],
              axes: Any,
              keep_dims: bool = NotSpecified,
              name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calculates sum(abs(x) ** p) ** (1./p).

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int|float p:
  :param str|list[str] axes:
  :param bool keep_dims:
  :param str|None name:
  """
  mod = MathNorm(
    p=p,
    keep_dims=keep_dims,
    )
  return mod(
    source,
    axes=axes,
    name=name)


class Slice(_ConcatInput):
  """
  Slicing on the input, i.e. x[start:end:step] in some axis.
  See also :class:`SliceNdLayer`, for variable start.
  See also :class:`GatherLayer`, for one single position.

  Note that __getitem__ on a TF tensor (or also Numpy ND array) is more generic,
  and supports slices in multiple axes, as well as adding new dimensions, etc.
  It even allows to get boolean values, and then applies a boolean mask.
  See TF _slice_helper (== tf.Tensor.__getitem__) for a generic implementation,
  which calls tf.strided_slice.
  If we ever need such more generic support, we might consider adding a new layer,
  like ``GenericSliceLayer``, which gets a ``splice_spec``,
  just like ``_slice_helper`` (argument to ``__getitem__``).
  But any such a slice can already be constructed with multiple individual layers,
  which perform individual slices (per axis).

  We just support slicing in a single axis here, with optional striding (slice_step).
  """
  returnn_layer_class = 'slice'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               slice_start: Optional[int] = NotSpecified,
               slice_end: Optional[int] = NotSpecified,
               slice_step: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param int|None slice_start:
    :param int|None slice_end:
    :param int|None slice_step:
    """
    super().__init__(**kwargs)
    self.slice_start = slice_start
    self.slice_end = slice_end
    self.slice_step = slice_step

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'slice_start': self.slice_start,
      'slice_end': self.slice_end,
      'slice_step': self.slice_step,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Union[int, str],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'slice',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def slice(
          source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
          *,
          axis: Union[int, str],
          slice_start: Optional[int] = NotSpecified,
          slice_end: Optional[int] = NotSpecified,
          slice_step: Optional[int] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Slicing on the input, i.e. x[start:end:step] in some axis.
  See also :class:`SliceNdLayer`, for variable start.
  See also :class:`GatherLayer`, for one single position.

  Note that __getitem__ on a TF tensor (or also Numpy ND array) is more generic,
  and supports slices in multiple axes, as well as adding new dimensions, etc.
  It even allows to get boolean values, and then applies a boolean mask.
  See TF _slice_helper (== tf.Tensor.__getitem__) for a generic implementation,
  which calls tf.strided_slice.
  If we ever need such more generic support, we might consider adding a new layer,
  like ``GenericSliceLayer``, which gets a ``splice_spec``,
  just like ``_slice_helper`` (argument to ``__getitem__``).
  But any such a slice can already be constructed with multiple individual layers,
  which perform individual slices (per axis).

  We just support slicing in a single axis here, with optional striding (slice_step).

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int|str axis:
  :param int|None slice_start:
  :param int|None slice_end:
  :param int|None slice_step:
  :param str|None name:
  """
  mod = Slice(
    slice_start=slice_start,
    slice_end=slice_end,
    slice_step=slice_step,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class SliceNd(_ConcatInput):
  """
  This takes out a slice-range from the time axis,
  e.g. ``x[start:start + size]``.
  If the input is of shape (B,T,F) and start is of shape (B,),
  then the output will be of shape (B,size,F).
  If the input is of shape (B,T,F) and start is of shape (B,T),
  then the output will be of shape (B,T,size,F).
  This layer allows a different start slice point for each batch,
  in contrast to :class:`SliceLayer`, and the start is variable.
  See also :class:`GatherNdLayer`.
  :class:`PrefixInTimeLayer` can recover the original shape (by zero-padding).
  """
  returnn_layer_class = 'slice_nd'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               min_size: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param int|None min_size: if size is None, but we want to have a min-size
    """
    super().__init__(**kwargs)
    self.min_size = min_size

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'min_size': self.min_size,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      start: LayerRef,
                      size: Optional[Union[int, LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'start': start,
      'size': size,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'slice_nd',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def slice_nd(
             source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
             *,
             start: LayerRef,
             size: Optional[Union[int, LayerRef]],
             min_size: Optional[int] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This takes out a slice-range from the time axis,
  e.g. ``x[start:start + size]``.
  If the input is of shape (B,T,F) and start is of shape (B,),
  then the output will be of shape (B,size,F).
  If the input is of shape (B,T,F) and start is of shape (B,T),
  then the output will be of shape (B,T,size,F).
  This layer allows a different start slice point for each batch,
  in contrast to :class:`SliceLayer`, and the start is variable.
  See also :class:`GatherNdLayer`.
  :class:`PrefixInTimeLayer` can recover the original shape (by zero-padding).

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase start: (B,...)
  :param int|LayerBase|None size: if None, it uses the max possible size,
    and it becomes a dynamic axis.
  :param int|None min_size: if size is None, but we want to have a min-size
  :param str|None name:
  """
  mod = SliceNd(
    min_size=min_size,
    )
  return mod(
    source,
    start=start,
    size=size,
    name=name)


class Gather(_ConcatInput):
  """
  Gathers slices on a specified axis from the input layer using indices from a ``position`` layer.
  If the input is a layer of the shape ``[B,D,F1]``, and position of shape ``[B,F2]``, this will yield output of the
  shape ``[B,F2,F1]`` where

  ``output[b,f2,f1] = input[b,position[b,f2],f1]``

  (if ``D`` is the axis to gather from).
  In general, all shared axes of the input and the positions will be considered as batch-axes.

  The ``position`` argument can also be an ``int``.
  In this case, this simply gives ``input[position]`` one the specified ``axis``.

  It's basically a wrapper around ``tf.gather``.
  It provides the same functionality as the deprecated ``GatherNdLayer``, but is more generic.
  See also :class:`GatherNdLayer`.
  """
  returnn_layer_class = 'gather'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      position: Union[LayerRef, int],
                      axis: str,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'position': position,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'gather',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def gather(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           *,
           position: Union[LayerRef, int],
           axis: str,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gathers slices on a specified axis from the input layer using indices from a ``position`` layer.
  If the input is a layer of the shape ``[B,D,F1]``, and position of shape ``[B,F2]``, this will yield output of the
  shape ``[B,F2,F1]`` where

  ``output[b,f2,f1] = input[b,position[b,f2],f1]``

  (if ``D`` is the axis to gather from).
  In general, all shared axes of the input and the positions will be considered as batch-axes.

  The ``position`` argument can also be an ``int``.
  In this case, this simply gives ``input[position]`` one the specified ``axis``.

  It's basically a wrapper around ``tf.gather``.
  It provides the same functionality as the deprecated ``GatherNdLayer``, but is more generic.
  See also :class:`GatherNdLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase|int position: Layer containing the indices used to select the slices of the input from.
    If another layer, must be of type ``int32`` or ``int64``.
    Can also specify a constant ``int``.
  :param str axis: The axis into which we gather the indices into
  :param str|None name:
  """
  mod = Gather()
  return mod(
    source,
    position=position,
    axis=axis,
    name=name)


class ScatterNd(_ConcatInput):
  """
  The inverse of :class:`GatherNdLayer`.
  Mostly a wrapper for ``tf.scatter_nd``.

  The input to the layer are the ``updates``, the ``indices`` are via the ``position`` argument.
  The indices are into the newly constructed output dimension.
  The output shape is constructed via the common shape of the input, the position,
  and the the unique common axis (if not unique, we would need to introduce an option to specify it)
  is replaced by the given output dimension (currently via ``output_dim_via_time_from``).

  Examples::

    position (indices): (B,eTs)
    input (updates): (eTs,D) or (B,eTs,D) -> expanded to (B,eTs,D)
    output shape: (B,eT,D)

    position (indices): (B,dT,eTs)
    input (updates): (eTs,D) -> expanded to (B,dT,eTs,D)
    output shape: (B,dT,eT,D)

    position (indices): (dT,eTs)
    input (updates): (eTs,D) -> expanded to (dT,eTs,D)
    output shape: (dT,eTs,D)

    position (indices): (dT,eTs)
    input (updates): (B,eTs,D) -> expanded to (dT,eTs,B,D)
    output shape: (dT,eT,B,D)

  In all these examples, output_dim_via_time_from is (B,eT,F), and eTs gets replaced by eT.
  """
  returnn_layer_class = 'scatter_nd'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               position_axis: Union[str, int],
               filter_invalid_indices: bool = NotSpecified,
               **kwargs):
    """
    :param str|int position_axis: axis in `position` to replace by the output-dim
    :param bool filter_invalid_indices: allow for indices <0 or >= output_dim, which will be discarded in the output
    """
    super().__init__(**kwargs)
    self.position_axis = position_axis
    self.filter_invalid_indices = filter_invalid_indices

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'position_axis': self.position_axis,
      'filter_invalid_indices': self.filter_invalid_indices,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      position: LayerRef,
                      output_dim_via_time_from: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'position': position,
      'output_dim_via_time_from': output_dim_via_time_from,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'scatter_nd',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def scatter_nd(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               position: LayerRef,
               position_axis: Union[str, int],
               output_dim_via_time_from: LayerRef,
               filter_invalid_indices: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  The inverse of :class:`GatherNdLayer`.
  Mostly a wrapper for ``tf.scatter_nd``.

  The input to the layer are the ``updates``, the ``indices`` are via the ``position`` argument.
  The indices are into the newly constructed output dimension.
  The output shape is constructed via the common shape of the input, the position,
  and the the unique common axis (if not unique, we would need to introduce an option to specify it)
  is replaced by the given output dimension (currently via ``output_dim_via_time_from``).

  Examples::

    position (indices): (B,eTs)
    input (updates): (eTs,D) or (B,eTs,D) -> expanded to (B,eTs,D)
    output shape: (B,eT,D)

    position (indices): (B,dT,eTs)
    input (updates): (eTs,D) -> expanded to (B,dT,eTs,D)
    output shape: (B,dT,eT,D)

    position (indices): (dT,eTs)
    input (updates): (eTs,D) -> expanded to (dT,eTs,D)
    output shape: (dT,eTs,D)

    position (indices): (dT,eTs)
    input (updates): (B,eTs,D) -> expanded to (dT,eTs,B,D)
    output shape: (dT,eT,B,D)

  In all these examples, output_dim_via_time_from is (B,eT,F), and eTs gets replaced by eT.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase position: indices into first axis (excluding batch) of the output
  :param str|int position_axis: axis in `position` to replace by the output-dim
  :param LayerBase output_dim_via_time_from: use the time-dim from this layer as the output-dim
  :param bool filter_invalid_indices: allow for indices <0 or >= output_dim, which will be discarded in the output
  :param str|None name:
  """
  mod = ScatterNd(
    position_axis=position_axis,
    filter_invalid_indices=filter_invalid_indices,
    )
  return mod(
    source,
    position=position,
    output_dim_via_time_from=output_dim_via_time_from,
    name=name)


class Linear(_ConcatInput):
  """
  Linear/forward/fully-connected/1x1-conv layer.
  Does a linear transformation on the feature-dimension of the input
  with an optional bias term and an optional activation function.
  See also :class:`DotLayer`, :class:`ElemwiseProdLayer`, :class:`WeightedSumLayer`.
  """
  returnn_layer_class = 'linear'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               activation: Optional[str] = NotSpecified,
               with_bias: bool = NotSpecified,
               grad_filter: Optional[float] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               bias_init: Union[str, float] = NotSpecified,
               use_transposed_weights: bool = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param str|None activation: e.g. "relu", or None
    :param bool with_bias:
    :param float|None grad_filter: if grad norm is higher than this threshold (before activation), the grad is removed
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str|float bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param bool use_transposed_weights: If True, define the weight matrix with transposed dimensions (n_out, n_in).
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.activation = activation
    self.with_bias = with_bias
    self.grad_filter = grad_filter
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.use_transposed_weights = use_transposed_weights

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'activation': self.activation,
      'with_bias': self.with_bias,
      'grad_filter': self.grad_filter,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'use_transposed_weights': self.use_transposed_weights,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'linear',
      'from': source,
      **self.get_opts()}


class Length(_Base):
  """
  Returns the length of sources as (B,), via input size_placeholder.
  """
  returnn_layer_class = 'length'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               add_time_axis: bool = NotSpecified,
               dtype: str = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param bool add_time_axis:
    :param str dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.add_time_axis = add_time_axis
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'add_time_axis': self.add_time_axis,
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      axis: Union[str, DimensionTag] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'length',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def length(
           source: LayerRef,
           *,
           axis: Union[str, DimensionTag] = NotSpecified,
           add_time_axis: bool = NotSpecified,
           dtype: str = NotSpecified,
           sparse: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Returns the length of sources as (B,), via input size_placeholder.

  :param LayerRef source:
  :param str|DimensionTag axis:
  :param bool add_time_axis:
  :param str dtype:
  :param bool sparse:
  :param str|None name:
  """
  mod = Length(
    add_time_axis=add_time_axis,
    dtype=dtype,
    sparse=sparse,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class SoftmaxOverSpatial(_ConcatInput):
  """
  This applies a softmax over spatial axis/axes (currently only time axis supported).
  E.g. when the input is of shape (B,T,dim), the output will be (B,T,dim).
  It automatically masks the frames outside the seq defined by the seq-len.
  In contrast to :class:`SoftmaxLayer`, this will not do a linear transformation.
  See :class:`SeqLenMaskLayer` if you just want to apply a masking.
  """
  returnn_layer_class = 'softmax_over_spatial'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               energy_factor: Optional[float] = NotSpecified,
               use_time_mask: bool = NotSpecified,
               log_space: bool = NotSpecified,
               **kwargs):
    """
    :param float|None energy_factor: the energy will be scaled by this factor.
      This is like a temperature for the softmax.
      In Attention-is-all-you-need, this is set to 1/sqrt(base_ctx.dim).
    :param bool use_time_mask: if True, assumes dyn seq len, and use it for masking.
      By default, if dyn seq len exists, it uses it.
    :param bool log_space: if True, returns in log space (i.e. uses log_softmax)
    """
    super().__init__(**kwargs)
    self.energy_factor = energy_factor
    self.use_time_mask = use_time_mask
    self.log_space = log_space

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'energy_factor': self.energy_factor,
      'use_time_mask': self.use_time_mask,
      'log_space': self.log_space,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Optional[str] = NotSpecified,
                      start: Optional[LayerRef] = NotSpecified,
                      window_start: Optional[Union[LayerRef, int]] = NotSpecified,
                      window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
      'start': start,
      'window_start': window_start,
      'window_size': window_size,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'softmax_over_spatial',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def softmax(
            source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
            *,
            axis: Optional[str] = NotSpecified,
            energy_factor: Optional[float] = NotSpecified,
            start: Optional[LayerRef] = NotSpecified,
            window_start: Optional[Union[LayerRef, int]] = NotSpecified,
            window_size: Optional[Union[LayerRef, int]] = NotSpecified,
            use_time_mask: bool = NotSpecified,
            log_space: bool = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This applies a softmax over spatial axis/axes (currently only time axis supported).
  E.g. when the input is of shape (B,T,dim), the output will be (B,T,dim).
  It automatically masks the frames outside the seq defined by the seq-len.
  In contrast to :class:`SoftmaxLayer`, this will not do a linear transformation.
  See :class:`SeqLenMaskLayer` if you just want to apply a masking.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|None axis: which axis to do the softmax over
  :param float|None energy_factor: the energy will be scaled by this factor.
    This is like a temperature for the softmax.
    In Attention-is-all-you-need, this is set to 1/sqrt(base_ctx.dim).
  :param LayerBase|None start: Tensor of shape (B,) indicating the start frame
  :param LayerBase|int|None window_start: Layer with output of shape (B,) or (constant) int value indicating
    the window start.
  :param LayerBase|int|None window_size: Layer with output of shape (B,) or (constant) int value indicating
    the window size.
  :param bool use_time_mask: if True, assumes dyn seq len, and use it for masking.
    By default, if dyn seq len exists, it uses it.
  :param bool log_space: if True, returns in log space (i.e. uses log_softmax)
  :param str|None name:
  """
  mod = SoftmaxOverSpatial(
    energy_factor=energy_factor,
    use_time_mask=use_time_mask,
    log_space=log_space,
    )
  return mod(
    source,
    axis=axis,
    start=start,
    window_start=window_start,
    window_size=window_size,
    name=name)


class SeqLenMask(_ConcatInput):
  """
  Masks some values away given the seq_len_source with mask_value.
  Also see :class:`SoftmaxOverSpatialLayer`.
  Also see :class:`SwitchLayer`, which can be used to apply a generic mask.
  """
  returnn_layer_class = 'seq_len_mask'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               mask_value: float,
               **kwargs):
    """
    :param float mask_value:
    """
    super().__init__(**kwargs)
    self.mask_value = mask_value

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mask_value': self.mask_value,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Union[str, int] = NotSpecified,
                      seq_len_source: Optional[LayerRef] = NotSpecified,
                      start: Optional[LayerRef] = NotSpecified,
                      window_start: Optional[LayerRef] = NotSpecified,
                      window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
      'seq_len_source': seq_len_source,
      'start': start,
      'window_start': window_start,
      'window_size': window_size,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'seq_len_mask',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def seq_len_mask(
                 source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                 *,
                 mask_value: float,
                 axis: Union[str, int] = NotSpecified,
                 seq_len_source: Optional[LayerRef] = NotSpecified,
                 start: Optional[LayerRef] = NotSpecified,
                 window_start: Optional[LayerRef] = NotSpecified,
                 window_size: Optional[Union[LayerRef, int]] = NotSpecified,
                 name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Masks some values away given the seq_len_source with mask_value.
  Also see :class:`SoftmaxOverSpatialLayer`.
  Also see :class:`SwitchLayer`, which can be used to apply a generic mask.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float mask_value:
  :param str|int axis:
  :param LayerBase|None seq_len_source: if not given, uses source
  :param LayerBase|None start: Tensor of shape (B,) indicating the start frame
  :param LayerBase|None window_start: Tensor of shape (B,) indicating the window start
  :param LayerBase|int|None window_size:
  :param str|None name:
  """
  mod = SeqLenMask(
    mask_value=mask_value,
    )
  return mod(
    source,
    axis=axis,
    seq_len_source=seq_len_source,
    start=start,
    window_start=window_start,
    window_size=window_size,
    name=name)


class RandInt(_Base):
  """
  Generates random numbers using ``tf.random.uniform``
  """
  returnn_layer_class = 'rand_int'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               shape: Any,
               maxval: int,
               minval: int = NotSpecified,
               dtype: str = NotSpecified,
               seed: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param tuple[DimensionTag|int]|list[DimensionTag|int] shape: desired shape of output tensor
    :param int maxval: upper bound (exclusive) on range of random values
    :param int minval: lower bound (inclusive) on range of random values
    :param str dtype: type of the output. For random ints, int32 and int64 make sense, but could also be floats
    :param int|None seed: random seed
    """
    super().__init__(**kwargs)
    self.shape = shape
    self.maxval = maxval
    self.minval = minval
    self.dtype = dtype
    self.seed = seed

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'shape': self.shape,
      'maxval': self.maxval,
      'minval': self.minval,
      'dtype': self.dtype,
      'seed': self.seed,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'rand_int',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def rand_int(
             source: LayerRef,
             *,
             shape: Any,
             maxval: int,
             minval: int = NotSpecified,
             dtype: str = NotSpecified,
             seed: Optional[int] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Generates random numbers using ``tf.random.uniform``

  :param LayerRef source:
  :param tuple[DimensionTag|int]|list[DimensionTag|int] shape: desired shape of output tensor
  :param int maxval: upper bound (exclusive) on range of random values
  :param int minval: lower bound (inclusive) on range of random values
  :param str dtype: type of the output. For random ints, int32 and int64 make sense, but could also be floats
  :param int|None seed: random seed
  :param str|None name:
  """
  mod = RandInt(
    shape=shape,
    maxval=maxval,
    minval=minval,
    dtype=dtype,
    seed=seed,
    )
  return mod(source, name=name)


class Range(_Base):
  """
  Generic wrapper around ``tf.range``.
  See also :class:`RangeInAxisLayer`.
  """
  returnn_layer_class = 'range'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               limit: Union[int, float],
               start: Union[int, float] = NotSpecified,
               delta: Union[int, float] = NotSpecified,
               dtype: Optional[str] = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param int|float limit:
    :param int|float start:
    :param int|float delta:
    :param str|None dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.limit = limit
    self.start = start
    self.delta = delta
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'limit': self.limit,
      'start': self.start,
      'delta': self.delta,
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'range',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def range_(
           source: LayerRef,
           *,
           limit: Union[int, float],
           start: Union[int, float] = NotSpecified,
           delta: Union[int, float] = NotSpecified,
           dtype: Optional[str] = NotSpecified,
           sparse: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Generic wrapper around ``tf.range``.
  See also :class:`RangeInAxisLayer`.

  :param LayerRef source:
  :param int|float limit:
  :param int|float start:
  :param int|float delta:
  :param str|None dtype:
  :param bool sparse:
  :param str|None name:
  """
  mod = Range(
    limit=limit,
    start=start,
    delta=delta,
    dtype=dtype,
    sparse=sparse,
    )
  return mod(source, name=name)


class RangeInAxis(_Base):
  """
  Assume that the input is e.g. (B,T,D), and you specify axis="T", you will get (B=1,T,D=1),
  where the specified axis is filled with ``tf.range``.
  See also :class:`RangeLayer`.
  """
  returnn_layer_class = 'range_in_axis'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               dtype: str = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param str dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      axis: str,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'range_in_axis',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def range_in_axis(
                  source: LayerRef,
                  *,
                  axis: str,
                  dtype: str = NotSpecified,
                  sparse: bool = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Assume that the input is e.g. (B,T,D), and you specify axis="T", you will get (B=1,T,D=1),
  where the specified axis is filled with ``tf.range``.
  See also :class:`RangeLayer`.

  :param LayerRef source:
  :param str axis:
  :param str dtype:
  :param bool sparse:
  :param str|None name:
  """
  mod = RangeInAxis(
    dtype=dtype,
    sparse=sparse,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class RangeFromLength(_Base):
  """
  Given some dynamic sequence lengths as input, this creates a tf.range over the implied dimension.
  As a side effect, this can create a new dyn dim tag for the given sequence lengths.
  This side effect can be the main functionality in certain use cases.
  See also :class:`RangeInAxisLayer`.

  Consider the example::

    y: {class: range_in_axis, from: x, axis: T}

  This is basically equivalent to::

    x_len: {class: length, from: x}
    y: {class: range_from_length, from: x_len}

  """
  returnn_layer_class = 'range_from_length'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               dtype: str = NotSpecified,
               sparse: bool = NotSpecified,
               **kwargs):
    """
    :param str dtype:
    :param bool sparse:
    """
    super().__init__(**kwargs)
    self.dtype = dtype
    self.sparse = sparse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dtype': self.dtype,
      'sparse': self.sparse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'range_from_length',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def range_from_length(
                      source: LayerRef,
                      *,
                      dtype: str = NotSpecified,
                      sparse: bool = NotSpecified,
                      name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Given some dynamic sequence lengths as input, this creates a tf.range over the implied dimension.
  As a side effect, this can create a new dyn dim tag for the given sequence lengths.
  This side effect can be the main functionality in certain use cases.
  See also :class:`RangeInAxisLayer`.

  Consider the example::

    y: {class: range_in_axis, from: x, axis: T}

  This is basically equivalent to::

    x_len: {class: length, from: x}
    y: {class: range_from_length, from: x_len}


  :param LayerRef source:
  :param str dtype:
  :param bool sparse:
  :param str|None name:
  """
  mod = RangeFromLength(
    dtype=dtype,
    sparse=sparse,
    )
  return mod(source, name=name)


class BatchSoftmax(_ConcatInput):
  """
  Softmax over spacial and feature axis
  """
  returnn_layer_class = 'batch_softmax'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'batch_softmax',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def batch_softmax(
                  source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                  *,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Softmax over spacial and feature axis

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|None name:
  """
  mod = BatchSoftmax()
  return mod(source, name=name)


class Constant(_Base):
  """
  Output is a constant value.
  """
  returnn_layer_class = 'constant'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               value: Union[int, float, bool] = NotSpecified,
               dtype: Optional[str] = NotSpecified,
               with_batch_dim: bool = NotSpecified,
               **kwargs):
    """
    :param int|float|bool value:
    :param str|None dtype:
    :param bool with_batch_dim:
    """
    super().__init__(**kwargs)
    self.value = value
    self.dtype = dtype
    self.with_batch_dim = with_batch_dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'value': self.value,
      'dtype': self.dtype,
      'with_batch_dim': self.with_batch_dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'constant',
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def constant(
             *,
             value: Union[int, float, bool] = NotSpecified,
             dtype: Optional[str] = NotSpecified,
             with_batch_dim: bool = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Output is a constant value.

  :param int|float|bool value:
  :param str|None dtype:
  :param bool with_batch_dim:
  :param str|None name:
  """
  mod = Constant(
    value=value,
    dtype=dtype,
    with_batch_dim=with_batch_dim,
    )
  return mod(
    name=name)


class Window(_ConcatInput):
  """
  Adds a window dimension.
  By default, uses the time axis and goes over it with a sliding window.
  The new axis for the window is created right after the time axis.
  Will always return as batch major mode.
  E.g. if the input is (batch, time, dim), the output is (batch, time, window_size, dim).
  If you want to merge the (window_size, dim) together to (window_size * dim,),
  you can use the MergeDimsLayer, e.g. {"class": "merge_dims", "axes": "except_time"}.
  Use stride==window_size and window_right=window_size - 1 in combination with a
  MergeDimsLayer to achieve feature stacking with right-hand zero padding.

  This is not to take out a window from the time-dimension.
  See :class:`SliceLayer` or :class:`SliceNdLayer`.
  """
  returnn_layer_class = 'window'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               window_size: int,
               window_left: Optional[int] = NotSpecified,
               window_right: Optional[int] = NotSpecified,
               padding: str = NotSpecified,
               stride: int = NotSpecified,
               **kwargs):
    """
    :param int window_size:
    :param int|None window_left:
    :param int|None window_right:
    :param str padding: "same" or "valid"
    :param int stride: return only each Nth window
    """
    super().__init__(**kwargs)
    self.window_size = window_size
    self.window_left = window_left
    self.window_right = window_right
    self.padding = padding
    self.stride = stride

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'window_size': self.window_size,
      'window_left': self.window_left,
      'window_right': self.window_right,
      'padding': self.padding,
      'stride': self.stride,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      *,
                      axis: str = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'window',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def window(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
           *,
           window_size: int,
           window_left: Optional[int] = NotSpecified,
           window_right: Optional[int] = NotSpecified,
           axis: str = NotSpecified,
           padding: str = NotSpecified,
           stride: int = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds a window dimension.
  By default, uses the time axis and goes over it with a sliding window.
  The new axis for the window is created right after the time axis.
  Will always return as batch major mode.
  E.g. if the input is (batch, time, dim), the output is (batch, time, window_size, dim).
  If you want to merge the (window_size, dim) together to (window_size * dim,),
  you can use the MergeDimsLayer, e.g. {"class": "merge_dims", "axes": "except_time"}.
  Use stride==window_size and window_right=window_size - 1 in combination with a
  MergeDimsLayer to achieve feature stacking with right-hand zero padding.

  This is not to take out a window from the time-dimension.
  See :class:`SliceLayer` or :class:`SliceNdLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param int window_size:
  :param int|None window_left:
  :param int|None window_right:
  :param str axis: see Data.get_axis_from_description()
  :param str padding: "same" or "valid"
  :param int stride: return only each Nth window
  :param str|None name:
  """
  mod = Window(
    window_size=window_size,
    window_left=window_left,
    window_right=window_right,
    padding=padding,
    stride=stride,
    )
  return mod(
    source,
    state,
    axis=axis,
    name=name)


class Cumsum(_ConcatInput):
  """
  Basically wraps tf.cumsum. Also supports that in the RecLayer.
  """
  returnn_layer_class = 'cumsum'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               additional_left_summand_per_element: Optional[Union[str, int, float]] = NotSpecified,
               reverse: bool = NotSpecified,
               **kwargs):
    """
    :param str|int|float|None additional_left_summand_per_element: the order matters for tf.string
    :param bool reverse:
    """
    super().__init__(**kwargs)
    self.additional_left_summand_per_element = additional_left_summand_per_element
    self.reverse = reverse

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'additional_left_summand_per_element': self.additional_left_summand_per_element,
      'reverse': self.reverse,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      *,
                      axis: str = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'cumsum',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def cumsum(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
           *,
           axis: str = NotSpecified,
           additional_left_summand_per_element: Optional[Union[str, int, float]] = NotSpecified,
           reverse: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Basically wraps tf.cumsum. Also supports that in the RecLayer.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param str axis: see :func:`Data.get_axis_from_description`
  :param str|int|float|None additional_left_summand_per_element: the order matters for tf.string
  :param bool reverse:
  :param str|None name:
  """
  mod = Cumsum(
    additional_left_summand_per_element=additional_left_summand_per_element,
    reverse=reverse,
    )
  return mod(
    source,
    state,
    axis=axis,
    name=name)


class Pad(_ConcatInput):
  """
  Adds (e.g. zero) padding in some axis or axes.
  """
  returnn_layer_class = 'pad'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               padding: Any,
               value: Union[int, float] = NotSpecified,
               mode: str = NotSpecified,
               **kwargs):
    """
    :param list[(int,int)]|(int,int)|int padding: how much to pad left/right in each axis
    :param int|float value: what constant value to pad, with mode=="constant"
    :param str mode: "constant", "reflect", "symmetric" and "replication"
    """
    super().__init__(**kwargs)
    self.padding = padding
    self.value = value
    self.mode = mode

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'padding': self.padding,
      'value': self.value,
      'mode': self.mode,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axes: Any,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axes': axes,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'pad',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def pad(
        source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
        *,
        axes: Any,
        padding: Any,
        value: Union[int, float] = NotSpecified,
        mode: str = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds (e.g. zero) padding in some axis or axes.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|list[str] axes: e.g. "F" etc. see :func:`Dataset.get_axes_from_description`.
  :param list[(int,int)]|(int,int)|int padding: how much to pad left/right in each axis
  :param int|float value: what constant value to pad, with mode=="constant"
  :param str mode: "constant", "reflect", "symmetric" and "replication"
  :param str|None name:
  """
  mod = Pad(
    padding=padding,
    value=value,
    mode=mode,
    )
  return mod(
    source,
    axes=axes,
    name=name)


class MergeDims(_ConcatInput):
  """
  Merges a list of axes into a single one. (Flatten the dims.)
  E.g. input is (batch, width, height, dim) and axes=(1,2), then we get (batch, width*height, dim).
  Or input is (batch, time, height, dim) and axes="except_time", then we get (batch, time, height*dim).
  See also :class:`CombineDimsLayer`.
  When batch and time got merged, :class:`SplitBatchTimeLayer` can undo this.
  When you want to merge batch and time, but remove the padding efficiently, i.e. flatten it,
  see :class:`FlattenBatchLayer`.
  """
  returnn_layer_class = 'merge_dims'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               keep_order: bool = NotSpecified,
               **kwargs):
    """
    :param bool keep_order: By default (for historical reasons), the axes are sorted, and then merged.
      Thus, the order of incoming axes will influence the result.
      E.g. inputs [B,S,F] and [B,F,S], with ``axes=["S","F"]``, will get different results,
      although the output shape is [B,S*F] in both cases.
      This is bad: In general, other layers in RETURNN might reorder the axes for various reasons,
      and all layers should behave in the same way, no matter the order.
      It is recommended to set ``keep_order=True``, such that the order defined in ``axes`` defines the behavior,
      and not the incoming axis order.
    """
    super().__init__(**kwargs)
    self.keep_order = keep_order

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'keep_order': self.keep_order,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axes: Any,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axes': axes,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'merge_dims',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def merge_dims(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               axes: Any,
               keep_order: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Merges a list of axes into a single one. (Flatten the dims.)
  E.g. input is (batch, width, height, dim) and axes=(1,2), then we get (batch, width*height, dim).
  Or input is (batch, time, height, dim) and axes="except_time", then we get (batch, time, height*dim).
  See also :class:`CombineDimsLayer`.
  When batch and time got merged, :class:`SplitBatchTimeLayer` can undo this.
  When you want to merge batch and time, but remove the padding efficiently, i.e. flatten it,
  see :class:`FlattenBatchLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|list[str]|list[int] axes: see Data.get_axes_from_description(), e.g. "except_time"
  :param bool keep_order: By default (for historical reasons), the axes are sorted, and then merged.
    Thus, the order of incoming axes will influence the result.
    E.g. inputs [B,S,F] and [B,F,S], with ``axes=["S","F"]``, will get different results,
    although the output shape is [B,S*F] in both cases.
    This is bad: In general, other layers in RETURNN might reorder the axes for various reasons,
    and all layers should behave in the same way, no matter the order.
    It is recommended to set ``keep_order=True``, such that the order defined in ``axes`` defines the behavior,
    and not the incoming axis order.
  :param str|None name:
  """
  mod = MergeDims(
    keep_order=keep_order,
    )
  return mod(
    source,
    axes=axes,
    name=name)


class _Split(_ConcatInput):
  """
  Splits one axis into multiple parts, via tf.split.
  self.output is simply the input copied.
  Each part can be accessed via the sublayers "/%i".
  """
  returnn_layer_class = 'split'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               num_splits: Optional[int] = NotSpecified,
               size_splits: Optional[List[int]] = NotSpecified,
               **kwargs):
    """
    :param int|None num_splits:
    :param list[int]|None size_splits:
    """
    super().__init__(**kwargs)
    self.num_splits = num_splits
    self.size_splits = size_splits

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'num_splits': self.num_splits,
      'size_splits': self.size_splits,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Optional[str] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'split',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def _split(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           *,
           axis: Optional[str] = NotSpecified,
           num_splits: Optional[int] = NotSpecified,
           size_splits: Optional[List[int]] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Splits one axis into multiple parts, via tf.split.
  self.output is simply the input copied.
  Each part can be accessed via the sublayers "/%i".

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|None axis: feature axis by default
  :param int|None num_splits:
  :param list[int]|None size_splits:
  :param str|None name:
  """
  mod = _Split(
    num_splits=num_splits,
    size_splits=size_splits,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class SplitDims(_ConcatInput):
  """
  Splits one axis into multiple axes.
  E.g. if you know that your feature-dim is composed by a window,
  i.e. the input is (batch, time, window * feature),
  you can set axis="F", dims=(window, -1),
  and you will get the output (batch, time, window, feature).

  If the split axis has a dynamic length,
  exactly one of the axes that we split into need to also have a dynamic length.
  You can e.g. use this to split the input dimension into smaller "chunks" of a fixed window size.
  E.g. you could have input (batch, time, feature) and set axis="T", dims=(-1, window),
  to get output (batch, split_time, window, feature).
  In this case, the exact sequence lengths are lost and everything is padded to multiples of the window size using
  the given padding value.
  Use :class:`ReinterpretDataLayer` to receive back the original sequence lengths after merging.

  Also see :class:`SplitBatchTimeLayer`.
  Also see :class:`MergeDimsLayer` which can undo this operation.
  """
  returnn_layer_class = 'split_dims'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               dims: Any,
               pad_to_multiples: Optional[bool] = NotSpecified,
               pad_value: Union[int, float] = NotSpecified,
               **kwargs):
    """
    :param tuple[int]|list[int] dims: what the axis should be split into. e.g. (window, -1)
    :param bool|None pad_to_multiples: If true, input will be padded to the next multiple of the product of the
      static dims, such that splitting is actually possible.
      By default this is done iff the axis has a dynamic size
    :param int|float pad_value: What pad value to use for pad_to_multiples
    """
    super().__init__(**kwargs)
    self.dims = dims
    self.pad_to_multiples = pad_to_multiples
    self.pad_value = pad_value

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dims': self.dims,
      'pad_to_multiples': self.pad_to_multiples,
      'pad_value': self.pad_value,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: str,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'split_dims',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def split_dims(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               axis: str,
               dims: Any,
               pad_to_multiples: Optional[bool] = NotSpecified,
               pad_value: Union[int, float] = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Splits one axis into multiple axes.
  E.g. if you know that your feature-dim is composed by a window,
  i.e. the input is (batch, time, window * feature),
  you can set axis="F", dims=(window, -1),
  and you will get the output (batch, time, window, feature).

  If the split axis has a dynamic length,
  exactly one of the axes that we split into need to also have a dynamic length.
  You can e.g. use this to split the input dimension into smaller "chunks" of a fixed window size.
  E.g. you could have input (batch, time, feature) and set axis="T", dims=(-1, window),
  to get output (batch, split_time, window, feature).
  In this case, the exact sequence lengths are lost and everything is padded to multiples of the window size using
  the given padding value.
  Use :class:`ReinterpretDataLayer` to receive back the original sequence lengths after merging.

  Also see :class:`SplitBatchTimeLayer`.
  Also see :class:`MergeDimsLayer` which can undo this operation.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str axis: e.g. "F"
  :param tuple[int]|list[int] dims: what the axis should be split into. e.g. (window, -1)
  :param bool|None pad_to_multiples: If true, input will be padded to the next multiple of the product of the
    static dims, such that splitting is actually possible.
    By default this is done iff the axis has a dynamic size
  :param int|float pad_value: What pad value to use for pad_to_multiples
  :param str|None name:
  """
  mod = SplitDims(
    dims=dims,
    pad_to_multiples=pad_to_multiples,
    pad_value=pad_value,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class SplitBatchTime(_ConcatInput):
  """
  A very specific layer which expects to get input of shape (batch * time, ...)
  and converts it into (batch, time, ...), where it recovers the seq-lens from some other layer.
  See :class:`SplitDimsLayer` for a more generic layer.
  """
  returnn_layer_class = 'split_batch_time'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      base: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'base': base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'split_batch_time',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def split_batch_time(
                     source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                     *,
                     base: LayerRef,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A very specific layer which expects to get input of shape (batch * time, ...)
  and converts it into (batch, time, ...), where it recovers the seq-lens from some other layer.
  See :class:`SplitDimsLayer` for a more generic layer.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase base: used to recover the seq-lens
  :param str|None name:
  """
  mod = SplitBatchTime()
  return mod(
    source,
    base=base,
    name=name)


class FlattenBatch(_ConcatInput):
  """
  Merges one axis into the batch axis.
  If the axis has dynamic lengths, this would use flattening,
  i.e. recalculate the padding, i.e. the size changes.
  This basically wraps :func:`flatten_with_seq_len_mask` or :func:`flatten_with_seq_len_mask_time_major`.
  See also :class:`MergeDimsLayer`, which does not do flattening,
  i.e. the size stays the same.
  """
  returnn_layer_class = 'flatten_batch'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               batch_major: bool = NotSpecified,
               **kwargs):
    """
    :param bool batch_major: if False, will flatten in time-major manner
    """
    super().__init__(**kwargs)
    self.batch_major = batch_major

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'batch_major': self.batch_major,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: str = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'flatten_batch',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def flatten_batch(
                  source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                  *,
                  axis: str = NotSpecified,
                  batch_major: bool = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Merges one axis into the batch axis.
  If the axis has dynamic lengths, this would use flattening,
  i.e. recalculate the padding, i.e. the size changes.
  This basically wraps :func:`flatten_with_seq_len_mask` or :func:`flatten_with_seq_len_mask_time_major`.
  See also :class:`MergeDimsLayer`, which does not do flattening,
  i.e. the size stays the same.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str axis:
  :param bool batch_major: if False, will flatten in time-major manner
  :param str|None name:
  """
  mod = FlattenBatch(
    batch_major=batch_major,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class UnflattenNd(_ConcatInput):
  """
  This keeps the batch axis as-is, i.e. the flattening/unflattening did not happen on the batch axis.

  Example:

    Assumes that the input is of shape (B,T,<Ds>) which represents flattened images,
    where each image is of size width * height.
    We additionally provide these image sizes (shape (B,2)), i.e. (width,height) tuples.
    We return the unflattened images of shape (B,W,H,<Ds>), where W/H are the max width/height.

  This basically wraps :func:`returnn.tf.util.basic.unflatten_nd`.
  """
  returnn_layer_class = 'unflatten_nd'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               num_axes: int,
               **kwargs):
    """
    :param int num_axes:
    """
    super().__init__(**kwargs)
    self.num_axes = num_axes

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'num_axes': self.num_axes,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      sizes: LayerRef,
                      declare_same_sizes_as: Optional[Dict[int, LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'sizes': sizes,
      'declare_same_sizes_as': declare_same_sizes_as,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'unflatten_nd',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def unflatten_nd(
                 source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                 *,
                 sizes: LayerRef,
                 num_axes: int,
                 declare_same_sizes_as: Optional[Dict[int, LayerRef]] = NotSpecified,
                 name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This keeps the batch axis as-is, i.e. the flattening/unflattening did not happen on the batch axis.

  Example:

    Assumes that the input is of shape (B,T,<Ds>) which represents flattened images,
    where each image is of size width * height.
    We additionally provide these image sizes (shape (B,2)), i.e. (width,height) tuples.
    We return the unflattened images of shape (B,W,H,<Ds>), where W/H are the max width/height.

  This basically wraps :func:`returnn.tf.util.basic.unflatten_nd`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase sizes:
  :param int num_axes:
  :param dict[int,LayerBase]|None declare_same_sizes_as:
  :param str|None name:
  """
  mod = UnflattenNd(
    num_axes=num_axes,
    )
  return mod(
    source,
    sizes=sizes,
    declare_same_sizes_as=declare_same_sizes_as,
    name=name)


class Repeat(_ConcatInput):
  """
  A wrapper around tf.repeat, but supports an additional batch axis for the durations
  The sum of the repetitions has to be non-zero for each sequence in the batch.

  This layer can only be used with Tensorflow 1.15.0 or newer.
  """
  returnn_layer_class = 'repeat'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      repetitions: Union[LayerRef, int],
                      axis: str = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'repetitions': repetitions,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'repeat',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def repeat(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           *,
           repetitions: Union[LayerRef, int],
           axis: str = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A wrapper around tf.repeat, but supports an additional batch axis for the durations
  The sum of the repetitions has to be non-zero for each sequence in the batch.

  This layer can only be used with Tensorflow 1.15.0 or newer.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase|int repetitions:
    number of repetitions for each sequence and position in target axis.
    Can be [B,T] or [T,B] or some subset of that shape
  :param str axis: (dynamic) axis for repetition (currently only time axis is supported)
  :param str|None name:
  """
  mod = Repeat()
  return mod(
    source,
    repetitions=repetitions,
    axis=axis,
    name=name)


class Tile(_ConcatInput):
  """
  A wrapper around tf.tile
  """
  returnn_layer_class = 'tile'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               multiples: Dict[str, int],
               **kwargs):
    """
    :param dict[str, int] multiples: number of multiples per axis (axis provided as str)
    """
    super().__init__(**kwargs)
    self.multiples = multiples

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'multiples': self.multiples,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'tile',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def tile(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         multiples: Dict[str, int],
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A wrapper around tf.tile

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param dict[str, int] multiples: number of multiples per axis (axis provided as str)
  :param str|None name:
  """
  mod = Tile(
    multiples=multiples,
    )
  return mod(source, name=name)


class Cast(Copy):
  """
  Cast to some other dtype.
  """
  returnn_layer_class = 'cast'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               dtype: str,
               **kwargs):
    """
    :param str dtype:
    """
    super().__init__(**kwargs)
    self.dtype = dtype

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'dtype': self.dtype,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'cast',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def cast(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         dtype: str,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Cast to some other dtype.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str dtype:
  :param str|None name:
  """
  mod = Cast(
    dtype=dtype,
    )
  return mod(source, name=name)


class ReinterpretData(_ConcatInput):
  """
  Acts like the :class:`CopyLayer` but reinterprets the role of some axes or data.
  """
  returnn_layer_class = 'reinterpret_data'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               switch_axes: Any = NotSpecified,
               set_axes: Any = NotSpecified,
               set_dim_tags: Any = NotSpecified,
               enforce_batch_major: bool = NotSpecified,
               enforce_time_major: bool = NotSpecified,
               set_sparse: Optional[bool] = NotSpecified,
               set_sparse_dim: Union[int, None, NotSpecified] = NotSpecified,
               increase_sparse_dim: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param str|list[str] switch_axes: e.g. "bt" to switch batch and time axes
    :param dict[str,int|str] set_axes:
      This can be used to overwrite the special axes like time_dim_axis or feature_dim_axis.
      For that, use keys "B","T" or "F", and a value via :func:`Data.get_axis_from_description`.
    :param dict[str|DimensionTag,DimensionTag] set_dim_tags: axis -> new dim tag. assigns new dim tags.
      If the dim tag is yet undefined, this will not use same_dim_tags_as (declare_same_as)
      but create a new dim tag.
      This option is useful for generalized self attention (https://github.com/rwth-i6/returnn/issues/391).
    :param bool enforce_batch_major:
    :param bool enforce_time_major:
    :param bool|None set_sparse: if bool, set sparse value to this
    :param int|None|NotSpecified set_sparse_dim: set sparse dim to this. assumes that it is sparse
    :param int|None increase_sparse_dim: add this to the dim. assumes that it is sparse
    """
    super().__init__(**kwargs)
    self.switch_axes = switch_axes
    self.set_axes = set_axes
    self.set_dim_tags = set_dim_tags
    self.enforce_batch_major = enforce_batch_major
    self.enforce_time_major = enforce_time_major
    self.set_sparse = set_sparse
    self.set_sparse_dim = set_sparse_dim
    self.increase_sparse_dim = increase_sparse_dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'switch_axes': self.switch_axes,
      'set_axes': self.set_axes,
      'set_dim_tags': self.set_dim_tags,
      'enforce_batch_major': self.enforce_batch_major,
      'enforce_time_major': self.enforce_time_major,
      'set_sparse': self.set_sparse,
      'set_sparse_dim': self.set_sparse_dim,
      'increase_sparse_dim': self.increase_sparse_dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      size_base: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'size_base': size_base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'reinterpret_data',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def reinterpret_data(
                     source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                     *,
                     switch_axes: Any = NotSpecified,
                     size_base: Optional[LayerRef] = NotSpecified,
                     set_axes: Any = NotSpecified,
                     set_dim_tags: Any = NotSpecified,
                     enforce_batch_major: bool = NotSpecified,
                     enforce_time_major: bool = NotSpecified,
                     set_sparse: Optional[bool] = NotSpecified,
                     set_sparse_dim: Union[int, None, NotSpecified] = NotSpecified,
                     increase_sparse_dim: Optional[int] = NotSpecified,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Acts like the :class:`CopyLayer` but reinterprets the role of some axes or data.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|list[str] switch_axes: e.g. "bt" to switch batch and time axes
  :param LayerBase|None size_base: copy the size_placeholder from the given layer
  :param dict[str,int|str] set_axes:
    This can be used to overwrite the special axes like time_dim_axis or feature_dim_axis.
    For that, use keys "B","T" or "F", and a value via :func:`Data.get_axis_from_description`.
  :param dict[str|DimensionTag,DimensionTag] set_dim_tags: axis -> new dim tag. assigns new dim tags.
    If the dim tag is yet undefined, this will not use same_dim_tags_as (declare_same_as)
    but create a new dim tag.
    This option is useful for generalized self attention (https://github.com/rwth-i6/returnn/issues/391).
  :param bool enforce_batch_major:
  :param bool enforce_time_major:
  :param bool|None set_sparse: if bool, set sparse value to this
  :param int|None|NotSpecified set_sparse_dim: set sparse dim to this. assumes that it is sparse
  :param int|None increase_sparse_dim: add this to the dim. assumes that it is sparse
  :param str|None name:
  """
  mod = ReinterpretData(
    switch_axes=switch_axes,
    set_axes=set_axes,
    set_dim_tags=set_dim_tags,
    enforce_batch_major=enforce_batch_major,
    enforce_time_major=enforce_time_major,
    set_sparse=set_sparse,
    set_sparse_dim=set_sparse_dim,
    increase_sparse_dim=increase_sparse_dim,
    )
  return mod(
    source,
    size_base=size_base,
    name=name)


class Conv(_ConcatInput):
  """
  A generic convolution layer which supports 1D, 2D and 3D convolution.
  Pooling can be done in the separate "pool" layer.
  """
  returnn_layer_class = 'conv'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               filter_size: Tuple[int],
               padding: str,
               strides: Any = NotSpecified,
               dilation_rate: Any = NotSpecified,
               groups: int = NotSpecified,
               input_expand_dims: int = NotSpecified,
               input_add_feature_dim: bool = NotSpecified,
               input_split_feature_dim: Optional[int] = NotSpecified,
               auto_use_channel_first: bool = NotSpecified,
               with_bias: Union[bool, NotSpecified] = NotSpecified,
               activation: Optional[str] = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param int n_out: number of outgoing features
    :param tuple[int] filter_size: (width,), (height,width) or (depth,height,width) for 1D/2D/3D conv.
      the input data ndim must match, or you can add dimensions via input_expand_dims or input_add_feature_dim.
      it will automatically swap the batch-dim to the first axis of the input data.
    :param str padding: "same" or "valid"
    :param int|tuple[int] strides: strides for the spatial dims,
      i.e. length of this tuple should be the same as filter_size, or a single int.
    :param int|tuple[int] dilation_rate: dilation for the spatial dims
    :param int groups: grouped convolution
    :param int input_expand_dims: number of dynamic dims to add to the input
    :param bool input_add_feature_dim: will add a dim at the end and use input-feature-dim == 1,
      and use the original input feature-dim as a spatial dim.
    :param None|int input_split_feature_dim: if set, like input_add_feature_dim it will add a new feature dim
      which is of value input_split_feature_dim, and the original input feature dim
      will be divided by input_split_feature_dim, thus it must be a multiple of that value.
    :param bool auto_use_channel_first: convert the input to NCHW or not
    :param bool|NotSpecified with_bias: if True, will add a bias to the output features. False by default
    :param None|str activation: if set, will apply this function at the end
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.filter_size = filter_size
    self.padding = padding
    self.strides = strides
    self.dilation_rate = dilation_rate
    self.groups = groups
    self.input_expand_dims = input_expand_dims
    self.input_add_feature_dim = input_add_feature_dim
    self.input_split_feature_dim = input_split_feature_dim
    self.auto_use_channel_first = auto_use_channel_first
    self.with_bias = with_bias
    self.activation = activation
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'filter_size': self.filter_size,
      'padding': self.padding,
      'strides': self.strides,
      'dilation_rate': self.dilation_rate,
      'groups': self.groups,
      'input_expand_dims': self.input_expand_dims,
      'input_add_feature_dim': self.input_add_feature_dim,
      'input_split_feature_dim': self.input_split_feature_dim,
      'auto_use_channel_first': self.auto_use_channel_first,
      'with_bias': self.with_bias,
      'activation': self.activation,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      filter: Optional[LayerRef] = NotSpecified,
                      bias: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'conv',
      'from': source,
      **args,
      **self.get_opts()}


class Pool(_ConcatInput):
  """
  A generic N-D pooling layer.
  This would usually be done after a convolution for down-sampling.
  """
  returnn_layer_class = 'pool'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               mode: str,
               pool_size: Tuple[int],
               padding: str = NotSpecified,
               dilation_rate: Any = NotSpecified,
               strides: Any = NotSpecified,
               use_channel_first: bool = NotSpecified,
               **kwargs):
    """
    :param str mode: "max" or "avg"
    :param tuple[int] pool_size: shape of the window of each reduce
    :param str padding: "valid" or "same"
    :param tuple[int]|int dilation_rate:
    :param tuple[int]|int|None strides: in contrast to tf.nn.pool, the default (if it is None) will be set to pool_size
    :param bool use_channel_first: if set, will transform input to NCHW format
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.pool_size = pool_size
    self.padding = padding
    self.dilation_rate = dilation_rate
    self.strides = strides
    self.use_channel_first = use_channel_first

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'pool_size': self.pool_size,
      'padding': self.padding,
      'dilation_rate': self.dilation_rate,
      'strides': self.strides,
      'use_channel_first': self.use_channel_first,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'pool',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def pool(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         mode: str,
         pool_size: Tuple[int],
         padding: str = NotSpecified,
         dilation_rate: Any = NotSpecified,
         strides: Any = NotSpecified,
         use_channel_first: bool = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  A generic N-D pooling layer.
  This would usually be done after a convolution for down-sampling.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str mode: "max" or "avg"
  :param tuple[int] pool_size: shape of the window of each reduce
  :param str padding: "valid" or "same"
  :param tuple[int]|int dilation_rate:
  :param tuple[int]|int|None strides: in contrast to tf.nn.pool, the default (if it is None) will be set to pool_size
  :param bool use_channel_first: if set, will transform input to NCHW format
  :param str|None name:
  """
  mod = Pool(
    mode=mode,
    pool_size=pool_size,
    padding=padding,
    dilation_rate=dilation_rate,
    strides=strides,
    use_channel_first=use_channel_first,
    )
  return mod(source, name=name)


class Dct(_ConcatInput):
  """
  Layer to perform DCT
  Wraps :func:`tf.signal.dct`. For further documentation on the input arguments, refer to
  https://www.tensorflow.org/api_docs/python/tf/signal/dct
  """
  returnn_layer_class = 'dct'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               type: int = NotSpecified,
               n: Optional[int] = NotSpecified,
               norm: Optional[str] = NotSpecified,
               **kwargs):
    """
    :param int type: DCT type to perform. Must be 1, 2, 3, or 4
    :param int|None n: length of the transform
    :param str|None norm: normalization to apply. Must be None or "ortho"
    """
    super().__init__(**kwargs)
    self.type = type
    self.n = n
    self.norm = norm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'type': self.type,
      'n': self.n,
      'norm': self.norm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dct',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def dct(
        source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
        *,
        type: int = NotSpecified,
        n: Optional[int] = NotSpecified,
        norm: Optional[str] = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Layer to perform DCT
  Wraps :func:`tf.signal.dct`. For further documentation on the input arguments, refer to
  https://www.tensorflow.org/api_docs/python/tf/signal/dct

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int type: DCT type to perform. Must be 1, 2, 3, or 4
  :param int|None n: length of the transform
  :param str|None norm: normalization to apply. Must be None or "ortho"
  :param str|None name:
  """
  mod = Dct(
    type=type,
    n=n,
    norm=norm,
    )
  return mod(source, name=name)


class TransposedConv(_ConcatInput):
  """
  Transposed convolution, sometimes also called deconvolution.
  See :func:`tf.nn.conv2d_transpose` (currently we support 1D/2D).
  """
  returnn_layer_class = 'transposed_conv'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               filter_size: List[int],
               activation: Optional[str],
               strides: Optional[List[int]] = NotSpecified,
               padding: str = NotSpecified,
               remove_padding: Any = NotSpecified,
               output_padding: Any = NotSpecified,
               with_bias: bool = NotSpecified,
               forward_weights_init: Any = NotSpecified,
               bias_init: Any = NotSpecified,
               filter_perm: Optional[Dict[str, str]] = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param list[int] filter_size:
    :param str|None activation:
    :param list[int]|None strides: specifies the upscaling. by default, same as filter_size
    :param str padding: "same" or "valid"
    :param list[int]|int remove_padding:
    :param list[int|None]|int|None output_padding:
    :param bool with_bias: whether to add a bias. enabled by default.
      Note that the default is different from ConvLayer!
    :param forward_weights_init:
    :param bias_init:
    :param dict[str,str]|None filter_perm: transposes the filter (input filter as layer)
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.filter_size = filter_size
    self.activation = activation
    self.strides = strides
    self.padding = padding
    self.remove_padding = remove_padding
    self.output_padding = output_padding
    self.with_bias = with_bias
    self.forward_weights_init = forward_weights_init
    self.bias_init = bias_init
    self.filter_perm = filter_perm

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'filter_size': self.filter_size,
      'activation': self.activation,
      'strides': self.strides,
      'padding': self.padding,
      'remove_padding': self.remove_padding,
      'output_padding': self.output_padding,
      'with_bias': self.with_bias,
      'forward_weights_init': self.forward_weights_init,
      'bias_init': self.bias_init,
      'filter_perm': self.filter_perm,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      filter: Optional[LayerRef] = NotSpecified,
                      bias: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'filter': filter,
      'bias': bias,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'transposed_conv',
      'from': source,
      **args,
      **self.get_opts()}


class Reduce(_ConcatInput):
  """
  This reduces some axis by using "sum" or "max".
  It's basically a wrapper around tf.reduce_sum or tf.reduce_max.
  """
  returnn_layer_class = 'reduce'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               mode: str,
               keep_dims: bool = NotSpecified,
               enforce_batch_dim_axis: int = NotSpecified,
               use_time_mask: bool = NotSpecified,
               **kwargs):
    """
    :param str mode: "sum" or "max", "argmin", "min", "argmax", "mean", "logsumexp"
    :param bool keep_dims: if dimensions should be kept (will be 1)
    :param int enforce_batch_dim_axis: will swap the batch-dim-axis of the input with the given axis.
      e.g. 0: will convert the input into batch-major format if not already like that.
      Note that this is still not enough in some cases, e.g. when the other axes are also not as expected.
      The strong recommendation is to use a symbolic axis description.
    :param bool use_time_mask: if we reduce over the time-dim axis, use the seq len info.
      By default, in that case, it will be True.
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.keep_dims = keep_dims
    self.enforce_batch_dim_axis = enforce_batch_dim_axis
    self.use_time_mask = use_time_mask

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'keep_dims': self.keep_dims,
      'enforce_batch_dim_axis': self.enforce_batch_dim_axis,
      'use_time_mask': self.use_time_mask,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axes: Any = NotSpecified,
                      axis: Any = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axes': axes,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'reduce',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def reduce(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           *,
           mode: str,
           axes: Any = NotSpecified,
           axis: Any = NotSpecified,
           keep_dims: bool = NotSpecified,
           enforce_batch_dim_axis: int = NotSpecified,
           use_time_mask: bool = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This reduces some axis by using "sum" or "max".
  It's basically a wrapper around tf.reduce_sum or tf.reduce_max.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str mode: "sum" or "max", "argmin", "min", "argmax", "mean", "logsumexp"
  :param int|list[int]|str axes: One axis or multiple axis to reduce.
    It accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature",
    and it is strongly recommended to use some of these symbolic names.
    See :func:`Data.get_axes_from_description`.
  :param int|list[int]|str axis: for compatibility, can be used instead of ``axes``
  :param bool keep_dims: if dimensions should be kept (will be 1)
  :param int enforce_batch_dim_axis: will swap the batch-dim-axis of the input with the given axis.
    e.g. 0: will convert the input into batch-major format if not already like that.
    Note that this is still not enough in some cases, e.g. when the other axes are also not as expected.
    The strong recommendation is to use a symbolic axis description.
  :param bool use_time_mask: if we reduce over the time-dim axis, use the seq len info.
    By default, in that case, it will be True.
  :param str|None name:
  """
  mod = Reduce(
    mode=mode,
    keep_dims=keep_dims,
    enforce_batch_dim_axis=enforce_batch_dim_axis,
    use_time_mask=use_time_mask,
    )
  return mod(
    source,
    axes=axes,
    axis=axis,
    name=name)


class ReduceOut(_ConcatInput):
  """
  Combination of :class:`SplitDimsLayer` applied to the feature dim
  and :class:`ReduceLayer` applied to the resulting feature dim.
  This can e.g. be used to do maxout.
  """
  returnn_layer_class = 'reduce_out'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               mode: str,
               num_pieces: int,
               **kwargs):
    """
    :param str mode: "sum" or "max" or "mean"
    :param int num_pieces: how many elements to reduce. The output dimension will be input.dim // num_pieces.
    """
    super().__init__(**kwargs)
    self.mode = mode
    self.num_pieces = num_pieces

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'mode': self.mode,
      'num_pieces': self.num_pieces,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'reduce_out',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def reduce_out(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               mode: str,
               num_pieces: int,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Combination of :class:`SplitDimsLayer` applied to the feature dim
  and :class:`ReduceLayer` applied to the resulting feature dim.
  This can e.g. be used to do maxout.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str mode: "sum" or "max" or "mean"
  :param int num_pieces: how many elements to reduce. The output dimension will be input.dim // num_pieces.
  :param str|None name:
  """
  mod = ReduceOut(
    mode=mode,
    num_pieces=num_pieces,
    )
  return mod(source, name=name)


class Squeeze(_ConcatInput):
  """
  Removes an axis with dimension 1.
  This is basically a wrapper around tf.squeeze.
  """
  returnn_layer_class = 'squeeze'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               enforce_batch_dim_axis: Optional[int] = NotSpecified,
               allow_no_op: bool = NotSpecified,
               **kwargs):
    """
    :param int|None enforce_batch_dim_axis:
    :param bool allow_no_op:
    """
    super().__init__(**kwargs)
    self.enforce_batch_dim_axis = enforce_batch_dim_axis
    self.allow_no_op = allow_no_op

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'enforce_batch_dim_axis': self.enforce_batch_dim_axis,
      'allow_no_op': self.allow_no_op,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Any,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'squeeze',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def squeeze(
            source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
            *,
            axis: Any,
            enforce_batch_dim_axis: Optional[int] = NotSpecified,
            allow_no_op: bool = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Removes an axis with dimension 1.
  This is basically a wrapper around tf.squeeze.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int|list[int]|str axis: one axis or multiple axis to squeeze.
    this is counted with batch-dim, which by default is axis 0 (see enforce_batch_dim_axis).
    it also accepts the special tokens "B"|"batch", "spatial", "spatial_except_time", or "F"|"feature"
  :param int|None enforce_batch_dim_axis:
  :param bool allow_no_op:
  :param str|None name:
  """
  mod = Squeeze(
    enforce_batch_dim_axis=enforce_batch_dim_axis,
    allow_no_op=allow_no_op,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class Stack(_Base):
  """
  Stacks multiple inputs together using :func:`tf.stack`.
  This creates a new dimension for the stack.

  For concatenation (in feature dimension), see :class:`CopyLayer`.
  """
  returnn_layer_class = 'stack'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Optional[int] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'stack',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def stack(
          source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
          *,
          axis: Optional[int] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Stacks multiple inputs together using :func:`tf.stack`.
  This creates a new dimension for the stack.

  For concatenation (in feature dimension), see :class:`CopyLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int|None axis: new axis.
    If not given, will use Data.get_default_new_axis_for_dim_tag(<spatial>),
    i.e. some reasonable default for a new spatial axis.
  :param str|None name:
  """
  mod = Stack()
  return mod(
    source,
    axis=axis,
    name=name)


class PrefixInTime(_ConcatInput):
  """
  Adds some prefix in time dimension.
  This is kind of the reverse of :class:`SliceNdLayer` does.
  """
  returnn_layer_class = 'prefix_in_time'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               prefix: Union[float, str] = NotSpecified,
               **kwargs):
    """
    :param float|str prefix: either some constant or another layer
    """
    super().__init__(**kwargs)
    self.prefix = prefix

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'prefix': self.prefix,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      repeat: Union[int, LayerRef] = NotSpecified,
                      size_base: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'repeat': repeat,
      'size_base': size_base,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'prefix_in_time',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def prefix_in_time(
                   source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                   *,
                   prefix: Union[float, str] = NotSpecified,
                   repeat: Union[int, LayerRef] = NotSpecified,
                   size_base: Optional[LayerRef] = NotSpecified,
                   name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds some prefix in time dimension.
  This is kind of the reverse of :class:`SliceNdLayer` does.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float|str prefix: either some constant or another layer
  :param int|LayerBase repeat: how often to repeat the postfix
  :param LayerBase|None size_base: copy seq-lens from here
  :param str|None name:
  """
  mod = PrefixInTime(
    prefix=prefix,
    )
  return mod(
    source,
    repeat=repeat,
    size_base=size_base,
    name=name)


class PostfixInTime(_ConcatInput):
  """
  Adds some postfix in time dimension.
  """
  returnn_layer_class = 'postfix_in_time'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               repeat: int = NotSpecified,
               **kwargs):
    """
    :param int repeat: how often to repeat the postfix
    """
    super().__init__(**kwargs)
    self.repeat = repeat

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'repeat': self.repeat,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      postfix: Union[float, int, LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'postfix': postfix,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'postfix_in_time',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def postfix_in_time(
                    source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                    *,
                    postfix: Union[float, int, LayerRef] = NotSpecified,
                    repeat: int = NotSpecified,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds some postfix in time dimension.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float|int|LayerBase postfix: constant or other layer without time axis to use as postfix
  :param int repeat: how often to repeat the postfix
  :param str|None name:
  """
  mod = PostfixInTime(
    repeat=repeat,
    )
  return mod(
    source,
    postfix=postfix,
    name=name)


class TimeChunking(_ConcatInput):
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.
  """
  returnn_layer_class = 'time_chunking'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               chunk_size: int,
               chunk_step: int,
               **kwargs):
    """
    :param int chunk_size:
    :param int chunk_step:
    """
    super().__init__(**kwargs)
    self.chunk_size = chunk_size
    self.chunk_step = chunk_step

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'chunk_size': self.chunk_size,
      'chunk_step': self.chunk_step,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'time_chunking',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def time_chunking(
                  source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                  *,
                  chunk_size: int,
                  chunk_step: int,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int chunk_size:
  :param int chunk_step:
  :param str|None name:
  """
  mod = TimeChunking(
    chunk_size=chunk_size,
    chunk_step=chunk_step,
    )
  return mod(source, name=name)


class TimeUnChunking(_ConcatInput):
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.
  """
  returnn_layer_class = 'time_unchunking'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      chunking_layer: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'chunking_layer': chunking_layer,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'time_unchunking',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def time_un_chunking(
                     source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                     *,
                     chunking_layer: LayerRef,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Performs chunking in time. See :func:`TFNativeOp.chunk`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param TimeChunkingLayer chunking_layer:
  :param str|None name:
  """
  mod = TimeUnChunking()
  return mod(
    source,
    chunking_layer=chunking_layer,
    name=name)


class Dot(_Base):
  """
  This performs a dot-product of two sources.
  The underlying matmul expects shapes (shared..., I, J) * (shared..., J, K) -> (shared..., I, K).
  We say that J is the axis to be reduced,
  I is the var-dim of source 1, and K is the var-dim of source 2.
  I, J, K can also be multiple axes from the sources.
  The var-dims don't need to exist.
  All other axes (shared...) are expected to match.
  """
  returnn_layer_class = 'dot'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               red1: Any = NotSpecified,
               red2: Any = NotSpecified,
               var1: Any = NotSpecified,
               var2: Any = NotSpecified,
               add_var2_if_empty: bool = NotSpecified,
               debug: bool = NotSpecified,
               **kwargs):
    """
    :param str|int|tuple[str|int]|list[str|int] red1: reduce axes of first source
    :param str|int|tuple[str|int]|list[str|int] red2: reduce axes of second source
    :param str|int|tuple[str|int]|list[str|int]|None var1: var axes of first source
    :param str|int|tuple[str|int]|list[str|int]|None var2: var axes of second source
    :param bool add_var2_if_empty: if var2=None, add dim=1 at the end
    :param bool debug: will print debug shapes, etc.
    """
    super().__init__(**kwargs)
    self.red1 = red1
    self.red2 = red2
    self.var1 = var1
    self.var2 = var2
    self.add_var2_if_empty = add_var2_if_empty
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'red1': self.red1,
      'red2': self.red2,
      'var1': self.var1,
      'var2': self.var2,
      'add_var2_if_empty': self.add_var2_if_empty,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'dot',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def dot(
        source: LayerRef,
        *,
        red1: Any = NotSpecified,
        red2: Any = NotSpecified,
        var1: Any = NotSpecified,
        var2: Any = NotSpecified,
        add_var2_if_empty: bool = NotSpecified,
        debug: bool = NotSpecified,
        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This performs a dot-product of two sources.
  The underlying matmul expects shapes (shared..., I, J) * (shared..., J, K) -> (shared..., I, K).
  We say that J is the axis to be reduced,
  I is the var-dim of source 1, and K is the var-dim of source 2.
  I, J, K can also be multiple axes from the sources.
  The var-dims don't need to exist.
  All other axes (shared...) are expected to match.

  :param LayerRef source:
  :param str|int|tuple[str|int]|list[str|int] red1: reduce axes of first source
  :param str|int|tuple[str|int]|list[str|int] red2: reduce axes of second source
  :param str|int|tuple[str|int]|list[str|int]|None var1: var axes of first source
  :param str|int|tuple[str|int]|list[str|int]|None var2: var axes of second source
  :param bool add_var2_if_empty: if var2=None, add dim=1 at the end
  :param bool debug: will print debug shapes, etc.
  :param str|None name:
  """
  mod = Dot(
    red1=red1,
    red2=red2,
    var1=var1,
    var2=var2,
    add_var2_if_empty=add_var2_if_empty,
    debug=debug,
    )
  return mod(source, name=name)


class ShiftAxis(_ConcatInput):
  """
  Shifts the dimensions in an axis around.
  This layer may change the axis-dimension.

  This name might be confusing. No axis will be shifted here. See :class:`SwapAxesLayer` for that.
  """
  returnn_layer_class = 'shift_axis'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               amount: int,
               pad: bool = NotSpecified,
               adjust_size_info: bool = NotSpecified,
               **kwargs):
    """
    :param int amount: number of elements to shift
                   (<0 for left-shift, >0 for right-shift)
    :param bool pad: preserve shape by padding
    :param bool adjust_size_info: whether to adjust the size_placeholder
    """
    super().__init__(**kwargs)
    self.amount = amount
    self.pad = pad
    self.adjust_size_info = adjust_size_info

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'amount': self.amount,
      'pad': self.pad,
      'adjust_size_info': self.adjust_size_info,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Union[str, int],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'shift_axis',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def shift_axis(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               *,
               axis: Union[str, int],
               amount: int,
               pad: bool = NotSpecified,
               adjust_size_info: bool = NotSpecified,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Shifts the dimensions in an axis around.
  This layer may change the axis-dimension.

  This name might be confusing. No axis will be shifted here. See :class:`SwapAxesLayer` for that.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str|int axis: single axis to shift
  :param int amount: number of elements to shift
                 (<0 for left-shift, >0 for right-shift)
  :param bool pad: preserve shape by padding
  :param bool adjust_size_info: whether to adjust the size_placeholder
  :param str|None name:
  """
  mod = ShiftAxis(
    amount=amount,
    pad=pad,
    adjust_size_info=adjust_size_info,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class Resize(_ConcatInput):
  """
  Resizes the input, i.e. upsampling or downsampling.
  Supports different kinds, such as linear interpolation or nearest-neighbor.
  """
  returnn_layer_class = 'resize'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               factor: int,
               kind: str = NotSpecified,
               fill_value: Optional[Union[int, float]] = NotSpecified,
               fill_dropout: float = NotSpecified,
               **kwargs):
    """
    :param int factor:
    :param str kind: "linear", "nn"/"nearest_neighbor", "cubic", "fill"
    :param None|int|float fill_value: if kind=="fill"
    :param float fill_dropout: if set, will dropout in the same axis
    """
    super().__init__(**kwargs)
    self.factor = factor
    self.kind = kind
    self.fill_value = fill_value
    self.fill_dropout = fill_dropout

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'factor': self.factor,
      'kind': self.kind,
      'fill_value': self.fill_value,
      'fill_dropout': self.fill_dropout,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      axis: Union[str, int],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'resize',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def resize(
           source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
           *,
           factor: int,
           axis: Union[str, int],
           kind: str = NotSpecified,
           fill_value: Optional[Union[int, float]] = NotSpecified,
           fill_dropout: float = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Resizes the input, i.e. upsampling or downsampling.
  Supports different kinds, such as linear interpolation or nearest-neighbor.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int factor:
  :param str|int axis: the axis to resize, counted with batch-dim. can also be "T" for time
  :param str kind: "linear", "nn"/"nearest_neighbor", "cubic", "fill"
  :param None|int|float fill_value: if kind=="fill"
  :param float fill_dropout: if set, will dropout in the same axis
  :param str|None name:
  """
  mod = Resize(
    factor=factor,
    kind=kind,
    fill_value=fill_value,
    fill_dropout=fill_dropout,
    )
  return mod(
    source,
    axis=axis,
    name=name)


class Remove(_Base):
  """
  Currently, assumes sparse data, and removes a specific symbol from the data.

  It is recommended to use :class:`MaskedComputationLayer` in combination with e.g.
  a :class:CompareLayer` instead, as this provides more flexibility.
  """
  returnn_layer_class = 'remove'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               symbol: int,
               **kwargs):
    """
    :param int symbol:
    """
    super().__init__(**kwargs)
    self.symbol = symbol

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'symbol': self.symbol,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'remove',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def remove(
           source: LayerRef,
           *,
           symbol: int,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Currently, assumes sparse data, and removes a specific symbol from the data.

  It is recommended to use :class:`MaskedComputationLayer` in combination with e.g.
  a :class:CompareLayer` instead, as this provides more flexibility.

  :param LayerRef source:
  :param int symbol:
  :param str|None name:
  """
  mod = Remove(
    symbol=symbol,
    )
  return mod(source, name=name)


class _Combine(_Base):
  """
  Applies a binary operation, such as addition, to all sources while accumulating the partial results.
  In the first step, the binary operation is performed on the first two sources.
  After the first step, the previous results is always the left-hand operator.

  Its basic working is similar to the `reduce` function used in functional programming.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.
  """
  returnn_layer_class = 'combine'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               kind: str,
               activation: Optional[str] = NotSpecified,
               with_bias: bool = NotSpecified,
               eval: Union[str, callable] = NotSpecified,
               eval_locals: Optional[Dict[str]] = NotSpecified,
               eval_for_output_loss: bool = NotSpecified,
               **kwargs):
    """
    :param str kind:
      currently accepted values are `average`, `add`, `sub`, `mul`, `truediv`, `logical_and`, `logical_or`, or `eval`
    :param str|None activation: if provided, activation function to apply, e.g. "tanh" or "relu"
    :param bool with_bias: if given, will add a trainable bias tensor
    :param str|callable eval: for kind="eval", will eval this string. or function. see :func:`_op_kind_eval`
    :param dict[str]|None eval_locals: locals for eval
    :param bool eval_for_output_loss: will do the same eval on layer.output_loss
    """
    super().__init__(**kwargs)
    self.kind = kind
    self.activation = activation
    self.with_bias = with_bias
    self.eval = eval
    self.eval_locals = eval_locals
    self.eval_for_output_loss = eval_for_output_loss

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'kind': self.kind,
      'activation': self.activation,
      'with_bias': self.with_bias,
      'eval': self.eval,
      'eval_locals': self.eval_locals,
      'eval_for_output_loss': self.eval_for_output_loss,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'combine',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def _combine(
             source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
             *,
             kind: str,
             activation: Optional[str] = NotSpecified,
             with_bias: bool = NotSpecified,
             eval: Union[str, callable] = NotSpecified,
             eval_locals: Optional[Dict[str]] = NotSpecified,
             eval_for_output_loss: bool = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Applies a binary operation, such as addition, to all sources while accumulating the partial results.
  In the first step, the binary operation is performed on the first two sources.
  After the first step, the previous results is always the left-hand operator.

  Its basic working is similar to the `reduce` function used in functional programming.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str kind:
    currently accepted values are `average`, `add`, `sub`, `mul`, `truediv`, `logical_and`, `logical_or`, or `eval`
  :param str|None activation: if provided, activation function to apply, e.g. "tanh" or "relu"
  :param bool with_bias: if given, will add a trainable bias tensor
  :param str|callable eval: for kind="eval", will eval this string. or function. see :func:`_op_kind_eval`
  :param dict[str]|None eval_locals: locals for eval
  :param bool eval_for_output_loss: will do the same eval on layer.output_loss
  :param str|None name:
  """
  mod = _Combine(
    kind=kind,
    activation=activation,
    with_bias=with_bias,
    eval=eval,
    eval_locals=eval_locals,
    eval_for_output_loss=eval_for_output_loss,
    )
  return mod(source, name=name)


class Eval(_Combine):
  """
  Evaluates some string.
  The :class:`CombineLayer` provides this functionality, thus this is just a special case of it.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  The output type is defined as a broadcasted extension of all sources.
  You can overwrite it by (partially) specifying `out_type`.
  `out_type` can also be a generic Python function, returning a `Data` instance.
  """
  returnn_layer_class = 'eval'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               eval: str,
               **kwargs):
    """
    :param str eval: will eval this string. see :func:`_op_kind_eval`
    """
    super().__init__(kind="eval", eval=eval, **kwargs)
    self.eval = eval

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'eval': self.eval,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    opts.update(super().get_opts())
    opts.pop('kind')
    return opts

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'eval',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def eval(
         source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
         *,
         eval: str,
         activation: Optional[str] = NotSpecified,
         with_bias: bool = NotSpecified,
         eval_locals: Optional[Dict[str]] = NotSpecified,
         eval_for_output_loss: bool = NotSpecified,
         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Evaluates some string.
  The :class:`CombineLayer` provides this functionality, thus this is just a special case of it.
  Also see :class:`ActivationLayer`, or :class:`CompareLayer`.

  The output type is defined as a broadcasted extension of all sources.
  You can overwrite it by (partially) specifying `out_type`.
  `out_type` can also be a generic Python function, returning a `Data` instance.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str eval: will eval this string. see :func:`_op_kind_eval`
  :param str|None activation: if provided, activation function to apply, e.g. "tanh" or "relu"
  :param bool with_bias: if given, will add a trainable bias tensor
  :param dict[str]|None eval_locals: locals for eval
  :param bool eval_for_output_loss: will do the same eval on layer.output_loss
  :param str|None name:
  """
  mod = Eval(
    eval=eval,
    activation=activation,
    with_bias=with_bias,
    eval_locals=eval_locals,
    eval_for_output_loss=eval_for_output_loss,
    )
  return mod(source, name=name)


class Compare(_Base):
  """
  Compares element-wise the tokens of all input sequences among themselves and/or with a specified given value.
  The comparisons are performed in a chain according to the order in which they are listed.

  Example::

      {"class": "compare", "from": ["i1", "i2"], "value": val, "kind": "less"}

  computes i1 < i2 < val and it is true only if the whole chain of operations is true.
  The final result is the logical "and" of all comparisons. Note that `value` is the last element to be compared to.

  A common example usage is the `end` layer in a rec subnetwork to specify the stopping criterion,
  e.g. the last generated token is equal to the end-of-sentence token::

      "output": {"class": "rec", "from": [], "unit": {
          .
          .
          .
          "end": {"class": "compare", "from": "output", "value": end_of_sentence_id}
      }, "target": "classes0"}

  """
  returnn_layer_class = 'compare'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               kind: str = NotSpecified,
               value: Optional[Union[float, int]] = NotSpecified,
               **kwargs):
    """
    :param str kind: which comparison operation to use, e.g. "equal", "greater", "less"
      or other supported TF comparison ops
    :param float|int|None value: if specified, will also compare to this
    """
    super().__init__(**kwargs)
    self.kind = kind
    self.value = value

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'kind': self.kind,
      'value': self.value,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'compare',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def compare(
            source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
            *,
            kind: str = NotSpecified,
            value: Optional[Union[float, int]] = NotSpecified,
            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Compares element-wise the tokens of all input sequences among themselves and/or with a specified given value.
  The comparisons are performed in a chain according to the order in which they are listed.

  Example::

      {"class": "compare", "from": ["i1", "i2"], "value": val, "kind": "less"}

  computes i1 < i2 < val and it is true only if the whole chain of operations is true.
  The final result is the logical "and" of all comparisons. Note that `value` is the last element to be compared to.

  A common example usage is the `end` layer in a rec subnetwork to specify the stopping criterion,
  e.g. the last generated token is equal to the end-of-sentence token::

      "output": {"class": "rec", "from": [], "unit": {
          .
          .
          .
          "end": {"class": "compare", "from": "output", "value": end_of_sentence_id}
      }, "target": "classes0"}


  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str kind: which comparison operation to use, e.g. "equal", "greater", "less"
    or other supported TF comparison ops
  :param float|int|None value: if specified, will also compare to this
  :param str|None name:
  """
  mod = Compare(
    kind=kind,
    value=value,
    )
  return mod(source, name=name)


class Switch(_Base):
  """
  Wrapper around ``tf.where()`` (or more generically :func:`returnn.tf.util.basic.where_bc`),
  or statically choose a single source if the condition is a callable (...)->bool.
  (``tf.cond`` is not useful here, as the sources would have been already constructed and computed.)

  This layer is also useful for applying any kind of generic masking to the frames.
  E.g. one could have a layer called "mask" computing a boolean mask for the values stored in another layer "input".
  Then use this layer with condition="mask", true_from="input", false_from=mask_value,
  to mask out all frames where the mask is false with the mask_value.

  See also :class:`CondLayer`.
  See also :class:`SeqLenMaskLayer` if you just want to mask using the sequence lengths.
  """
  returnn_layer_class = 'switch'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      *,
                      condition: Union[LayerRef, bool],
                      true_from: Optional[Union[LayerRef, float, int]],
                      false_from: Optional[Union[LayerRef, float, int]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'condition': condition,
      'true_from': true_from,
      'false_from': false_from,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'switch',
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def switch(
           *,
           condition: Union[LayerRef, bool],
           true_from: Optional[Union[LayerRef, float, int]],
           false_from: Optional[Union[LayerRef, float, int]],
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Wrapper around ``tf.where()`` (or more generically :func:`returnn.tf.util.basic.where_bc`),
  or statically choose a single source if the condition is a callable (...)->bool.
  (``tf.cond`` is not useful here, as the sources would have been already constructed and computed.)

  This layer is also useful for applying any kind of generic masking to the frames.
  E.g. one could have a layer called "mask" computing a boolean mask for the values stored in another layer "input".
  Then use this layer with condition="mask", true_from="input", false_from=mask_value,
  to mask out all frames where the mask is false with the mask_value.

  See also :class:`CondLayer`.
  See also :class:`SeqLenMaskLayer` if you just want to mask using the sequence lengths.

  :param LayerBase|bool condition: if callable, expected to be (...)->bool, and called in transform_config_dict
  :param LayerBase|float|int|None true_from:
  :param LayerBase|float|int|None false_from:
  :param str|None name:
  """
  mod = Switch()
  return mod(
    condition=condition,
    true_from=true_from,
    false_from=false_from,
    name=name)


class SearchSorted(_Base):
  """
  Basically wraps :func:`tf.searchsorted`.

  Takes a tensor `sorted_sequence` that is sorted along one axis, and a tensor `values`.
  Will compute an output tensor with the same axes as `values`,
  where each entry is the index of the value within the sorted sequence.
  All (batch) axes of `sorted_sequence` except for the axis it is sorted along must be present in `values`.
  """
  returnn_layer_class = 'search_sorted'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               side: str = NotSpecified,
               **kwargs):
    """
    :param str side: "left" or "right".
      When one of the `values` exactly matches an element of the `sorted_sequence`,
      whether to choose the lower or higher index.
    """
    super().__init__(**kwargs)
    self.side = side

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'side': self.side,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      sorted_sequence: LayerRef,
                      values: LayerRef,
                      axis: str = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'sorted_sequence': sorted_sequence,
      'values': values,
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'search_sorted',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def search_sorted(
                  source: LayerRef,
                  *,
                  sorted_sequence: LayerRef,
                  values: LayerRef,
                  axis: str = NotSpecified,
                  side: str = NotSpecified,
                  name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Basically wraps :func:`tf.searchsorted`.

  Takes a tensor `sorted_sequence` that is sorted along one axis, and a tensor `values`.
  Will compute an output tensor with the same axes as `values`,
  where each entry is the index of the value within the sorted sequence.
  All (batch) axes of `sorted_sequence` except for the axis it is sorted along must be present in `values`.

  :param LayerRef source:
  :param LayerBase sorted_sequence:
  :param LayerBase values: search values
  :param str axis: the axis along which `sorted_sequence` is sorted
  :param str side: "left" or "right".
    When one of the `values` exactly matches an element of the `sorted_sequence`,
    whether to choose the lower or higher index.
  :param str|None name:
  """
  mod = SearchSorted(
    side=side,
    )
  return mod(
    source,
    sorted_sequence=sorted_sequence,
    values=values,
    axis=axis,
    name=name)


class Variable(_Base):
  """
  Represents a variable. Can add batch/time dimension if wanted. Can be trainable.
  See defaults.
  """
  returnn_layer_class = 'variable'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               shape: Any,
               dtype: str = NotSpecified,
               add_batch_axis: bool = NotSpecified,
               add_time_axis: bool = NotSpecified,
               trainable: bool = NotSpecified,
               init: Union[str, float, int] = NotSpecified,
               **kwargs):
    """
    :param tuple[int]|list[int] shape:
    :param str dtype:
    :param bool add_batch_axis:
    :param bool add_time_axis:
    :param bool trainable:
    :param str|float|int init: see :func:`returnn.tf.util.basic.get_initializer`
    """
    super().__init__(trainable=trainable, **kwargs)
    self.shape = shape
    self.dtype = dtype
    self.add_batch_axis = add_batch_axis
    self.add_time_axis = add_time_axis
    self.trainable = trainable
    self.init = init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'shape': self.shape,
      'dtype': self.dtype,
      'add_batch_axis': self.add_batch_axis,
      'add_time_axis': self.add_time_axis,
      'trainable': self.trainable,
      'init': self.init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  def make_layer_dict(self) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'variable',
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def variable(
             *,
             shape: Any,
             dtype: str = NotSpecified,
             add_batch_axis: bool = NotSpecified,
             add_time_axis: bool = NotSpecified,
             trainable: bool = NotSpecified,
             init: Union[str, float, int] = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Represents a variable. Can add batch/time dimension if wanted. Can be trainable.
  See defaults.

  :param tuple[int]|list[int] shape:
  :param str dtype:
  :param bool add_batch_axis:
  :param bool add_time_axis:
  :param bool trainable:
  :param str|float|int init: see :func:`returnn.tf.util.basic.get_initializer`
  :param str|None name:
  """
  mod = Variable(
    shape=shape,
    dtype=dtype,
    add_batch_axis=add_batch_axis,
    add_time_axis=add_time_axis,
    trainable=trainable,
    init=init,
    )
  return mod(
    name=name)


class ForcedAlignment(_ConcatInput):
  """
  Calculates a forced alignment, via Viterbi algorithm.
  """
  returnn_layer_class = 'forced_align'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               topology: str,
               input_type: str,
               **kwargs):
    """
    :param str topology: e.g. "ctc" or "rna" (RNA is CTC without label loop)
    :param str input_type: "log_prob" or "prob"
    """
    super().__init__(**kwargs)
    self.topology = topology
    self.input_type = input_type

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'topology': self.topology,
      'input_type': self.input_type,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      align_target: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'align_target': align_target,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'forced_align',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def forced_alignment(
                     source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                     *,
                     align_target: LayerRef,
                     topology: str,
                     input_type: str,
                     name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calculates a forced alignment, via Viterbi algorithm.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase align_target:
  :param str topology: e.g. "ctc" or "rna" (RNA is CTC without label loop)
  :param str input_type: "log_prob" or "prob"
  :param str|None name:
  """
  mod = ForcedAlignment(
    topology=topology,
    input_type=input_type,
    )
  return mod(
    source,
    align_target=align_target,
    name=name)


class FastBaumWelch(_ConcatInput):
  """
  Calls :func:`fast_baum_welch` or :func:`fast_baum_welch_by_sprint_automata`.
  We expect that our input are +log scores, e.g. use log-softmax.
  """
  returnn_layer_class = 'fast_bw'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               align_target: str,
               align_target_key: Optional[str] = NotSpecified,
               ctc_opts: Dict[str] = NotSpecified,
               sprint_opts: Dict[str] = NotSpecified,
               input_type: str = NotSpecified,
               tdp_scale: float = NotSpecified,
               am_scale: float = NotSpecified,
               min_prob: float = NotSpecified,
               **kwargs):
    """
    :param str align_target: e.g. "sprint" or "staircase"
    :param str|None align_target_key: e.g. "classes", used for e.g. align_target "ctc"
    :param dict[str] ctc_opts: used for align_target "ctc"
    :param dict[str] sprint_opts: used for Sprint (RASR) for align_target "sprint"
    :param str input_type: "log_prob" or "prob"
    :param float tdp_scale:
    :param float am_scale:
    :param float min_prob: clips the minimum prob (value in [0,1])
    """
    super().__init__(**kwargs)
    self.align_target = align_target
    self.align_target_key = align_target_key
    self.ctc_opts = ctc_opts
    self.sprint_opts = sprint_opts
    self.input_type = input_type
    self.tdp_scale = tdp_scale
    self.am_scale = am_scale
    self.min_prob = min_prob

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'align_target': self.align_target,
      'align_target_key': self.align_target_key,
      'ctc_opts': self.ctc_opts,
      'sprint_opts': self.sprint_opts,
      'input_type': self.input_type,
      'tdp_scale': self.tdp_scale,
      'am_scale': self.am_scale,
      'min_prob': self.min_prob,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      staircase_seq_len_source: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'staircase_seq_len_source': staircase_seq_len_source,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'fast_bw',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def fast_baum_welch(
                    source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                    *,
                    align_target: str,
                    align_target_key: Optional[str] = NotSpecified,
                    ctc_opts: Dict[str] = NotSpecified,
                    sprint_opts: Dict[str] = NotSpecified,
                    input_type: str = NotSpecified,
                    tdp_scale: float = NotSpecified,
                    am_scale: float = NotSpecified,
                    min_prob: float = NotSpecified,
                    staircase_seq_len_source: Optional[LayerRef] = NotSpecified,
                    name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Calls :func:`fast_baum_welch` or :func:`fast_baum_welch_by_sprint_automata`.
  We expect that our input are +log scores, e.g. use log-softmax.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param str align_target: e.g. "sprint" or "staircase"
  :param str|None align_target_key: e.g. "classes", used for e.g. align_target "ctc"
  :param dict[str] ctc_opts: used for align_target "ctc"
  :param dict[str] sprint_opts: used for Sprint (RASR) for align_target "sprint"
  :param str input_type: "log_prob" or "prob"
  :param float tdp_scale:
  :param float am_scale:
  :param float min_prob: clips the minimum prob (value in [0,1])
  :param LayerBase|None staircase_seq_len_source:
  :param str|None name:
  """
  mod = FastBaumWelch(
    align_target=align_target,
    align_target_key=align_target_key,
    ctc_opts=ctc_opts,
    sprint_opts=sprint_opts,
    input_type=input_type,
    tdp_scale=tdp_scale,
    am_scale=am_scale,
    min_prob=min_prob,
    )
  return mod(
    source,
    staircase_seq_len_source=staircase_seq_len_source,
    name=name)


class SyntheticGradient(_ConcatInput):
  """
  This is a generalized way to be able to replace the true gradient with any kind of predicted gradient.
  This enabled to implement the idea from here:
    Decoupled Neural Interfaces using Synthetic Gradients, https://arxiv.org/abs/1608.05343
  """
  returnn_layer_class = 'synthetic_gradient'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               meta_loss_scale: float = NotSpecified,
               **kwargs):
    """
    :param float meta_loss_scale:
    """
    super().__init__(**kwargs)
    self.meta_loss_scale = meta_loss_scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'meta_loss_scale': self.meta_loss_scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      gradient: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'gradient': gradient,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'synthetic_gradient',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def synthetic_gradient(
                       source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                       *,
                       gradient: LayerRef,
                       meta_loss_scale: float = NotSpecified,
                       name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is a generalized way to be able to replace the true gradient with any kind of predicted gradient.
  This enabled to implement the idea from here:
    Decoupled Neural Interfaces using Synthetic Gradients, https://arxiv.org/abs/1608.05343

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerBase gradient:
  :param float meta_loss_scale:
  :param str|None name:
  """
  mod = SyntheticGradient(
    meta_loss_scale=meta_loss_scale,
    )
  return mod(
    source,
    gradient=gradient,
    name=name)


class TikhonovRegularization(Copy):
  """
  Adds the Tikhonov regularization as a meta-loss (see :class:`returnn.tf.util.basic.MetaLosses`).
  """
  returnn_layer_class = 'tikhonov_regularization'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               meta_loss_scale: float = NotSpecified,
               **kwargs):
    """
    :param float meta_loss_scale:
    """
    super().__init__(**kwargs)
    self.meta_loss_scale = meta_loss_scale

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'meta_loss_scale': self.meta_loss_scale,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'tikhonov_regularization',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def tikhonov_regularization(
                            source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                            *,
                            meta_loss_scale: float = NotSpecified,
                            name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Adds the Tikhonov regularization as a meta-loss (see :class:`returnn.tf.util.basic.MetaLosses`).

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param float meta_loss_scale:
  :param str|None name:
  """
  mod = TikhonovRegularization(
    meta_loss_scale=meta_loss_scale,
    )
  return mod(source, name=name)


class Print(_Base):
  """
  Prints the sources to console/log, via :func:`returnn.tf.util.basic.py_print`.
  """
  returnn_layer_class = 'print'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               summarize: Optional[int] = NotSpecified,
               extra_print_args: Union[List, Tuple] = NotSpecified,
               **kwargs):
    """
    :param int|None summarize: passed to :func:`py_print`
    :param list|tuple extra_print_args:
    """
    super().__init__(**kwargs)
    self.summarize = summarize
    self.extra_print_args = extra_print_args

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'summarize': self.summarize,
      'extra_print_args': self.extra_print_args,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'print',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def print(
          source: LayerRef,
          *,
          summarize: Optional[int] = NotSpecified,
          extra_print_args: Union[List, Tuple] = NotSpecified,
          name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Prints the sources to console/log, via :func:`returnn.tf.util.basic.py_print`.

  :param LayerRef source:
  :param int|None summarize: passed to :func:`py_print`
  :param list|tuple extra_print_args:
  :param str|None name:
  """
  mod = Print(
    summarize=summarize,
    extra_print_args=extra_print_args,
    )
  return mod(source, name=name)


class HDFDump(_Base):
  """
  Dumps into HDF file, compatible to :class:`HDFDataset`.

  The HDF will be written to disk under the specified filename, if there was no error,
  by default at graph reset, via :func:`TFNetwork.register_graph_reset_callback`.
  Or after the dataset iteration run loop, with dump_per_run,
  via :func:`TFNetwork.register_run_finished_callback`.

  Common usage would be to add this to your network with "is_output_layer": True,
  such that you don't need to make other layers depend on it.

  It currently uses :class:`SimpleHDFWriter` internally.
  """
  returnn_layer_class = 'hdf_dump'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               filename: Any,
               dump_whole_batches: bool = NotSpecified,
               labels: Optional[List[str]] = NotSpecified,
               extend_existing_file: bool = NotSpecified,
               dump_per_run: bool = NotSpecified,
               **kwargs):
    """
    :param str|(()->str) filename:
    :param bool dump_whole_batches: dumps the whole batch as a single sequence into the HDF
    :param list[str]|None labels:
    :param bool extend_existing_file: True also means we expect that it exists
    :param bool dump_per_run: write via :func:`TFNetwork.register_run_finished_callback`
    """
    super().__init__(**kwargs)
    self.filename = filename
    self.dump_whole_batches = dump_whole_batches
    self.labels = labels
    self.extend_existing_file = extend_existing_file
    self.dump_per_run = dump_per_run

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'filename': self.filename,
      'dump_whole_batches': self.dump_whole_batches,
      'labels': self.labels,
      'extend_existing_file': self.extend_existing_file,
      'dump_per_run': self.dump_per_run,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      extra: Optional[Dict[str, LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'extra': extra,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'hdf_dump',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def hdf_dump(
             source: LayerRef,
             *,
             filename: Any,
             extra: Optional[Dict[str, LayerRef]] = NotSpecified,
             dump_whole_batches: bool = NotSpecified,
             labels: Optional[List[str]] = NotSpecified,
             extend_existing_file: bool = NotSpecified,
             dump_per_run: bool = NotSpecified,
             name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Dumps into HDF file, compatible to :class:`HDFDataset`.

  The HDF will be written to disk under the specified filename, if there was no error,
  by default at graph reset, via :func:`TFNetwork.register_graph_reset_callback`.
  Or after the dataset iteration run loop, with dump_per_run,
  via :func:`TFNetwork.register_run_finished_callback`.

  Common usage would be to add this to your network with "is_output_layer": True,
  such that you don't need to make other layers depend on it.

  It currently uses :class:`SimpleHDFWriter` internally.

  :param LayerRef source:
  :param str|(()->str) filename:
  :param None|dict[str,LayerBase] extra:
  :param bool dump_whole_batches: dumps the whole batch as a single sequence into the HDF
  :param list[str]|None labels:
  :param bool extend_existing_file: True also means we expect that it exists
  :param bool dump_per_run: write via :func:`TFNetwork.register_run_finished_callback`
  :param str|None name:
  """
  mod = HDFDump(
    filename=filename,
    dump_whole_batches=dump_whole_batches,
    labels=labels,
    extend_existing_file=extend_existing_file,
    dump_per_run=dump_per_run,
    )
  return mod(
    source,
    extra=extra,
    name=name)


class Rec(_ConcatInput):
  """
  Recurrent layer, has support for several implementations of LSTMs (via ``unit`` argument),
  see :ref:`tf_lstm_benchmark` (https://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html),
  and also GRU, or simple RNN.
  Via `unit` parameter, you specify the operation/model performed in the recurrence.
  It can be a string and specify a RNN cell, where all TF cells can be used,
  and the `"Cell"` suffix can be omitted; and case is ignored.
  Some possible LSTM implementations are (in all cases for both CPU and GPU):

   * BasicLSTM (the cell), via official TF, pure TF implementation
   * LSTMBlock (the cell), via tf.contrib.rnn.
   * LSTMBlockFused, via tf.contrib.rnn. should be much faster than BasicLSTM
   * CudnnLSTM, via tf.contrib.cudnn_rnn. This is experimental yet.
   * NativeLSTM, our own native LSTM. should be faster than LSTMBlockFused.
   * NativeLstm2, improved own native LSTM, should be the fastest and most powerful.

  We default to the current tested fastest one, i.e. NativeLSTM.
  Note that they are currently not compatible to each other, i.e. the way the parameters are represented.

  A subnetwork can also be given which will be evaluated step-by-step,
  which can use attention over some separate input,
  which can be used to implement a decoder in a sequence-to-sequence scenario.
  The subnetwork will get the extern data from the parent net as templates,
  and if there is input to the RecLayer,
  then it will be available as the "source" data key in the subnetwork.
  The subnetwork is specified as a `dict` for the `unit` parameter.
  In the subnetwork, you can access outputs from layers from the previous time step when they
  are referred to with the "prev:" prefix.

  Example::

      {
          "class": "rec",
          "from": "input",
          "unit": {
            # Recurrent subnet here, operate on a single time-step:
            "output": {
              "class": "linear",
              "from": ["prev:output", "data:source"],
              "activation": "relu",
              "n_out": n_out},
          },
          "n_out": n_out},
      }

  More examples can be seen in :mod:`test_TFNetworkRecLayer` and :mod:`test_TFEngine`.

  The subnetwork can automatically optimize the inner recurrent loop
  by moving layers out of the loop if possible.
  It will try to do that greedily. This can be disabled via the option `optimize_move_layers_out`.
  It assumes that those layers behave the same with time-dimension or without time-dimension and used per-step.
  Examples for such layers are :class:`LinearLayer`, :class:`RnnCellLayer`
  or :class:`SelfAttentionLayer` with option `attention_left_only`.

  This layer can also be inside another RecLayer. In that case, it behaves similar to :class:`RnnCellLayer`.
  (This support is somewhat incomplete yet. It should work for the native units such as NativeLstm.)

  Also see :ref:`recurrency`.
  """
  returnn_layer_class = 'rec'
  has_recurrent_state = True
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               unit: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               direction: Optional[int] = NotSpecified,
               input_projection: bool = NotSpecified,
               max_seq_len: Optional[Union[str, int]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               optimize_move_layers_out: Optional[bool] = NotSpecified,
               cheating: bool = NotSpecified,
               unroll: bool = NotSpecified,
               back_prop: Optional[bool] = NotSpecified,
               use_global_rec_step_offset: bool = NotSpecified,
               include_eos: bool = NotSpecified,
               debug: Optional[bool] = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param str|_SubnetworkRecCell unit: the RNNCell/etc name, e.g. "nativelstm". see comment below.
      alternatively a whole subnetwork, which will be executed step by step,
      and which can include "prev" in addition to "from" to refer to previous steps.
      The subnetwork is specified as a net dict in the config.
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param int|None direction: None|1 -> forward, -1 -> backward
    :param bool input_projection: True -> input is multiplied with matrix. False only works if same input dim
    :param int|tf.Tensor|None max_seq_len: if unit is a subnetwork. str will be evaluated. see code
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str recurrent_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param bool|None optimize_move_layers_out: will automatically move layers out of the loop when possible
    :param bool cheating: Unused, is now part of ChoiceLayer
    :param bool unroll: if possible, unroll the loop (implementation detail)
    :param bool|None back_prop: for tf.while_loop. the default will use self.network.train_flag
    :param bool use_global_rec_step_offset:
    :param bool include_eos: for search, whether we should include the frame where "end" is True
    :param bool|None debug:
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.unit = unit
    self.unit_opts = unit_opts
    self.direction = direction
    self.input_projection = input_projection
    self.max_seq_len = max_seq_len
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init
    self.optimize_move_layers_out = optimize_move_layers_out
    self.cheating = cheating
    self.unroll = unroll
    self.back_prop = back_prop
    self.use_global_rec_step_offset = use_global_rec_step_offset
    self.include_eos = include_eos
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'unit': self.unit,
      'unit_opts': self.unit_opts,
      'direction': self.direction,
      'input_projection': self.input_projection,
      'max_seq_len': self.max_seq_len,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
      'optimize_move_layers_out': self.optimize_move_layers_out,
      'cheating': self.cheating,
      'unroll': self.unroll,
      'back_prop': self.back_prop,
      'use_global_rec_step_offset': self.use_global_rec_step_offset,
      'include_eos': self.include_eos,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]] = (),
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'rec',
      'from': source,
      **args,
      **self.get_opts()}


class _GetLastHiddenState(_Base):
  """
  Will combine (concat or add or so) all the last hidden states from all sources.
  """
  returnn_layer_class = 'get_last_hidden_state'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               combine: str = NotSpecified,
               key: Optional[Union[str, int]] = NotSpecified,
               **kwargs):
    """
    :param int n_out: dimension. output will be of shape (batch, n_out)
    :param str combine: "concat" or "add"
    :param str|int|None key: for the state, which could be a namedtuple. see :func:`RnnCellLayer.get_state_by_key`
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.combine = combine
    self.key = key

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'combine': self.combine,
      'key': self.key,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'get_last_hidden_state',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def _get_last_hidden_state(
                           source: LayerRef,
                           *,
                           n_out: int,
                           combine: str = NotSpecified,
                           key: Optional[Union[str, int]] = NotSpecified,
                           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Will combine (concat or add or so) all the last hidden states from all sources.

  :param LayerRef source:
  :param int n_out: dimension. output will be of shape (batch, n_out)
  :param str combine: "concat" or "add"
  :param str|int|None key: for the state, which could be a namedtuple. see :func:`RnnCellLayer.get_state_by_key`
  :param str|None name:
  """
  mod = _GetLastHiddenState(
    n_out=n_out,
    combine=combine,
    key=key,
    )
  return mod(source, name=name)


class RecUnstack(_Base):
  """
  This is supposed to be used inside a :class:`RecLayer`.
  The input is supposed to be outside the rec layer (i.e. via ``base:``).
  Uses tf.TensorArray and then unstack on the inputs to make it available per-frame.
  This is an alternative to making some input to the rec layer,
  such that the rec layer can have multiple inputs (as long as they have the same time dim).

  Note that due to automatic optimization, this layer will be optimized out of the rec loop anyway,
  and then the tf.TensorArray logic happens internally in RecLayer,
  thus we do not need to care about this here.
  (See get_input_moved_out for some internal handling.)

  Effectively, this layer is very similar to :class:`CopyLayer`,
  with the only special behavior that it assigns the loop dimension of RecLayer.
  """
  returnn_layer_class = 'rec_unstack'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      axis: Union[str, DimensionTag],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'axis': axis,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'rec_unstack',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def rec_unstack(
                source: LayerRef,
                *,
                axis: Union[str, DimensionTag],
                name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is supposed to be used inside a :class:`RecLayer`.
  The input is supposed to be outside the rec layer (i.e. via ``base:``).
  Uses tf.TensorArray and then unstack on the inputs to make it available per-frame.
  This is an alternative to making some input to the rec layer,
  such that the rec layer can have multiple inputs (as long as they have the same time dim).

  Note that due to automatic optimization, this layer will be optimized out of the rec loop anyway,
  and then the tf.TensorArray logic happens internally in RecLayer,
  thus we do not need to care about this here.
  (See get_input_moved_out for some internal handling.)

  Effectively, this layer is very similar to :class:`CopyLayer`,
  with the only special behavior that it assigns the loop dimension of RecLayer.

  :param LayerRef source:
  :param str|DimensionTag axis:
  :param str|None name:
  """
  mod = RecUnstack()
  return mod(
    source,
    axis=axis,
    name=name)


class BaseChoice(_Base):
  """
  This is a base-class for any layer which defines a new search choice,
  i.e. which defines ``self.search_choices``.
  """
  returnn_layer_class = None
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               beam_size: Optional[int],
               search: Union[NotSpecified, bool] = NotSpecified,
               **kwargs):
    """
    :param int|None beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
    :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
      If not specified, it will depend on `network.search_flag`.
    """
    super().__init__(**kwargs)
    self.beam_size = beam_size
    self.search = search

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'beam_size': self.beam_size,
      'search': self.search,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  make_layer_dict = ILayerMaker.make_layer_dict  # abstract


class Choice(BaseChoice):
  """
  This layer represents a choice to be made in search during inference,
  such as choosing the top-k outputs from a log-softmax for beam search.
  During training, this layer can return the true label.
  This is supposed to be used inside the rec layer.
  This can be extended in various ways.

  We present the scores in +log space, and we will add them up along the path.
  Assume that we get input (batch,dim) from a (log-)softmax.
  Assume that each batch is already a choice via search.
  In search with a beam size of N, we would output
  sparse (batch=N,) and scores for each.

  In case of multiple sources, this layer computes the top-k combinations of choices. The score of such a combination
  is determined by adding up the (log-space) scores of the choices for the individual sources. In this case, the
  'target' parameter of the layer has to be set to a list of targets corresponding to the sources respectively. Because
  computing all possible combinations of source scores is costly, the sources are pruned beforehand using the beam
  sizes set by the 'source_beam_sizes' parameter. The choices made for the different sources can be accessed via the
  sublayers '<choice layer name>/out_0', '<choice layer name>/out_1' and so on.
  Note, that the way scores are combined assumes the sources to be independent. If you want to model a dependency,
  use separate ChoiceLayers and let the input of one depend on the output of the other.
  """
  returnn_layer_class = 'choice'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               beam_size: int,
               keep_beams: bool = NotSpecified,
               search: Union[NotSpecified, bool] = NotSpecified,
               input_type: str = NotSpecified,
               prob_scale: float = NotSpecified,
               base_beam_score_scale: float = NotSpecified,
               random_sample_scale: float = NotSpecified,
               length_normalization: bool = NotSpecified,
               length_normalization_exponent: Any = NotSpecified,
               custom_score_combine: Optional[callable] = NotSpecified,
               source_beam_sizes: Optional[List[int]] = NotSpecified,
               scheduled_sampling: Optional[Dict] = NotSpecified,
               cheating: Union[bool, str] = NotSpecified,
               **kwargs):
    """
    :param int beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
    :param bool keep_beams: specifies that we keep the beam_in entries,
      i.e. we just expand, i.e. we just search on the dim. beam_size must be a multiple of beam_in.
    :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
      If not specified, it will depend on `network.search_flag`.
    :param str input_type: "prob" or "log_prob", whether the input is in probability space, log-space, etc.
      or "regression", if it is a prediction of the data as-is. If there are several inputs, same format
      for all is assumed.
    :param float prob_scale: factor for prob (score in +log space from source)
    :param float base_beam_score_scale: factor for beam base score (i.e. prev prob scores)
    :param float random_sample_scale: if >0, will add Gumbel scores. you might want to set base_beam_score_scale=0
    :param bool length_normalization: evaluates score_t/len in search
    :param length_normalization_exponent:
    :param callable|None custom_score_combine:
    :param list[int]|None source_beam_sizes: If there are several sources, they are pruned with these beam sizes
       before combination. If None, 'beam_size' is used for all sources. Has to have same length as number of sources.
    :param dict|None scheduled_sampling:
    :param bool|str cheating: if True, will always add the true target in the beam.
      if "exclusive", enables cheating_exclusive. see :func:`returnn.tf.util.basic.beam_search`.
    """
    super().__init__(beam_size=beam_size, search=search, **kwargs)
    self.beam_size = beam_size
    self.keep_beams = keep_beams
    self.search = search
    self.input_type = input_type
    self.prob_scale = prob_scale
    self.base_beam_score_scale = base_beam_score_scale
    self.random_sample_scale = random_sample_scale
    self.length_normalization = length_normalization
    self.length_normalization_exponent = length_normalization_exponent
    self.custom_score_combine = custom_score_combine
    self.source_beam_sizes = source_beam_sizes
    self.scheduled_sampling = scheduled_sampling
    self.cheating = cheating

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'beam_size': self.beam_size,
      'keep_beams': self.keep_beams,
      'search': self.search,
      'input_type': self.input_type,
      'prob_scale': self.prob_scale,
      'base_beam_score_scale': self.base_beam_score_scale,
      'random_sample_scale': self.random_sample_scale,
      'length_normalization': self.length_normalization,
      'length_normalization_exponent': self.length_normalization_exponent,
      'custom_score_combine': self.custom_score_combine,
      'source_beam_sizes': self.source_beam_sizes,
      'scheduled_sampling': self.scheduled_sampling,
      'cheating': self.cheating,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      *,
                      target: LayerRef,
                      explicit_search_sources: Optional[List[LayerRef]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'target': target,
      'explicit_search_sources': explicit_search_sources,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'choice',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice(
           source: LayerRef,
           *,
           target: LayerRef,
           beam_size: int,
           keep_beams: bool = NotSpecified,
           search: Union[NotSpecified, bool] = NotSpecified,
           input_type: str = NotSpecified,
           prob_scale: float = NotSpecified,
           base_beam_score_scale: float = NotSpecified,
           random_sample_scale: float = NotSpecified,
           length_normalization: bool = NotSpecified,
           length_normalization_exponent: Any = NotSpecified,
           custom_score_combine: Optional[callable] = NotSpecified,
           source_beam_sizes: Optional[List[int]] = NotSpecified,
           scheduled_sampling: Optional[Dict] = NotSpecified,
           cheating: Union[bool, str] = NotSpecified,
           explicit_search_sources: Optional[List[LayerRef]] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This layer represents a choice to be made in search during inference,
  such as choosing the top-k outputs from a log-softmax for beam search.
  During training, this layer can return the true label.
  This is supposed to be used inside the rec layer.
  This can be extended in various ways.

  We present the scores in +log space, and we will add them up along the path.
  Assume that we get input (batch,dim) from a (log-)softmax.
  Assume that each batch is already a choice via search.
  In search with a beam size of N, we would output
  sparse (batch=N,) and scores for each.

  In case of multiple sources, this layer computes the top-k combinations of choices. The score of such a combination
  is determined by adding up the (log-space) scores of the choices for the individual sources. In this case, the
  'target' parameter of the layer has to be set to a list of targets corresponding to the sources respectively. Because
  computing all possible combinations of source scores is costly, the sources are pruned beforehand using the beam
  sizes set by the 'source_beam_sizes' parameter. The choices made for the different sources can be accessed via the
  sublayers '<choice layer name>/out_0', '<choice layer name>/out_1' and so on.
  Note, that the way scores are combined assumes the sources to be independent. If you want to model a dependency,
  use separate ChoiceLayers and let the input of one depend on the output of the other.

  :param LayerRef source:
  :param LayerBase target: target
  :param int beam_size: the outgoing beam size. i.e. our output will be (batch * beam_size, ...)
  :param bool keep_beams: specifies that we keep the beam_in entries,
    i.e. we just expand, i.e. we just search on the dim. beam_size must be a multiple of beam_in.
  :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
    If not specified, it will depend on `network.search_flag`.
  :param str input_type: "prob" or "log_prob", whether the input is in probability space, log-space, etc.
    or "regression", if it is a prediction of the data as-is. If there are several inputs, same format
    for all is assumed.
  :param float prob_scale: factor for prob (score in +log space from source)
  :param float base_beam_score_scale: factor for beam base score (i.e. prev prob scores)
  :param float random_sample_scale: if >0, will add Gumbel scores. you might want to set base_beam_score_scale=0
  :param bool length_normalization: evaluates score_t/len in search
  :param length_normalization_exponent:
  :param callable|None custom_score_combine:
  :param list[int]|None source_beam_sizes: If there are several sources, they are pruned with these beam sizes
     before combination. If None, 'beam_size' is used for all sources. Has to have same length as number of sources.
  :param dict|None scheduled_sampling:
  :param bool|str cheating: if True, will always add the true target in the beam.
    if "exclusive", enables cheating_exclusive. see :func:`returnn.tf.util.basic.beam_search`.
  :param list[LayerBase]|None explicit_search_sources: will mark it as an additional dependency.
    You might use these also in custom_score_combine.
  :param str|None name:
  """
  mod = Choice(
    beam_size=beam_size,
    keep_beams=keep_beams,
    search=search,
    input_type=input_type,
    prob_scale=prob_scale,
    base_beam_score_scale=base_beam_score_scale,
    random_sample_scale=random_sample_scale,
    length_normalization=length_normalization,
    length_normalization_exponent=length_normalization_exponent,
    custom_score_combine=custom_score_combine,
    source_beam_sizes=source_beam_sizes,
    scheduled_sampling=scheduled_sampling,
    cheating=cheating,
    )
  return mod(
    source,
    target=target,
    explicit_search_sources=explicit_search_sources,
    name=name)


class Decide(BaseChoice):
  """
  This is kind of the counter-part to the choice layer.
  This only has an effect in search mode.
  E.g. assume that the input is of shape (batch * beam, time, dim)
  and has search_sources set.
  Then this will output (batch, time, dim) where the beam with the highest score is selected.
  Thus, this will do a decision based on the scores.
  In will convert the data to batch-major mode.
  """
  returnn_layer_class = 'decide'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               length_normalization: bool = NotSpecified,
               **kwargs):
    """
    :param bool length_normalization: performed on the beam scores
    """
    super().__init__(beam_size=1, **kwargs)
    self.length_normalization = length_normalization

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'length_normalization': self.length_normalization,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    opts.update(super().get_opts())
    opts.pop('beam_size')
    return opts

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'decide',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def decide(
           source: LayerRef,
           *,
           length_normalization: bool = NotSpecified,
           search: Union[NotSpecified, bool] = NotSpecified,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is kind of the counter-part to the choice layer.
  This only has an effect in search mode.
  E.g. assume that the input is of shape (batch * beam, time, dim)
  and has search_sources set.
  Then this will output (batch, time, dim) where the beam with the highest score is selected.
  Thus, this will do a decision based on the scores.
  In will convert the data to batch-major mode.

  :param LayerRef source:
  :param bool length_normalization: performed on the beam scores
  :param NotSpecified|bool search: whether to perform search, or use the ground truth (`target` option).
    If not specified, it will depend on `network.search_flag`.
  :param str|None name:
  """
  mod = Decide(
    length_normalization=length_normalization,
    search=search,
    )
  return mod(source, name=name)


class ChoiceGetBeamScores(_Base):
  """
  Gets beam scores from :class:`SearchChoices`.
  This requires that the source has search choices.

  .. note::

    This layer might be deprecated in the future.

  """
  returnn_layer_class = 'choice_get_beam_scores'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'choice_get_beam_scores',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice_get_beam_scores(
                           source: LayerRef,
                           *,
                           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gets beam scores from :class:`SearchChoices`.
  This requires that the source has search choices.

  .. note::

    This layer might be deprecated in the future.


  :param LayerRef source:
  :param str|None name:
  """
  mod = ChoiceGetBeamScores()
  return mod(source, name=name)


class ChoiceGetSrcBeams(_Base):
  """
  Gets source beam indices from :class:`SearchChoices`.
  This requires that the source has search choices.
  """
  returnn_layer_class = 'choice_get_src_beams'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'choice_get_src_beams',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def choice_get_src_beams(
                         source: LayerRef,
                         *,
                         name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Gets source beam indices from :class:`SearchChoices`.
  This requires that the source has search choices.

  :param LayerRef source:
  :param str|None name:
  """
  mod = ChoiceGetSrcBeams()
  return mod(source, name=name)


class PositionalEncoding(_ConcatInput):
  """
  Provides positional encoding in the form of (batch, time, n_out) or (time, batch, n_out)
  where n_out is the number of channels, if it is run outside a :class:`RecLayer`,
  and (batch, n_out) or (n_out, batch)
  if run inside a :class:`RecLayer`, where it will depend on the current time frame.

  Assumes one source input with a time dimension if outside a :class:`RecLayer`.
  With `add_to_input`, it will calculate `x + input`, and the output shape is the same as the input

  The positional encoding is the same as in Tensor2Tensor.
  See :func:`returnn.tf.util.basic.get_positional_encoding`.
  """
  returnn_layer_class = 'positional_encoding'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               add_to_input: bool = NotSpecified,
               constant: int = NotSpecified,
               **kwargs):
    """
    :param int n_out: output dimension
    :param bool add_to_input: will add the signal to the input
    :param int constant: if positive, always output the corresponding positional encoding.
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.add_to_input = add_to_input
    self.constant = constant

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'add_to_input': self.add_to_input,
      'constant': self.constant,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      *,
                      offset: Optional[LayerRef] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'offset': offset,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'positional_encoding',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def positional_encoding(
                        source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                        *,
                        n_out: int,
                        add_to_input: bool = NotSpecified,
                        constant: int = NotSpecified,
                        offset: Optional[LayerRef] = NotSpecified,
                        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Provides positional encoding in the form of (batch, time, n_out) or (time, batch, n_out)
  where n_out is the number of channels, if it is run outside a :class:`RecLayer`,
  and (batch, n_out) or (n_out, batch)
  if run inside a :class:`RecLayer`, where it will depend on the current time frame.

  Assumes one source input with a time dimension if outside a :class:`RecLayer`.
  With `add_to_input`, it will calculate `x + input`, and the output shape is the same as the input

  The positional encoding is the same as in Tensor2Tensor.
  See :func:`returnn.tf.util.basic.get_positional_encoding`.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param int n_out: output dimension
  :param bool add_to_input: will add the signal to the input
  :param int constant: if positive, always output the corresponding positional encoding.
  :param None|LayerBase offset: Specify the offset to be added to positions. Expect shape (batch, time) or (batch,).
  :param str|None name:
  """
  mod = PositionalEncoding(
    n_out=n_out,
    add_to_input=add_to_input,
    constant=constant,
    )
  return mod(
    source,
    offset=offset,
    name=name)


class KenLmState(_ConcatInput):
  """
  Get next word (or subword) each frame,
  accumulates string,
  keeps state of seen string so far,
  returns score (+log space, natural base e) of sequence,
  using KenLM (https://kheafield.com/code/kenlm/) (see :mod:`TFKenLM`).
  EOS (</s>) token must be used explicitly.
  """
  returnn_layer_class = 'kenlm'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               lm_file: Any,
               vocab_file: Optional[str] = NotSpecified,
               vocab_unknown_label: str = NotSpecified,
               bpe_merge_symbol: Optional[str] = NotSpecified,
               input_step_offset: int = NotSpecified,
               dense_output: bool = NotSpecified,
               debug: bool = NotSpecified,
               **kwargs):
    """
    :param str|()->str lm_file: ARPA file or so. whatever KenLM supports
    :param str|None vocab_file: if the inputs are symbols, this must be provided. see :class:`Vocabulary`
    :param str vocab_unknown_label: for the vocabulary
    :param str|None bpe_merge_symbol: e.g. "@@" if you want to apply BPE merging
    :param int input_step_offset: if provided, will consider the input only from this step onwards
    :param bool dense_output: whether we output the score for all possible succeeding tokens
    :param bool debug: prints debug info
    """
    super().__init__(**kwargs)
    self.lm_file = lm_file
    self.vocab_file = vocab_file
    self.vocab_unknown_label = vocab_unknown_label
    self.bpe_merge_symbol = bpe_merge_symbol
    self.input_step_offset = input_step_offset
    self.dense_output = dense_output
    self.debug = debug

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'lm_file': self.lm_file,
      'vocab_file': self.vocab_file,
      'vocab_unknown_label': self.vocab_unknown_label,
      'bpe_merge_symbol': self.bpe_merge_symbol,
      'input_step_offset': self.input_step_offset,
      'dense_output': self.dense_output,
      'debug': self.debug,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'kenlm',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def ken_lm_state(
                 source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                 state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                 *,
                 lm_file: Any,
                 vocab_file: Optional[str] = NotSpecified,
                 vocab_unknown_label: str = NotSpecified,
                 bpe_merge_symbol: Optional[str] = NotSpecified,
                 input_step_offset: int = NotSpecified,
                 dense_output: bool = NotSpecified,
                 debug: bool = NotSpecified,
                 name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Get next word (or subword) each frame,
  accumulates string,
  keeps state of seen string so far,
  returns score (+log space, natural base e) of sequence,
  using KenLM (https://kheafield.com/code/kenlm/) (see :mod:`TFKenLM`).
  EOS (</s>) token must be used explicitly.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param str|()->str lm_file: ARPA file or so. whatever KenLM supports
  :param str|None vocab_file: if the inputs are symbols, this must be provided. see :class:`Vocabulary`
  :param str vocab_unknown_label: for the vocabulary
  :param str|None bpe_merge_symbol: e.g. "@@" if you want to apply BPE merging
  :param int input_step_offset: if provided, will consider the input only from this step onwards
  :param bool dense_output: whether we output the score for all possible succeeding tokens
  :param bool debug: prints debug info
  :param str|None name:
  """
  mod = KenLmState(
    lm_file=lm_file,
    vocab_file=vocab_file,
    vocab_unknown_label=vocab_unknown_label,
    bpe_merge_symbol=bpe_merge_symbol,
    input_step_offset=input_step_offset,
    dense_output=dense_output,
    debug=debug,
    )
  return mod(source, state, name=name)


class EditDistanceTable(_Base):
  """
  Given a source and a target, calculates the edit distance table between them.
  Source can be inside a recurrent loop.
  It uses :func:`TFNativeOp.next_edit_distance_row`.

  Usually, if you are inside a rec layer, and "output" is the :class:`ChoiceLayer`,
  you would use "from": "output"
  and "target": "layer:base:data:target" (make sure it has the time dimension).

  See also :class:`OptimalCompletionsLayer`.
  """
  returnn_layer_class = 'edit_distance_table'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               debug: bool = NotSpecified,
               blank_idx: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param bool debug:
    :param int|None blank_idx: if given, will keep the same row for this source label
    """
    super().__init__(**kwargs)
    self.debug = debug
    self.blank_idx = blank_idx

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'debug': self.debug,
      'blank_idx': self.blank_idx,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'edit_distance_table',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def edit_distance_table(
                        source: LayerRef,
                        state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                        *,
                        debug: bool = NotSpecified,
                        blank_idx: Optional[int] = NotSpecified,
                        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Given a source and a target, calculates the edit distance table between them.
  Source can be inside a recurrent loop.
  It uses :func:`TFNativeOp.next_edit_distance_row`.

  Usually, if you are inside a rec layer, and "output" is the :class:`ChoiceLayer`,
  you would use "from": "output"
  and "target": "layer:base:data:target" (make sure it has the time dimension).

  See also :class:`OptimalCompletionsLayer`.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param bool debug:
  :param int|None blank_idx: if given, will keep the same row for this source label
  :param str|None name:
  """
  mod = EditDistanceTable(
    debug=debug,
    blank_idx=blank_idx,
    )
  return mod(source, state, name=name)


class OptimalCompletions(_Base):
  """
  We expect to get the inputs from :class:`EditDistanceTableLayer`, esp from the prev frame, like this:
  "opt_completions": {"class": "optimal_completions", "from": "prev:edit_dist_table"}.

  You can also then define this further layer:
  "opt_completion_soft_targets": {
    "class": "eval", "eval": "tf.nn.softmax(tf.cast(source(0), tf.float32))",
    "from": "opt_completions", "out_type": {"dtype": "float32"}},
  and use that as the :class:`CrossEntropyLoss` soft targets
  for the input of the "output" :class:`ChoiceLayer`, e.g. "output_prob".
  This makes most sense when you enable beam search (even, or esp, during training).
  Note that you probably want to have this all before the last choice, where you still have more beams open.
  """
  returnn_layer_class = 'optimal_completions'
  has_recurrent_state = False
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               debug: bool = NotSpecified,
               blank_idx: Optional[int] = NotSpecified,
               **kwargs):
    """
    :param bool debug:
    :param int|None blank_idx:
    """
    super().__init__(**kwargs)
    self.debug = debug
    self.blank_idx = blank_idx

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'debug': self.debug,
      'blank_idx': self.blank_idx,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'optimal_completions',
      'from': source,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def optimal_completions(
                        source: LayerRef,
                        *,
                        debug: bool = NotSpecified,
                        blank_idx: Optional[int] = NotSpecified,
                        name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  We expect to get the inputs from :class:`EditDistanceTableLayer`, esp from the prev frame, like this:
  "opt_completions": {"class": "optimal_completions", "from": "prev:edit_dist_table"}.

  You can also then define this further layer:
  "opt_completion_soft_targets": {
    "class": "eval", "eval": "tf.nn.softmax(tf.cast(source(0), tf.float32))",
    "from": "opt_completions", "out_type": {"dtype": "float32"}},
  and use that as the :class:`CrossEntropyLoss` soft targets
  for the input of the "output" :class:`ChoiceLayer`, e.g. "output_prob".
  This makes most sense when you enable beam search (even, or esp, during training).
  Note that you probably want to have this all before the last choice, where you still have more beams open.

  :param LayerRef source:
  :param bool debug:
  :param int|None blank_idx:
  :param str|None name:
  """
  mod = OptimalCompletions(
    debug=debug,
    blank_idx=blank_idx,
    )
  return mod(source, name=name)


class Unmask(_Base):
  """
  This is meant to be used together with :class:`MaskedComputationLayer`,
  which operates on input [B,T,D], and given a mask, returns [B,T',D'].
  This layer :class:`UnmaskLayer` is supposed to undo the masking,
  i.e. to recover the original time dimension, i.e. given [B,T',D'], we output [B,T,D'].
  This is done by repeating the output for the non-masked frames,
  via the last masked frame.

  If this layer is inside a recurrent loop, i.e. we get [B,D'] as input,
  this is a no-op, and we just return the input as is.
  In that case, the repetition logic is handled via :class:`MaskedComputationLayer`.
  """
  returnn_layer_class = 'unmask'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      *,
                      mask: LayerRef,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
      'mask': mask,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'unmask',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def unmask(
           source: LayerRef,
           state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
           *,
           mask: LayerRef,
           name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  This is meant to be used together with :class:`MaskedComputationLayer`,
  which operates on input [B,T,D], and given a mask, returns [B,T',D'].
  This layer :class:`UnmaskLayer` is supposed to undo the masking,
  i.e. to recover the original time dimension, i.e. given [B,T',D'], we output [B,T,D'].
  This is done by repeating the output for the non-masked frames,
  via the last masked frame.

  If this layer is inside a recurrent loop, i.e. we get [B,D'] as input,
  this is a no-op, and we just return the input as is.
  In that case, the repetition logic is handled via :class:`MaskedComputationLayer`.

  :param LayerRef source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param LayerBase mask: the same as as used for :class:`MaskedComputationLayer`.
    Outside loop: [B,T] or [T,B], original T. Inside loop, just [B].
  :param str|None name:
  """
  mod = Unmask()
  return mod(
    source,
    state,
    mask=mask,
    name=name)


class TwoDLSTM(_Base):
  """
  2D LSTM.

  Currently only from left-to-right in the time axis.
  Can be inside a recurrent loop, or outside.
  """
  returnn_layer_class = 'twod_lstm'
  has_recurrent_state = True
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               pooling: str = NotSpecified,
               unit_opts: Optional[Dict[str]] = NotSpecified,
               forward_weights_init: str = NotSpecified,
               recurrent_weights_init: str = NotSpecified,
               bias_init: str = NotSpecified,
               **kwargs):
    """
    :param str pooling: defines how the 1D return value is computed based on the 2D lstm result. Either 'last' or 'max'
    :param None|dict[str] unit_opts: passed to RNNCell creation
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str recurrent_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param str bias_init: see :func:`returnn.tf.util.basic.get_initializer`
    """
    super().__init__(**kwargs)
    self.pooling = pooling
    self.unit_opts = unit_opts
    self.forward_weights_init = forward_weights_init
    self.recurrent_weights_init = recurrent_weights_init
    self.bias_init = bias_init

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'pooling': self.pooling,
      'unit_opts': self.unit_opts,
      'forward_weights_init': self.forward_weights_init,
      'recurrent_weights_init': self.recurrent_weights_init,
      'bias_init': self.bias_init,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: LayerRef,
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'twod_lstm',
      'from': source,
      **args,
      **self.get_opts()}


class RelativePositionalEncoding(_ConcatInput):
  """
  Relative positioning term as introduced by Shaw et al., 2018

  Usually added to Self-Attention using key_shift.
  Parts of the code are adapted from Tensor2Tensor (https://github.com/tensorflow/tensor2tensor).

  Example usage::

      d[output + '_rel_pos'] = {"class": "relative_positional_encoding",
                                "from": [output + '_self_att_laynorm'],
                                "n_out": self.EncKeyTotalDim // self.AttNumHeads,
                                "forward_weights_init": self.ff_init}
      d[output + '_self_att_att'] = {"class": "self_attention",
                                     "num_heads": self.AttNumHeads,
                                     "total_key_dim": self.EncKeyTotalDim,
                                     "n_out": self.EncValueTotalDim, "from": [output + '_self_att_laynorm'],
                                     "attention_left_only": False, "attention_dropout": self.attention_dropout,
                                     "forward_weights_init": self.ff_init,
                                     "key_shift": output + '_rel_pos'}

  """
  returnn_layer_class = 'relative_positional_encoding'
  has_recurrent_state = False
  has_variables = True

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               n_out: int,
               *,
               forward_weights_init: str = NotSpecified,
               clipping: int = NotSpecified,
               fixed: bool = NotSpecified,
               **kwargs):
    """
    :param int n_out: Feature dimension of encoding.
    :param str forward_weights_init: see :func:`returnn.tf.util.basic.get_initializer`
    :param int clipping: After which distance to fallback to the last encoding
    :param bool fixed: Uses sinusoid positional encoding instead of learned parameters
    """
    super().__init__(**kwargs)
    self.n_out = n_out
    self.forward_weights_init = forward_weights_init
    self.clipping = clipping
    self.fixed = fixed

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'n_out': self.n_out,
      'forward_weights_init': self.forward_weights_init,
      'clipping': self.clipping,
      'fixed': self.fixed,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    return {
      'class': 'relative_positional_encoding',
      'from': source,
      **self.get_opts()}


class CumConcat(_ConcatInput):
  """
  Concatenates all previous frames of a time-axis.
  Like :class:`CumsumLayer` uses `sum`, this layer uses `concat`.

  This layer can be used as a base for auto-regressive self-attention.

  This layer expects to be inside a :class:`RecLayer`.

  Inside a rec loop (not optimized out),
  this will concatenate the current input
  to the previous accumulated inputs.
  For an input of shape `input_shape`,
  it will output a tensor of shape `[new_dim] + input_shape`.
  `new_dim` is a special dimension, usually of length `i`,
  where `i` is the current loop frame,
  i.e. the length increases in every loop frame.
  `new_dim` is specified by a separate own dim tag.
  For example, in the first frame,
  this will be of shape `[1] + input_shape`,
  in the second frame shape `[2] + input_shape`,
  and so on,
  and in the last frame shape `[T] + input_shape`.

  Outside the rec loop (optimized out),
  this layer expects an input with the time dim of the rec layer,
  and returns the input as-is,
  but replacing the time dim tag with the dim tag `new_dim`
  converted as outside the loop.

  Normally the optimization should not matter for the user,
  i.e. for the user, the logical behavior is always as being inside the rec loop.
  Outside the loop,
  the output represents a tensor of shape `[T, new_dim] + input_shape`,
  although we actually have another `new_dim` outside the loop,
  and `T` is not actually there,
  but we still have all the information,
  because the last frame has all information.
  This `new_dim` outside the loop stores all the dynamic seq lengths
  per frame of the loop, i.e. the dyn seq len are extended of shape [B,T] or [T]
  (unlike usually just [B]).
  This way following layers use different seq lengths of `new_dim` for different loop frames,
  just like if the `T` dim would actually exist.
  """
  returnn_layer_class = 'cum_concat'
  has_recurrent_state = True
  has_variables = False

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def __init__(self,
               *,
               new_dim: DimensionTag,
               **kwargs):
    """
    :param DimensionTag new_dim:
    """
    super().__init__(**kwargs)
    self.new_dim = new_dim

  def get_opts(self):
    """
    Return all options
    """
    opts = {
      'new_dim': self.new_dim,
    }
    opts = {key: value for (key, value) in opts.items() if value is not NotSpecified}
    return {**opts, **super().get_opts()}

  # noinspection PyShadowingBuiltins,PyShadowingNames
  def make_layer_dict(self,
                      source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
                      state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
                      ) -> LayerDictRaw:
    """
    Make layer dict
    """
    args = {
      'initial_state': state,
    }
    args = {key: value for (key, value) in args.items() if value is not NotSpecified}
    return {
      'class': 'cum_concat',
      'from': source,
      **args,
      **self.get_opts()}


# noinspection PyShadowingBuiltins,PyShadowingNames
def cum_concat(
               source: Union[LayerRef, List[LayerRef], Tuple[LayerRef]],
               state: Optional[Union[LayerRef, List[LayerRef], Tuple[LayerRef], NotSpecified]] = NotSpecified,
               *,
               new_dim: DimensionTag,
               name: Optional[Union[str, NameCtx]] = None) -> Layer:
  """
  Concatenates all previous frames of a time-axis.
  Like :class:`CumsumLayer` uses `sum`, this layer uses `concat`.

  This layer can be used as a base for auto-regressive self-attention.

  This layer expects to be inside a :class:`RecLayer`.

  Inside a rec loop (not optimized out),
  this will concatenate the current input
  to the previous accumulated inputs.
  For an input of shape `input_shape`,
  it will output a tensor of shape `[new_dim] + input_shape`.
  `new_dim` is a special dimension, usually of length `i`,
  where `i` is the current loop frame,
  i.e. the length increases in every loop frame.
  `new_dim` is specified by a separate own dim tag.
  For example, in the first frame,
  this will be of shape `[1] + input_shape`,
  in the second frame shape `[2] + input_shape`,
  and so on,
  and in the last frame shape `[T] + input_shape`.

  Outside the rec loop (optimized out),
  this layer expects an input with the time dim of the rec layer,
  and returns the input as-is,
  but replacing the time dim tag with the dim tag `new_dim`
  converted as outside the loop.

  Normally the optimization should not matter for the user,
  i.e. for the user, the logical behavior is always as being inside the rec loop.
  Outside the loop,
  the output represents a tensor of shape `[T, new_dim] + input_shape`,
  although we actually have another `new_dim` outside the loop,
  and `T` is not actually there,
  but we still have all the information,
  because the last frame has all information.
  This `new_dim` outside the loop stores all the dynamic seq lengths
  per frame of the loop, i.e. the dyn seq len are extended of shape [B,T] or [T]
  (unlike usually just [B]).
  This way following layers use different seq lengths of `new_dim` for different loop frames,
  just like if the `T` dim would actually exist.

  :param LayerRef|list[LayerRef]|tuple[LayerRef] source:
  :param LayerRef|list[LayerRef]|tuple[LayerRef]|NotSpecified|None state:
  :param DimensionTag new_dim:
  :param str|None name:
  """
  mod = CumConcat(
    new_dim=new_dim,
    )
  return mod(source, state, name=name)
